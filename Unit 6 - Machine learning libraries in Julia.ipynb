{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d977b995-8f5e-4031-b67f-acbd83c49a53",
   "metadata": {},
   "source": [
    "# The ScikitLearn.jl library\n",
    "\n",
    "The Scikit-learn library is an open-source machine learning library developed for the Python programming language, the first version of which dates back to 2010. It implements many machine learning models, related to classification, regression, clustering or dimensionality reduction. These models include Support Vector Machines (SVM), decision trees, random forests, or k-means. It is currently one of the most widely used libraries in the field of machine learning, due to the large number of functionalities it offers as well as its ease of use, since it provides a uniform interface for training and using models. The documentation for this library is available at https://scikit-learn.org/stable/.\n",
    "\n",
    "For Julia, the ScikitLearn.jl library implements this interface and the algorithms contained in the scikit-learn library, supporting both Julia's own models and those of the scikit-learn library. The latter is done by means of the PyCall.jl library, which allows code written in Python to be executed from Julia in a transparent way for the user, who only needs to have ScikitLearn.jl installed. Documentation for this library can be found at https://scikitlearnjl.readthedocs.io/en/latest/.\n",
    "\n",
    "However, recently, some incompatibilities have been reported with some versions of the SSL library. To avoid potential compatibility issues between Julia, PyCall, and ScikitLearn, we will use a different library for this exercise.\n",
    "The library we will use is MLJ (Machine Learning in Julia), which is not strictly a library but rather a framework that allows the use of various related libraries through a common interface.\n",
    "As a result, the function names used to create and train models remain the same regardless of the specific models being used.\n",
    "In the practical sessions of this course, in addition to ANNs, we will use the following models, available within the MLJ framework:\n",
    "\n",
    "- Support Vector Machines (SVM)\n",
    "- Decision trees\n",
    "- kNN\n",
    "\n",
    "In order to use these models, it is first necessary to install and import the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74f7a80f-4883-46f1-be06-db52e981c19a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "import Pkg;\n",
    "using MLJ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f4b6f9-e0c7-4e04-a4d8-21d63495a95c",
   "metadata": {},
   "source": [
    "Similarly, it is necessary to install the packages that contain the specific learning algorithms (e.g., LIBSVM, NearestNeighborModels, DecisionTree) as well as the packages that provide the interfaces between these algorithms and the MLJ framework (MLJLIBSVMInterface, MLJDecisionTreeInterface).\n",
    "To import the models to be used, we can rely on the `MLJ.@load` macro. For example, the following lines import the three models mentioned above, which will be used in this course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac8124b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "Pkg.add(\"MLJ\")\n",
    "Pkg.add(\"MLJBase\")\n",
    "Pkg.add(\"MLJLIBSVMInterface\");\n",
    "Pkg.add(\"NearestNeighborModels\");\n",
    "Pkg.add(\"MLJDecisionTreeInterface\");\n",
    "Pkg.add(\"CategoricalArrays\")\n",
    "Pkg.add(\"LIBSVM\"); using LIBSVM;\n",
    "using Random;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a935a0dc-d5b2-4880-b692-e338679d8346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLJDecisionTreeInterface.DecisionTreeClassifier"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SVMClassifier = MLJ.@load SVC pkg=LIBSVM verbosity=0\n",
    "kNNClassifier = MLJ.@load KNNClassifier pkg=NearestNeighborModels verbosity=0\n",
    "DTClassifier = MLJ.@load DecisionTreeClassifier pkg=DecisionTree verbosity=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0066a56-b180-4e1e-9ac0-153915e072d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "As can be seen, each model is loaded from a different package. The `verbosity=0` option is simply used to suppress the output message that would otherwise be printed during the import.\n",
    "This way, we define three functions to create each one of the three models. Each function receives as arguments the specific hyperparameters for the corresponding model.\n",
    "Below are three examples, one for each type of model that will be used in these course exercises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "286fcfa5-2185-4bd4-a80e-d75d23f1e9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(\n",
       "  kernel = LIBSVM.Kernel.RadialBasis, \n",
       "  gamma = 2.0, \n",
       "  cost = 1.0, \n",
       "  cachesize = 200.0, \n",
       "  degree = 3, \n",
       "  coef0 = 0.0, \n",
       "  tolerance = 0.001, \n",
       "  shrinking = true)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SVMClassifier(kernel=LIBSVM.Kernel.RadialBasis, cost=1.0, gamma=2.0, degree=Int32(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9004a1f-2ebb-4f1d-8142-ab113f0b3e9e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(\n",
       "  max_depth = 4, \n",
       "  min_samples_leaf = 1, \n",
       "  min_samples_split = 2, \n",
       "  min_purity_increase = 0.0, \n",
       "  n_subfeatures = 0, \n",
       "  post_prune = false, \n",
       "  merge_purity_threshold = 1.0, \n",
       "  display_depth = 5, \n",
       "  feature_importance = :impurity, \n",
       "  rng = MersenneTwister(1))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = DTClassifier(max_depth=4, rng=Random.MersenneTwister(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f6f400f-9e7f-400f-b30d-3582be8f145d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNNClassifier(\n",
       "  K = 3, \n",
       "  algorithm = :kdtree, \n",
       "  metric = Distances.Euclidean(0.0), \n",
       "  leafsize = 10, \n",
       "  reorder = true, \n",
       "  weights = NearestNeighborModels.Uniform())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = kNNClassifier(K=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f622c21e-8ad3-4152-a9c1-5f939ccc644f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "When creating a kNN model, the main hyperparameter is `K`, which defines the number of neighbors.\n",
    "For decision trees, the main hyperparameter is `max_depth`, which sets the maximum depth of the tree.\n",
    "In the case of decision trees, as shown earlier, there is also a parameter called `rng`. This parameter controls the randomness involved in a specific part of the model construction process.\n",
    "\n",
    "Specifically, for decision trees, this randomness occurs during the selection of features used to split a node. The `DecisionTree` library uses a random number generator (RNG) for this step, which is updated with each call. As a result, different calls to the function (along with subsequent calls to `fit!`) may produce different models, even with the same data.\n",
    "\n",
    "To control this randomness and make the process deterministic, it is advisable to provide a fixed integer value as the RNG seed, as shown in the previous example.\n",
    "This ensures that creating a model with a given input-output dataset and a defined set of hyperparameters becomes a reproducible process.\n",
    "\n",
    "In general, it is preferable to control randomness across the entire model development workflow (e.g., cross-validation, training/test splits) by setting a global random seed at the beginning.\n",
    "However, for the purposes of these exercises, we will use the `rng` keyword specifically for the decision tree model.\n",
    "\n",
    "SVMs have a more complex set of hyperparameters, which depend on the kernel function being used.  \n",
    "First, the hyperparameter `C` controls the trade-off between the margin width and classification error. Lower values allow for more misclassifications (more tolerance), while higher values fit the model more tightly to the data.  \n",
    "In MLJ, this parameter is passed using the keyword `cost` when calling `SVMClassifier`.\n",
    "\n",
    "Additionally, it is necessary to specify which kernel to use. This is done using the keyword `kernel`, which can take one of the following values provided by the `LIBSVM` library:\n",
    "\n",
    "- `LIBSVM.Kernel.Linear`\n",
    "- `LIBSVM.Kernel.RadialBasis`\n",
    "- `LIBSVM.Kernel.Sigmoid`\n",
    "- `LIBSVM.Kernel.Polynomial`\n",
    "\n",
    "Depending on the kernel selected, different additional hyperparameters are used:\n",
    "\n",
    "- **Linear kernel**: Only requires `C` (via `cost`).\n",
    "- **RBF (Radial Basis Function) kernel**: In addition to `C`, it uses `gamma`, which controls the influence of each support vector.\n",
    "- **Sigmoid kernel**: Uses `C`, `gamma`, and `coef0`. This kernel behaves similarly to a neural network, where `gamma` and `coef0` influence the shape of the decision function.\n",
    "- **Polynomial kernel**: Uses `C`, `degree` (the degree of the polynomial), `gamma`, and `coef0`.\n",
    "\n",
    "Typical values for these hyperparameters include:  \n",
    "`0.001`, `0.1`, `1`, `10`, `100`, `1000`.\n",
    "\n",
    "The following table summarises the different hyperparameters, the kernels that use them, and typical values they may take.\n",
    "\n",
    "Note that when calling the `SVMClassifier` function, the hyperparameters use the same names as listed here, except for `C`, which must be passed as `cost`, as shown in the previous example.\n",
    "\n",
    "It is also important that the arguments passed to the function have the correct type, as required by the `LIBSVM` library. Otherwise, an error may occur.\n",
    "\n",
    "To prevent this, it is recommended to explicitly cast each hyperparameter to the appropriate type when calling the function.\n",
    "\n",
    "| Hyperparameter | Applicable Kernels                 | Typical Values                     | Required Type in LIBSVM |\n",
    "|----------------|------------------------------------|------------------------------------|--------------------------|\n",
    "| `cost` (`C`)   | Linear, RBF, Sigmoid, Polynomial   | 0.001, 0.1, 1, 10, 100, 1000       | `Float64`               |\n",
    "| `gamma`        | RBF, Sigmoid, Polynomial           | 0.1, 0.01, 0.001, 0.0001           | `Float64`               |\n",
    "| `coef0`        | Sigmoid, Polynomial                | 0, 1, 5, 10                        | `Int32`                 |\n",
    "| `degree`       | Polynomial                         | 2, 3, 4, 5                         | `Float64`               |\n",
    "\n",
    "Although the basic SVM model is inherently binary, the implementation provided in MLJ already supports multi-class classification.  \n",
    "Therefore, it is not necessary to manually apply a one-vs-all strategy for multi-class problems.\n",
    "\n",
    "Once a model has been created, it must be wrapped in a `machine` object. This object acts as a container that associates the model with the data and handles both training and prediction.  \n",
    "It is a core concept in MLJ and simplifies model workflows by centralizing model fitting (`fit!`) and prediction (`predict`) logic.\n",
    "\n",
    "A `machine` has three main components:\n",
    "\n",
    "- **Model**: Specifies the algorithm to be used. It has been created earlier, without any data or learned state.\n",
    "- **Data**: Provides the input features and target labels (if supervised).\n",
    "- **Internal State**: Stores learned parameters after the model is trained.\n",
    "\n",
    "To create a `machine`, you can use the `machine` function, passing in the model, the input features, and the target labels.  \n",
    "Note that the input data must not be plain arrays. Instead, it should be converted to a supported table format such as `Tables.table`, `DataFrame`, or a `NamedTuple`.  \n",
    "If your data is currently stored as arrays, as is the case in these exercises, the following line shows how to construct the machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8cd0c3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module Utils.\n"
     ]
    }
   ],
   "source": [
    "using DelimitedFiles\n",
    "using Flux\n",
    "include(\"utils.jl\")\n",
    "using .Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73203665-5261-45d4-8ce4-59ce0ffdd53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes -> train: 90, val: 30, test: 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Float32[0.71428573 0.5333333 0.7457627 0.83333325; 0.5142856 -0.0666666 0.59322035 0.5833333; … ; 0.17142855 0.4666667 0.10169492 0.041666664; 0.19999994 0.8 0.0677966 0.083333336], Bool[0 0 1; 0 1 0; … ; 1 0 0; 1 0 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using DelimitedFiles\n",
    "using Random\n",
    "using CategoricalArrays\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "topology      = [4, 3]     # Hidden layers\n",
    "learningRate  = 0.01\n",
    "numMaxEpochs  = 1000\n",
    "Pval          = 0.2        # Validation split proportion\n",
    "Ptest         = 0.2        # Test split proportion\n",
    "rng           = MersenneTwister(1234) \n",
    "\n",
    "# -----------------------------\n",
    "# Load and parse the dataset\n",
    "# -----------------------------\n",
    "dataset = readdlm(\"./iris/iris.data\", ',')\n",
    "\n",
    "# Features (N×4) as Float32\n",
    "inputs = convert(Array{Float32,2}, dataset[:, 1:4])\n",
    "\n",
    "# Original string labels (N)\n",
    "labels = Vector{String}(dataset[:, 5])\n",
    "\n",
    "# Class list (kept for one-hot encoding with Utils)\n",
    "classes = unique(labels)\n",
    "\n",
    "# -----------------------------\n",
    "# One-hot targets (N×C, Bool) via Utils\n",
    "# -----------------------------\n",
    "targets_oh = Utils.oneHotEncoding(labels, classes)  # shape: N × C (Bool)\n",
    "\n",
    "# -----------------------------\n",
    "# Train/Validation/Test split (by indices) using Utils.holdOut\n",
    "# -----------------------------\n",
    "N = size(inputs, 1)\n",
    "train_idx, val_idx, test_idx = Utils.holdOut(N, Pval, Ptest)\n",
    "\n",
    "# Subset feature matrices\n",
    "train_inputs_raw = inputs[train_idx, :]\n",
    "val_inputs_raw   = inputs[val_idx,   :]\n",
    "test_inputs_raw  = inputs[test_idx,  :]\n",
    "\n",
    "# Subset one-hot targets (optional, for your ANN with Utils)\n",
    "train_targets_oh = targets_oh[train_idx, :]  # shape: N_train × C\n",
    "val_targets_oh   = targets_oh[val_idx,   :]  # shape: N_val × C\n",
    "test_targets_oh  = targets_oh[test_idx,  :]  # shape: N_test × C\n",
    "\n",
    "# -----------------------------\n",
    "# Targets as a categorical vector (required by MLJ/LIBSVM)\n",
    "#   - MLJ’s supervised models expect a 1D categorical target, not one-hot.\n",
    "# -----------------------------\n",
    "y_all         = categorical(labels)      # CategoricalVector{String}\n",
    "train_targets = y_all[train_idx]         # <- this is what Code B will use\n",
    "val_targets   = y_all[val_idx]\n",
    "test_targets  = y_all[test_idx]\n",
    "\n",
    "println(\"Sizes -> train: $(size(train_inputs_raw,1)), val: $(size(val_inputs_raw,1)), test: $(size(test_inputs_raw,1))\")\n",
    "\n",
    "# -----------------------------\n",
    "# Min–Max normalization with Utils\n",
    "# -----------------------------\n",
    "norm_params = Utils.calculateMinMaxNormalizationParameters(train_inputs_raw)\n",
    "\n",
    "train_inputs = Float32.(Utils.normalizeMinMax(train_inputs_raw, norm_params))  # <- used by Code B\n",
    "val_inputs   = Float32.(Utils.normalizeMinMax(val_inputs_raw,   norm_params))\n",
    "test_inputs  = Float32.(Utils.normalizeMinMax(test_inputs_raw,  norm_params))\n",
    "\n",
    "trainDS = (train_inputs, train_targets_oh)\n",
    "valDS   = (val_inputs,   val_targets_oh)\n",
    "testDS  = (test_inputs,  test_targets_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8d2ab75-c752-4497-ba35-22d028e61179",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "untrained Machine; caches model-specific representations of data\n",
       "  model: SVC(kernel = RadialBasis, …)\n",
       "  args: \n",
       "    1:\tSource @318 ⏎ Table{AbstractVector{Continuous}}\n",
       "    2:\tSource @146 ⏎ AbstractVector{Multiclass{3}}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define the model\n",
    "model = SVMClassifier(kernel=LIBSVM.Kernel.RadialBasis, cost=1.0, gamma=2.0, degree=Int32(3))\n",
    "# create the machine object\n",
    "mach = machine(model, MLJ.table(train_inputs), categorical(train_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6435ff-3ca4-4e90-98f0-e9e7ac9d79d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "As shown, the input matrix is converted into a table, and the target vector is converted into a categorical array, since this exercise involves classification problems.  \n",
    "\n",
    "It is important to note that the variable `targets` (the output labels) should be a **vector**, not a matrix. Each element in the vector corresponds to the label of one input sample and can be of any type (e.g., integer, string, etc.).\n",
    "\n",
    "Although some models may support one-hot encoded labels, others do not. Therefore, in these exercises, we will use a vector of labels, with one label per instance, rather than one-hot encoding (which is typically used in neural networks).\n",
    "\n",
    "To prevent compatibility issues with certain model implementations, we convert all label values to `String` before passing them to the model.\n",
    "\n",
    "Once the machine object has been created, the model can be trained using the `fit!` function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db6aa2ad-7df8-4d8c-af9b-0a60473a511d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trained Machine; caches model-specific representations of data\n",
       "  model: SVC(kernel = RadialBasis, …)\n",
       "  args: \n",
       "    1:\tSource @318 ⏎ Table{AbstractVector{Continuous}}\n",
       "    2:\tSource @146 ⏎ AbstractVector{Multiclass{3}}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MLJ.fit!(mach, verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be84f61-ec1e-4d43-9643-3adb74f2c94e",
   "metadata": {},
   "source": [
    "This function only requires the `machine` object as an argument, since the training data has already been bound to it.\n",
    "The optional argument `verbosity=0` is used to suppress output messages during training.\n",
    "\n",
    "### Question 6.1\n",
    "\n",
    "> ❓ What does the fact that the name of this function ends in bang (!) indicate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8b8524",
   "metadata": {},
   "source": [
    "In Julia, the exclamation mark (`!`) at the end of a function’s name is a naming convention that indicates the function modifies its argument(s) in place or has side effects.\n",
    "\n",
    "Therefore, in the case of `fit!(mach)`, the ! denotes that this function changes the internal state of the `machine` object — specifically, it performs the training process and stores the learned model parameters within the object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5151f1",
   "metadata": {},
   "source": [
    "Contrary to the Flux library, where it was necessary to write the ANN training loop, in this library the loop is already implemented, and it is called automatically when the `fit!` function is executed. Therefore, it is not necessary to write the code for the training loop.\n",
    "\n",
    "An important aspect to consider is the layout of the data to be used.  \n",
    "\n",
    "As shown in previous exercises, when training an Artificial Neural Network (ANN), the input samples (patterns) are arranged in **columns**, and each **row** in the input matrix represents a feature.\n",
    "\n",
    "However, outside the scope of ANNs — and therefore for all other techniques used in this course — it is assumed that the samples are arranged in **rows**, meaning each **column** in the input matrix corresponds to a feature. This format is generally more intuitive and will be used throughout the rest of the course.\n",
    "\n",
    "\n",
    "### Question 6.2\n",
    "\n",
    "> ❓ As in the case of ANNs, a loop is necessary for training several models. Where in the code (inside or outside the loop) will you need to create the model? Which models will need to be trained several times and which ones only once? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7437a0",
   "metadata": {},
   "source": [
    "The model should be **created outside the loop** as a reusable template that defines the algorithm and its hyperparameters, while the **machine object should be instantiated and trained inside the loop** for each iteration (for example, for every cross-validation fold or random seed). This is because `fit!` modifies the machine in place, and each training cycle must start from an unfitted state to ensure independent and unbiased results.\n",
    "\n",
    "Models that involve randomness or depend on data partitions — such as neural networks, stochastic algorithms, or models under cross-validation or hyperparameter tuning — must be trained **multiple times**, once for each iteration or configuration. In contrast, **deterministic models** with fixed hyperparameters (for example, logistic regression or SVMs) need to be trained **only once** on the full training data after model selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dc832c",
   "metadata": {},
   "source": [
    "### Question 6.3\n",
    "\n",
    "> ❓ Which condition must the matrix of inputs and the vector of desired outputs passed as an argument to this function fulfil?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ed4cd",
   "metadata": {},
   "source": [
    "The matrix of inputs (`X`) and the vector of desired outputs (`y`) must have **the same number of samples**. In other words, the **number of rows in the input matrix must match the length of the output vector**, since each row in `X` represents the set of features corresponding to a single training instance, and each element in `y` provides the target label for that same instance.\n",
    "\n",
    "Additionally, the data types must be consistent with the model’s requirements: the inputs should contain **continuous numerical values** (for example, `Float32` or `Float64`), and the output vector should be **categorical** in classification problems or **numeric** in regression problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a267fcc1-45d6-4b06-ae4c-dedea71c57b6",
   "metadata": {},
   "source": [
    "Finally, once the model has been trained, it can be used to make predictions. This is done using the `predict` function. The following is an example of how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "886ce4d7-835e-45bf-adca-163802d7690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "testOutputs = MLJ.predict(mach, MLJ.table(test_inputs));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b6a68-0c3e-4ad0-8eeb-d583e691b5eb",
   "metadata": {},
   "source": [
    "As shown, the `predict` function requires two arguments: the `machine` object and the input matrix, which must be converted to a table format.\n",
    "\n",
    "In classification problems, the type of the prediction result depends on the model and the underlying library:\n",
    "\n",
    "- **For SVMs**, `predict` returns a `CategoricalArray`, which can be directly compared with the ground truth labels. No post-processing is needed.\n",
    "- **For Decision Trees and kNN**, `predict` returns a `UnivariateFiniteArray`, which represents a probability distribution over the possible classes.  \n",
    "  To convert this into a single predicted label (so it can be compared with the true values), you can use the `mode` function to extract the most likely class.\n",
    "\n",
    "The model being used is stored in memory as a structured object with several fields, and it can be very useful to inspect its contents.  \n",
    "The `machine` object holds the model, the data, and the results of training. Therefore, you can access the trained model through the `machine`, or more directly through the variable `model`.\n",
    "\n",
    "For example, when training an SVM, you can access one of its hyperparameters in either of the following ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3696ef1c-191f-4966-9bc7-e2150cb393c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.gamma\n",
    "mach.model.gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0490a57e-8944-4f04-9740-b9fc248b270a",
   "metadata": {},
   "source": [
    "To inspect the learned parameters after training, MLJ provides several options.\n",
    "\n",
    "One particularly interesting case is with SVMs, where it is useful to check which instances were selected as support vectors.  \n",
    "This can be done in two ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12558b7d-ace7-49fa-895a-193e52f5df14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39-element Vector{Int32}:\n",
       "  1\n",
       "  3\n",
       "  6\n",
       "  8\n",
       " 12\n",
       " 15\n",
       " 18\n",
       " 26\n",
       " 32\n",
       " 33\n",
       "  ⋮\n",
       " 61\n",
       " 70\n",
       " 74\n",
       " 80\n",
       " 20\n",
       " 31\n",
       " 35\n",
       " 37\n",
       " 51"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mach.fitresult[1].SVs.indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef68586-6d89-4d24-baa4-81f3bb8a647d",
   "metadata": {},
   "source": [
    "or using the higher-level MLJ interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e6475af-37d1-430f-8b54-0956955a378d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39-element Vector{Int32}:\n",
       "  1\n",
       "  3\n",
       "  6\n",
       "  8\n",
       " 12\n",
       " 15\n",
       " 18\n",
       " 26\n",
       " 32\n",
       " 33\n",
       "  ⋮\n",
       " 61\n",
       " 70\n",
       " 74\n",
       " 80\n",
       " 20\n",
       " 31\n",
       " 35\n",
       " 37\n",
       " 51"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fitted_params(mach)[:libsvm_model].SVs.indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f10de28-9d84-4634-b0a1-7eb309d92900",
   "metadata": {},
   "source": [
    "These commands return the indices of the support vectors in the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef0987a-95ed-45fe-b197-c56a137b4694",
   "metadata": {},
   "source": [
    "In this notebook, the task will be to develop a single function that allows training the three different models using the MLJ library, and, in addition, artificial neural networks (ANNs) using the functions developed in previous exercises.\n",
    "\n",
    "The training will be performed using cross-validation. For each fold, the specified model will be trained, and metrics will be computed on the test set.\n",
    "\n",
    "As in the previous exercise, it is useful to generate a confusion matrix that reflects the distribution of instances across the test sets. In this case, it is simpler than before because the methods used are deterministic, so only one confusion matrix will be created per fold, and the final confusion matrix will be the sum of all of them.\n",
    "\n",
    "Nevertheless, the considerations from the previous exercise still apply — in particular, that the metrics derived from this global confusion matrix may not match the metrics obtained through cross-validation.\n",
    "\n",
    "In this exercise, you will develop a single function called `modelCrossValidation` that, in addition to training artificial neural networks (ANNs), performs cross-validation for SVMs, decision trees, and kNN.\n",
    "\n",
    "The function should receive the following arguments:\n",
    "\n",
    "- **`modelType::Symbol`**: This parameter indicates the type of model to train. It should take one of the following values:\n",
    "  - `:ANN` — Artificial Neural Network\n",
    "  - `:SVC` — Support Vector Machine\n",
    "  - `:DecisionTreeClassifier` — Decision Trees\n",
    "  - `:KNeighborsClassifier` — k-Nearest Neighbors\n",
    "\n",
    "- **`modelHyperparameters::Dict`**: A dictionary containing the model's hyperparameters. Keys may be of type `String` or `Symbol`.\n",
    "  \n",
    "  To check whether a hyperparameter is defined, you can use `haskey`.  \n",
    "  To retrieve a value that may or may not exist in the dictionary, the `get` function is also useful.\n",
    "\n",
    "  - **ANN (`:ANN`)**:\n",
    "        The expected Hyperparameters are\n",
    "        - Topology (number of hidden layers and number of neurons in each hidden layer, required) and transfer funtion in each layer. In \"shallow\" networks such as those used in this course, the transfer function has less impact, so a standard one, shuch as `tansig` or `logsig`, can be used.\n",
    "        - Learning rate\n",
    "        - Ratio of patterns used for validation\n",
    "        - Number of consecutive iterations without improving the validation loss to stop the process\n",
    "        - Number of times each ANN is trained.\n",
    "        \n",
    "### Question 6.4    \n",
    "> ❓ Why should a linear transfer function not be used for neurons in the hidden layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606c790d-d2ef-4044-87cf-1e999c5e57b0",
   "metadata": {},
   "source": [
    "A **linear transfer function** should not be used for neurons in the hidden layers because it does **not introduce nonlinearity** into the model. Without nonlinear activation functions, the entire network, regardless of how many hidden layers it has, would behave as a **single linear transformation** of its inputs. In other words, stacking multiple linear layers simply results in another linear function, providing no additional representational power.\n",
    "\n",
    "The purpose of hidden layers is to allow the network to learn **complex, nonlinear relationships** between the input features and the output targets. Nonlinear activation functions (such as sigmoid, tanh, or ReLU) enable the model to approximate arbitrary nonlinear mappings, which are essential for solving complex classification or regression tasks that cannot be captured by a purely linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecaf97a-afd8-4a4d-8027-5730da4efc7b",
   "metadata": {},
   "source": [
    "  For the other models, the expected hyperparameters are:\n",
    "\n",
    "  - **SVM (`:SVC`)**:  \n",
    "    The expected hyperparameters are:\n",
    "    - `C`\n",
    "    - `kernel`\n",
    "    - `degree`\n",
    "    - `gamma`\n",
    "    - `coef0`\n",
    "\n",
    "    The `kernel` parameter should be provided as a `String` with one of the following values:\n",
    "  `\"linear\"`, `\"rbf\"`, `\"sigmoid\"`, or `\"poly\"`.\n",
    "\n",
    "    Depending on the selected kernel, some of the hyperparameters may be ignored. For example:\n",
    "    - The `\"poly\"` kernel uses `degree`, `gamma`, and `coef0`.\n",
    "    - The `\"sigmoid\"` kernel uses `gamma` and `coef0`.\n",
    "    - The `\"linear\"` kernel only uses `C`.\n",
    "\n",
    "    The `C` hyperparameter must be passed using the keyword `cost`, and the kernel must be translated to one of the predefined constants in the `LIBSVM` library:\n",
    "\n",
    "    - `LIBSVM.Kernel.Linear`\n",
    "    - `LIBSVM.Kernel.RadialBasis`\n",
    "    - `LIBSVM.Kernel.Sigmoid`\n",
    "    - `LIBSVM.Kernel.Polynomial`\n",
    "\n",
    "    To avoid type errors, it is recommended to cast each value explicitly.  \n",
    "    For example, to create a polynomial SVM:\n",
    "\n",
    "  ```julia\n",
    "    model = SVMClassifier(\n",
    "        kernel = LIBSVM.Kernel.Polynomial,\n",
    "        cost = Float64(C),\n",
    "        gamma = Float64(gamma),\n",
    "        degree = Int32(degree),\n",
    "        coef0 = Float64(coef0)\n",
    "    )\n",
    "  ```\n",
    "\n",
    "  - **Decision Tree (`:DecisionTreeClassifier`)**:\n",
    "\n",
    "    - `max_depth`: defines the maximum depth of the tree.\n",
    "    - `rng`: the random seed generator. It should be set to `Random.MersenneTwister(1)` to ensure reproducibility.\n",
    "\n",
    "  - **k-Nearest Neighbors (`:KNeighborsClassifier`)**:\n",
    "    - `n_neighbors`: the value of k, which determines the number of neighbors to consider.\n",
    "\n",
    "- **`dataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{<:Any,1}}`**:  \n",
    "  A tuple containing two elements:\n",
    "  - The first is the input matrix (`X`). Unlike neural network training, there is no need to convert the data to `Float32`, since both `Float32` and `Float64` are commonly used in this library depending on the desired precision.\n",
    "  - The second is the target vector (`y`), which contains the labels.\n",
    "\n",
    "- **`crossValidationIndices::Array{Int64,1}`**:  \n",
    "  This vector contains the indices used to assign each sample to a fold in the cross-validation process.\n",
    "\n",
    "  As in the previous exercise, the fold assignment must be done **outside** the `modelCrossValidation` function.  \n",
    "  This ensures that the exact same data partitioning is used when training different models, allowing fair comparisons.\n",
    "\n",
    "\n",
    "The function will begin by checking whether the model to be trained is a neural network, by examining the `modelType` parameter.  \n",
    "If this is the case, it will call the `ANNCrossValidation` function, passing the hyperparameters provided in `modelHyperparameters`.\n",
    "\n",
    "Keep in mind that many of the hyperparameters for neural networks may not be defined in the dictionary.  \n",
    "As mentioned earlier, the function `haskey` can be used to check whether a key is present in a `Dict`.  \n",
    "Alternatively, the `get` function can be used to safely retrieve a value with a default if the key is missing.\n",
    "\n",
    "Once the call to `ANNCrossValidation` is made, the function returns its result and exits — meaning that no further processing will occur in this case.\n",
    "\n",
    "If a different type of model is to be trained, the logic continues similarly to the previous exercise:\n",
    "\n",
    "- Create seven vectors to store the results of the metrics for each fold.\n",
    "- Create a 2D array to accumulate the confusion matrix, initialized with zeros.\n",
    "\n",
    "A key modification when using models from the MLJ library is to **convert the target labels to strings** before training any model.  \n",
    "This helps prevent errors caused by internal type mismatches in some model implementations.\n",
    "\n",
    "This can be done with the following simple line:\n",
    "\n",
    "```julia\n",
    "targets = string.(targets);\n",
    "```\n",
    "\n",
    "Additionally, it will be necessary to compute the vector of unique classes, just like in the previous exercise.  \n",
    "This can be done with:\n",
    "\n",
    "```julia\n",
    "classes = unique(targets);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab6990e-eed7-4e30-9f27-756abc4e1b67",
   "metadata": {},
   "source": [
    "Once these initial steps are completed, the cross-validation loop can begin.\n",
    "\n",
    "In each iteration, the following steps are performed:\n",
    "\n",
    "1. Extract the training and test input matrices and the corresponding target vectors.  \n",
    "   These should be of type `AbstractArray{<:Any,1}` for the targets.\n",
    "\n",
    "2. Create the model with the specified hyperparameters.\n",
    "\n",
    "3. For MLJ models (SVM, Decision Tree, kNN):\n",
    "   - Instantiate the model using the appropriate constructor: `SVMClassifier`, `DTClassifier`, or `kNNClassifier`, depending on `modelType`.\n",
    "   - Wrap the model in a `machine` with the training data.\n",
    "   - Train the model using `fit!`.\n",
    "\n",
    "4. Perform predictions on the test data using `predict`.\n",
    "\n",
    "   - For Decision Trees and kNN, use `mode` to convert the probabilistic predictions into categorical labels:\n",
    "     ```julia\n",
    "     ŷ = mode.(predict(mach, MLJ.table(Xtest)))\n",
    "     ```\n",
    "\n",
    "   - For SVMs, the output of `predict` can be compared directly with the ground truth, since it returns a `CategoricalArray`.\n",
    "\n",
    "Although the general structure of the code will be the same for the three model types, each model requires a different constructor and may require post-processing (e.g., `mode`) depending on the prediction format.\n",
    "\n",
    "Once the predicted labels for the test set are available, the evaluation metrics and the confusion matrix should be computed using the `confusionMatrix` function.\n",
    "\n",
    "- The metrics returned should be stored in their respective positions within the metric vectors.\n",
    "- The confusion matrix obtained for each fold should be **added** to a global confusion matrix for the test set.\n",
    "\n",
    "A key difference compared to the ANN training in the previous exercise is that these models (SVM, Decision Tree, kNN) are **deterministic**.  \n",
    "Therefore, each model only needs to be trained **once per fold**, without requiring multiple executions or averaging across runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0f9e3d-c590-49c8-968a-eb2d8e27f0b7",
   "metadata": {},
   "source": [
    "   ### Question 6.5\n",
    "   > ❓ The other models do not have the number of times to train them as a parameter. Why? If you train several times, Which statistical properties will the results of these trainings have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229dc093-08a0-41e5-9f35-47ef962694e9",
   "metadata": {},
   "source": [
    "Unlike ANNs, the other models such as SVMs, Decision Trees, and k-Nearest Neighbors are **deterministic** algorithms. This means that, given the same training data and hyperparameters, they will always produce the same model and the same predictions, there is no randomness involved in their optimization or initialization process. Consequently, there is no need to repeat their training multiple times, as doing so would yield identical results.\n",
    "\n",
    "If these models were trained multiple times under identical conditions, the results of their metrics would exhibit **zero variance**, since the outcomes would be perfectly reproducible. In contrast, repeating the training of stochastic models like ANNs allows one to estimate the **statistical variability** of their performance, providing a measure of the model’s stability and sensitivity to random initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedae31a-0e8c-4b41-a10b-934e4b6af791",
   "metadata": {},
   "source": [
    "As previously described, when using techniques such as SVM, decision trees, kNN, or DoME, **one-hot encoding is not used**.  \n",
    "Instead, metrics are computed using the `confusionMatrix` function developed in a previous exercise, which takes three arguments:\n",
    "- The predicted labels\n",
    "- The true labels\n",
    "- The list of class labels\n",
    "\n",
    "All of these must be of type `AbstractArray{<:Any,1}`.\n",
    "\n",
    "It is important to use the version of the `confusionMatrix` function that receives the vector of classes.\n",
    "\n",
    "### Question 6.6\n",
    "\n",
    "> ❓ What could happen if the version that does not receive the class vector is used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1434e675-c83c-48d1-9939-8bd784178170",
   "metadata": {},
   "source": [
    "If the version of the `confusionMatrix` function that does **not** receive the explicit class vector is used, the function will automatically infer the set of classes by combining the labels found in the predicted and true output vectors. This can lead to **inconsistent or incorrect class ordering**, especially when some classes are missing in the predictions or in a particular test fold.\n",
    "\n",
    "As a result, the confusion matrix may have **misaligned rows and columns**, meaning that the counts of true and predicted labels would not correspond to the same class positions. This would produce incorrect metric calculations (such as accuracy, precision, or recall) and potentially misleading interpretations of model performance. Providing the explicit list of classes ensures that the confusion matrix maintains a consistent and correct class ordering across all folds and models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a078f16-01c0-4719-9aef-96f19a751533",
   "metadata": {},
   "source": [
    "The `modelCrossValidation` function must return the same structure as in the previous exercise: a tuple with 8 elements.\n",
    "\n",
    "- The first **7 elements** correspond to metrics: **accuracy**, **error rate**, **recall**, **specificity**, **PPV**, **NPV**, and **F1-score**.  \n",
    "  Each of these is itself a tuple with the **mean** and **standard deviation** across the folds.\n",
    "\n",
    "- The **8th element** is the **global confusion matrix** computed on the test sets.\n",
    "\n",
    "Once the function has been developed, it can be used to evaluate different model configurations by comparing test results across the selected metrics.  \n",
    "This process does **not return a final model ready for production**, but rather identifies the best-performing model type and hyperparameter configuration.\n",
    "\n",
    "After selecting the best configuration, the final model should be trained **from scratch** using **all available data**, without performing cross-validation.  \n",
    "This training is done just once, without setting aside a test set.  \n",
    "As a result, the final model is expected to perform slightly better than during cross-validation, since it benefits from more training data.\n",
    "\n",
    "This final model is the one intended for production use, and a confusion matrix can be computed for it as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15eecf3-286c-40ee-9ed7-2920c62e4f3a",
   "metadata": {},
   "source": [
    "### Question 6.7\n",
    "\n",
    "> ❓ In the case of using decision trees or kNN, a corresponding function is not necessary to perform the \"one-against-all\" strategy, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e501314",
   "metadata": {},
   "source": [
    "In the case of Decision Trees and k-Nearest Neighbors (kNN), a separate “one-against-all” strategy is unnecessary because these algorithms are **inherently multiclass classifiers**. Both methods are designed to handle multiple classes natively, Decision Trees split the data recursively based on feature values to directly assign samples to one of several possible classes, while kNN determines the class of a sample by **majority voting** among its *k* nearest neighbors, which can belong to any of the available classes.\n",
    "\n",
    "In contrast, models such as Support Vector Machines (SVMs) were originally formulated for **binary classification**, separating only two classes with a single hyperplane. Extending SVMs to multiclass problems requires strategies like “one-against-all” or “one-against-one,” which combine multiple binary classifiers to make a final multiclass prediction. Decision Trees and kNN do not require such decomposition because they naturally operate with any number of class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "17e6c492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using MLJBase.classes in module Main conflicts with an existing identifier.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "safe_predict_labels (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Random, Statistics\n",
    "using MLJBase\n",
    "using MLJBase: UnivariateFinite\n",
    "using CategoricalArrays\n",
    "using LIBSVM \n",
    "using .Utils\n",
    "\n",
    "# --- MLJ model wrappers (do NOT use raw LIBSVM.SVC directly) ---\n",
    "const SVCModel = @load SVC pkg=LIBSVM verbosity=0\n",
    "const DecisionTreeClassifier = @load DecisionTreeClassifier pkg=DecisionTree verbosity=0\n",
    "const KNNModel = @load KNNClassifier pkg=NearestNeighborModels verbosity=0\n",
    "\n",
    "# --- Always return hard labels as Vector{String}, regardless of model output ---\n",
    "function safe_predict_labels(mach, Xnew)::Vector{String}\n",
    "    yhat = MLJ.predict(mach, MLJ.table(Xnew))\n",
    "    if yhat isa AbstractVector{<:UnivariateFinite}\n",
    "        return String.(mode.(yhat))\n",
    "    elseif yhat isa AbstractVector\n",
    "        return String.(yhat)\n",
    "    else\n",
    "        error(\"Unexpected prediction type: $(typeof(yhat))\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e3bed5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modelCrossValidation (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function modelCrossValidation(\n",
    "    modelType::Symbol, modelHyperparameters::Dict,\n",
    "    dataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{<:Any,1}},\n",
    "    crossValidationIndices::Vector{Int}\n",
    ")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Unpack and basic validation\n",
    "    # ---------------------------\n",
    "    X_raw, y_raw = dataset\n",
    "    N = size(X_raw, 1)\n",
    "    @assert length(y_raw) == N \"Length of y must match number of rows in X.\"\n",
    "    @assert length(crossValidationIndices) == N \"CV index length must match N.\"\n",
    "    numFolds = maximum(crossValidationIndices)\n",
    "    @assert minimum(crossValidationIndices) ≥ 1 \"Fold indices must start at 1.\"\n",
    "    @assert all(in(1:numFolds), crossValidationIndices) \"Fold indices must be in 1..numFolds.\"\n",
    "\n",
    "    # Canonical types for MLJ backends\n",
    "    X = Array{Float64}(X_raw)\n",
    "    y_str = string.(y_raw)\n",
    "    classes = unique(y_str)\n",
    "    C = length(classes)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # ANN branch: delegate and return directly\n",
    "    # ----------------------------------------\n",
    "    if modelType == :ANN\n",
    "        topology        = get(modelHyperparameters, :topology, nothing)\n",
    "        @assert topology !== nothing \"For :ANN you must provide :topology (hidden layer sizes).\"\n",
    "\n",
    "        transferFuncs   = get(modelHyperparameters, :transferFunctions, nothing)\n",
    "        maxEpochs       = get(modelHyperparameters, :maxEpochs,       200)\n",
    "        minLoss         = get(modelHyperparameters, :minLoss,         0.0)\n",
    "        learningRate    = get(modelHyperparameters, :learningRate,    1e-3)\n",
    "        validationRatio = get(modelHyperparameters, :validationRatio, 0.0)\n",
    "        maxEpochsVal    = get(modelHyperparameters, :maxEpochsVal,    25)\n",
    "        numExecutions   = get(modelHyperparameters, :numExecutions,   1)\n",
    "\n",
    "        return ANNCrossValidation(\n",
    "            topology,\n",
    "            (X, y_str),\n",
    "            crossValidationIndices;\n",
    "            numExecutions     = numExecutions,\n",
    "            transferFunctions = transferFuncs,\n",
    "            maxEpochs         = maxEpochs,\n",
    "            minLoss           = minLoss,\n",
    "            learningRate      = learningRate,\n",
    "            validationRatio   = validationRatio,\n",
    "            maxEpochsVal      = maxEpochsVal\n",
    "        )\n",
    "    end\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # Non-ANN models: accumulators (once per fold)\n",
    "    # ----------------------------------------------------\n",
    "    acc_f  = zeros(Float64, numFolds)\n",
    "    err_f  = zeros(Float64, numFolds)\n",
    "    sens_f = zeros(Float64, numFolds)\n",
    "    spec_f = zeros(Float64, numFolds)\n",
    "    ppv_f  = zeros(Float64, numFolds)\n",
    "    npv_f  = zeros(Float64, numFolds)\n",
    "    f1_f   = zeros(Float64, numFolds)\n",
    "    globalCM = zeros(Float64, C, C)\n",
    "\n",
    "    # ---------------------\n",
    "    # Helpers: model build\n",
    "    # ---------------------\n",
    "    _kernel_of = function (kstr::AbstractString)\n",
    "        ks = lowercase(strip(kstr))\n",
    "        if     ks == \"linear\"   ; return LIBSVM.Kernel.Linear\n",
    "        elseif ks == \"rbf\"      ; return LIBSVM.Kernel.RadialBasis\n",
    "        elseif ks == \"sigmoid\"  ; return LIBSVM.Kernel.Sigmoid\n",
    "        elseif ks == \"poly\" || ks == \"polynomial\"\n",
    "                                 return LIBSVM.Kernel.Polynomial\n",
    "        else\n",
    "            error(\"Unknown SVM kernel: '$kstr'. Expected: linear | rbf | sigmoid | poly.\")\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # --------------------------\n",
    "    # Cross-validation main loop\n",
    "    # --------------------------\n",
    "    for fold in 1:numFolds\n",
    "        test_mask  = crossValidationIndices .== fold\n",
    "        train_mask = .!test_mask\n",
    "\n",
    "        Xtr, Xte = X[train_mask, :], X[test_mask, :]\n",
    "        ytr_str, yte_str = y_str[train_mask], y_str[test_mask]\n",
    "\n",
    "        # 1) Build and fit the model for this fold (one fit per fold)\n",
    "        if modelType == :SVC\n",
    "            # Hyperparameters (with defaults)\n",
    "            Cval   = Float64(get(modelHyperparameters, :C,      1.0))\n",
    "            gamma  = Float64(get(modelHyperparameters, :gamma,  0.0))   # 0 => 1/n_features in LIBSVM\n",
    "            degree = Int32(get(modelHyperparameters, :degree,   3))     # Int32 required by LIBSVM\n",
    "            coef0  = Float64(get(modelHyperparameters, :coef0,  0.0))\n",
    "            kern_s = String(get(modelHyperparameters, :kernel, \"rbf\"))\n",
    "            kern   = _kernel_of(kern_s)\n",
    "\n",
    "            model  = SVCModel(kernel = kern, cost = Cval, gamma = gamma, degree = degree, coef0 = coef0)\n",
    "            mach   = machine(model, MLJ.table(Xtr), categorical(ytr_str))\n",
    "            MLJ.fit!(mach; verbosity=0)\n",
    "\n",
    "            yhat = safe_predict_labels(mach, Xte)\n",
    "\n",
    "        elseif modelType == :DecisionTreeClassifier\n",
    "            max_depth = get(modelHyperparameters, :max_depth, nothing)\n",
    "            rng_dt    = get(modelHyperparameters, :rng, Random.MersenneTwister(1))\n",
    "\n",
    "            model = DecisionTreeClassifier(max_depth = max_depth, rng = rng_dt)\n",
    "            mach  = machine(model, MLJ.table(Xtr), categorical(ytr_str))\n",
    "            MLJ.fit!(mach; verbosity=0)\n",
    "\n",
    "            yhat = safe_predict_labels(mach, Xte)\n",
    "\n",
    "        elseif modelType == :KNeighborsClassifier\n",
    "            k = Int(get(modelHyperparameters, :n_neighbors, 5))\n",
    "\n",
    "            model = KNNModel(K = k)\n",
    "            mach  = machine(model, MLJ.table(Xtr), categorical(ytr_str))\n",
    "            MLJ.fit!(mach; verbosity=0)\n",
    "\n",
    "            yhat = safe_predict_labels(mach, Xte)\n",
    "\n",
    "        else\n",
    "            error(\"Unknown modelType: $(modelType). Expected :ANN | :SVC | :DecisionTreeClassifier | :KNeighborsClassifier\")\n",
    "        end\n",
    "\n",
    "        # 2) Evaluate on the test subset using explicit class list (Utils)\n",
    "        acc, err, sens, spec, ppv, npv, f1, CM = Utils.confusionMatrix(yhat, yte_str, classes)\n",
    "\n",
    "        acc_f[fold]  = acc\n",
    "        err_f[fold]  = err\n",
    "        sens_f[fold] = sens\n",
    "        spec_f[fold] = spec\n",
    "        ppv_f[fold]  = ppv\n",
    "        npv_f[fold]  = npv\n",
    "        f1_f[fold]   = f1\n",
    "\n",
    "        globalCM .+= Float64.(CM)\n",
    "    end\n",
    "\n",
    "    # 3) Aggregate across folds (mean ± std) and return\n",
    "    return (\n",
    "        (mean(acc_f),  std(acc_f)),\n",
    "        (mean(err_f),  std(err_f)),\n",
    "        (mean(sens_f), std(sens_f)),\n",
    "        (mean(spec_f), std(spec_f)),\n",
    "        (mean(ppv_f),  std(ppv_f)),\n",
    "        (mean(npv_f),  std(npv_f)),\n",
    "        (mean(f1_f),   std(f1_f)),\n",
    "        globalCM\n",
    "    )\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7894b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== K-Fold Cross-Validation on Iris Dataset (k = 5) — SVC ===\n",
      "Accuracy:   0.96 ± 0.0149\n",
      "Error:      0.04 ± 0.0149\n",
      "Sensitivity:0.96 ± 0.0149\n",
      "Specificity:0.98 ± 0.0075\n",
      "PPV:        0.9646 ± 0.0113\n",
      "NPV:        0.9812 ± 0.0065\n",
      "F1-Score:   0.9598 ± 0.0152\n",
      "\n",
      "Global Confusion Matrix (sum over folds):\n",
      "3×3 Matrix{Float64}:\n",
      " 50.0   0.0   0.0\n",
      "  0.0  49.0   1.0\n",
      "  0.0   5.0  45.0\n"
     ]
    }
   ],
   "source": [
    "using DelimitedFiles\n",
    "\n",
    "# 1) Load the dataset (CSV with 5 columns: 4 numeric + 1 label)\n",
    "dataset = readdlm(\"iris/iris.data\", ',')\n",
    "\n",
    "# 2) Separate features (X) and class labels (targets)\n",
    "X = Float64.(dataset[:, 1:4])            # 150×4 matrix of features\n",
    "targets = Vector{String}(dataset[:, 5])  # 150 labels (Setosa, Versicolor, Virginica)\n",
    "\n",
    "# 3) Normalize each feature column to [0, 1] using Utils\n",
    "Xn = Utils.normalizeMinMax(X)\n",
    "@assert all(minimum(Xn, dims=1) .≈ 0.0)\n",
    "@assert all(maximum(Xn, dims=1) .≈ 1.0)\n",
    "\n",
    "# 4) Generate stratified fold indices for 5-fold cross-validation using Utils\n",
    "k = 5\n",
    "rng = Random.MersenneTwister(123)  # reproducibility for the fold assignment\n",
    "cv_idx = Utils.stratified_kfold_indices(targets, k; rng=rng)\n",
    "\n",
    "# 5) Define model type and hyperparameters, then run cross-validation\n",
    "modelType = :SVC\n",
    "modelHyperparameters = Dict(\n",
    "    :C       => 1.0,\n",
    "    :kernel  => \"rbf\",    # \"linear\" | \"rbf\" | \"sigmoid\" | \"poly\"\n",
    "    :degree  => 3,        # cast to Int32 inside the function\n",
    "    :gamma   => 0.25,     # ~ 1 / n_features\n",
    "    :coef0   => 0.0\n",
    ")\n",
    "\n",
    "result = modelCrossValidation(\n",
    "    modelType,\n",
    "    modelHyperparameters,\n",
    "    (Xn, targets),\n",
    "    cv_idx\n",
    ")\n",
    "\n",
    "# 6) Unpack results and display metrics\n",
    "((meanAcc,  stdAcc),\n",
    " (meanErr,  stdErr),\n",
    " (meanSens, stdSens),\n",
    " (meanSpec, stdSpec),\n",
    " (meanPPV,  stdPPV),\n",
    " (meanNPV,  stdNPV),\n",
    " (meanF1,   stdF1),\n",
    " globalCM) = result\n",
    "\n",
    "println(\"=== K-Fold Cross-Validation on Iris Dataset (k = $k) — $(modelType) ===\")\n",
    "println(\"Accuracy:   $(round(meanAcc, digits=4)) ± $(round(stdAcc, digits=4))\")\n",
    "println(\"Error:      $(round(meanErr, digits=4)) ± $(round(stdErr, digits=4))\")\n",
    "println(\"Sensitivity:$(round(meanSens, digits=4)) ± $(round(stdSens, digits=4))\")\n",
    "println(\"Specificity:$(round(meanSpec, digits=4)) ± $(round(stdSpec, digits=4))\")\n",
    "println(\"PPV:        $(round(meanPPV, digits=4)) ± $(round(stdPPV, digits=4))\")\n",
    "println(\"NPV:        $(round(meanNPV, digits=4)) ± $(round(stdNPV, digits=4))\")\n",
    "println(\"F1-Score:   $(round(meanF1, digits=4)) ± $(round(stdF1, digits=4))\")\n",
    "\n",
    "println(\"\\nGlobal Confusion Matrix (sum over folds):\")\n",
    "show(stdout, \"text/plain\", round.(globalCM, digits=2))\n",
    "println()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a7ded7",
   "metadata": {},
   "source": [
    "### Learn Julia\n",
    "\n",
    "#### Symbols and Dictionaries in Julia\n",
    "One Julia type that is important for this exercise is the `Symbol` type. An object of this type can be any symbol you want, simply by typing its name after a colon (\":\"). In this practice, you can use it to indicate which model you want to train, for example, in the `modelCrossValidation` function, symbols will be used to indicate which model to train:\n",
    "\n",
    "```julia\n",
    ":KNeighborsClassifier, :SVC, :DecisionTreeClassifier, :ANN\n",
    "```\n",
    "\n",
    "#### Passing Model-Specific Parameters\n",
    "This function will also require model-specific parameters to be passed.\n",
    "The recommended way to do this is to define a variable of type Dict, which works similarly to Python dictionaries.\n",
    "\n",
    "For instance, to define the hyperparameters for an artificial neural network (ANN):\n",
    "\n",
    "```julia\n",
    "  modelHyperparameters = Dict(\n",
    "      \"topology\" => [5, 3],\n",
    "      \"learningRate\" => 0.01,\n",
    "      \"validationRatio\" => 0.2,\n",
    "      \"numExecutions\" => 50,\n",
    "      \"maxEpochs\" => 1000,\n",
    "      \"maxEpochsVal\" => 6\n",
    "  )\n",
    "```\n",
    "\n",
    "Another way to define the same dictionary:\n",
    "\n",
    "```julia\n",
    "  modelHyperparameters = Dict()\n",
    "  modelHyperparameters[\"topology\"] = topology\n",
    "  modelHyperparameters[\"learningRate\"] = learningRate\n",
    "  modelHyperparameters[\"validationRatio\"] = validationRatio\n",
    "  modelHyperparameters[\"numExecutions\"] = numRepetitionsANNTraining\n",
    "  modelHyperparameters[\"maxEpochs\"] = numMaxEpochs\n",
    "  modelHyperparameters[\"maxEpochsVal\"] = maxEpochsVal\n",
    "```\n",
    "To access a value, simply use:\n",
    "```julia\n",
    "  modelHyperparameters[\"topology\"]\n",
    "```\n",
    "#### Example for SVM Parameters\n",
    "You can also define hyperparameters for other models similarly.\n",
    "For example, for an SVM:\n",
    "```julia\n",
    "  modelHyperparameters = Dict(\"C\" => 1, \"kernel\" => \"rbf\", \"gamma\" => 2)\n",
    "```\n",
    "Or using the alternative form:\n",
    "\n",
    "```julia\n",
    "  modelHyperparameters = Dict()\n",
    "  modelHyperparameters[\"C\"] = 1\n",
    "  modelHyperparameters[\"kernel\"] = \"rbf\"\n",
    "  modelHyperparameters[\"gamma\"] = 2\n",
    "```\n",
    "Other kernels may require different parameters, such as `degree` and `coef0`.\n",
    "\n",
    "When building the SVM model inside the function, you might write:\n",
    "```julia\n",
    "  if modelHyperparameters[\"kernel\"] == \"rbf\"\n",
    "    model = SVMClassifier(\n",
    "        kernel = LIBSVM.Kernel.RadialBasis,\n",
    "        cost = Float64(modelHyperparameters[\"C\"]),\n",
    "        gamma = Float64(modelHyperparameters[\"gamma\"])\n",
    "    )\n",
    "```\n",
    "\n",
    "You can apply a similar strategy for decision trees, kNN, and DoME models.\n",
    "\n",
    "In the examples above, the dictionary keys are `String`, but you may also use `Symbol` keys interchangeably.\n",
    "For example:\n",
    "```julia\n",
    "  modelHyperparameters = Dict(:C => 1, :kernel => \"rbf\", :gamma => 2)\n",
    "```\n",
    "\n",
    "Another type of Julia that may be interesting for this assignment is the `Symbol` type. An object of this type can be any symbol you want, simply by typing its name after a colon (\":\"). In this practice, you can use it to indicate which model you want to train, for example `:ANN`, `:SVM`, `:DecisionTree` or `:kNN`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.7",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
