{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "923c9266",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Cross-validation\n",
    "\n",
    "With the code developed so far, it is possible to train an ANN and provide an estimate of the results it would offer in its real execution (with unseen patterns, represented by a test set). However, in this last aspect there are two factors to consider, as a consequence of the non-deterministic nature of the process we are following:\n",
    "\n",
    "- The partitioning of the set of patterns into training/test is random (hold out), and is therefore overly dependent on good or bad luck in choosing training and test patterns.\n",
    "- ANN training is not deterministic, as the initialisation of the weights is random. As before, it is too dependent on good or bad luck to start the training at a good or bad starting point.\n",
    "\n",
    "For these two reasons, the test result of a single training is not significant when assessing the goodness of fit of the model in the presence of unseen patterns. To solve this problem, the experiment is repeated several times and the results are averaged. This can be implemented in a simple way by means of a loop; however, it is necessary to do this in an orderly way as there are two different sources of randomness.\n",
    "\n",
    "Firstly, to minimise the randomness due to the partitioning of the data set, it is necessary to have a method that ensures that each data is used for training at least once, and for testing at least once. The most commonly used method is cross-validation. In this method, the data set is split into k disjoint subsets and k experiments are performed. In the k-th experiment, the k subset is separated for testing, and the remaining k-1 substes are used for training, performing a k-fold cross-validation. A common value is k=10, which gives a 10-fold cross-validation. Finally, the test value corresponding to the appropriate metric will be the average value of the values of the k experiments.\n",
    "\n",
    "A widely used variant of cross-validation is stratified cross-validation. In this case, each subset is created in such a way as to keep the proportion of patterns of each class the same (or similar) as in the original dataset. This is particularly used when the data set is imbalanced.\n",
    "\n",
    "It is usual to save not only the mean, but also the k values, in order to subsequently perform a paired hypothesis test with another model. To do this, it is necessary that both models have been trained using the same training and test sets.\n",
    "\n",
    "This way of evaluating the model is often considered to be slightly pessimistic, i.e. the results obtained in tests are slightly worse than those that would be obtained from real training with all available data. In a hold out experiment, as mentioned above, several data are separated for testing. This means that the model is trained with less data than is available, and that by chance the data separated for testing can be of great importance (especially if there is little data). For this reason, when training with less data and possibly no \"important\" data, hold out is considered a pessimistic assessment. In the same way, cross-validation also separates data for testing, so it does not train on all available data, and is therefore also pessimistic. However, it is guaranteed that all data are used at least once in training and once in testing, thus trying to minimise the impact of chance in separating data, so it is considered only a slightly pessimistic evaluation.\n",
    "\n",
    "Doing this is as simple as splitting the data set and performing a loop with k iterations in which at the k-th iteration a model is trained and evaluated with the corresponding sets. However, if the model is not deterministic, the result obtained at the k-th iteration will not be meaningful, since it is again dependent on chance. In this case, what needs to be done is a second nested loop within iteration k in which the model is repeatedly trained, and finally an average of the results is made to finally output the result of iteration k. The number of trainings must be high for the average results to be really significant, at least 50 trainings.\n",
    "\n",
    "### Question 5.1\n",
    "\n",
    "> ❓ If this second loop is performed with a deterministic model, what will be the standard deviation of the test results obtained? Is there a difference between performing this second loop and averaging the results, or doing a single training?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0294d11a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If the model is deterministic, then for the same training and test sets each run will produce exactly the same result. Therefore:\n",
    "\n",
    "- **Standard deviation:** **0** (all inner repetitions on the same fold will yield the same value).\n",
    "- **Difference compared to doing a single training:** **None.** Averaging identical results gives the same number, so repeating the inner loop adds no benefit.\n",
    "\n",
    "This second loop is only useful when the model is **non-deterministic**, as in the case of neural networks with random weight initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af15c6a2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this way, it is possible to evaluate a model together with its hyperparameters in solving a problem. A very common situation is to compare several models (or the same model with different hyperparameters), for which this scheme has to be applied with an important caveat: the sets used in the cross-validation must be the same for each model. Since the distribution of patterns in different sets is random, having the same subsets in different runs is achieved by setting the random seed at the beginning of the program to be executed. Setting the random seed not only allows the same subsets to be generated, but is also important in order to be able to repeat the results in different runs.\n",
    "\n",
    "It is also important to bear in mind that this methodology allows estimating the real performance of a model (although slightly pessimistic). The final model that would be used in production would be the result of training it with all the available patterns, since, as seen in the theory class, and very generally speaking, the more patterns you train with, the better the model will be.\n",
    "\n",
    "Another commonly requested task is to obtain a **confusion matrix on the test set** as a result of performing **cross-validation**.\n",
    "\n",
    "In general, this is **not directly possible**, because multiple neural networks are trained for each fold.  \n",
    "If only a single model were trained per fold (as will be the case in the next exercise), it would be possible to compute a confusion matrix for each test set and then sum all of them to obtain a **global test confusion matrix**.\n",
    "\n",
    "However, since multiple models are trained in each fold here, we use an alternative approach:\n",
    "\n",
    "- For each fold, a **confusion matrix is computed for every execution**.\n",
    "- These matrices are then **averaged within each fold**.\n",
    "- Finally, the averaged matrices from each fold are **summed** to produce a **global confusion matrix**.\n",
    "\n",
    "---\n",
    "\n",
    "### ⚠️ Important considerations\n",
    "\n",
    "- This global confusion matrix **does not represent the result of any single model**.  \n",
    "  Therefore, calling it a \"confusion matrix\" can be misleading — but it is still useful to analyze how instances are distributed in the test sets.\n",
    "\n",
    "- For the same reason, the **metrics derived from this matrix** will generally **not match** the metric values returned by the function (which are averages of the metric scores for each fold).  \n",
    "  These matrices are meant to provide **a visual and aggregated summary**, while the true evaluation metrics are the ones computed per fold.\n",
    "\n",
    "- As a result of averaging the confusion matrices, the **final confusion matrix will contain real numbers**, not integers.  \n",
    "  While this is technically not correct, it is acceptable to include it in a report — as long as the nature of the matrix is clearly explained.\n",
    "\n",
    "In this assignment, you are asked to:\n",
    "\n",
    "1. Develop a function called `crossvalidation` that receives a value `N` (equal to the number of patterns), and a value `k` (number of subsets into which the dataset is to be split), and returns a vector of length N, where each element indicates in which subset that pattern should be included.\n",
    "\n",
    "    To perform this function, one possibility is to perform the following steps:\n",
    "    \n",
    "    - Create a vector with k sorted elements, from 1 to k.\n",
    "    - Create a new vector with repetitions of the previous vector until its length is greater than or equal to N. The functions `repeat` and `ceil` can be used for this purpose.\n",
    "    - Take the first N values of this vector.\n",
    "    - Shuffle this vector (using the function `shuffle!` and return it. To use this function, the module `Random` should be loaded.\n",
    "    \n",
    "    This function should return a vector of **integer values**, with a length equal to **N**. No loop should be used in this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf859a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module Utils.\n"
     ]
    }
   ],
   "source": [
    "include(\"utils.jl\")\n",
    "using .Utils\n",
    "using Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35def6db",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crossvalidation (generic function with 4 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Random\n",
    "\n",
    "function crossvalidation(N::Int64, k::Int64)\n",
    "    # Create vector 1:k\n",
    "    base = collect(1:k)\n",
    "    \n",
    "    # Repeat it enough times to cover N elements\n",
    "    reps = ceil(Int, N / k)\n",
    "    v = repeat(base, reps)\n",
    "    \n",
    "    # Take the first N elements\n",
    "    v = v[1:N]\n",
    "    \n",
    "    # Shuffle in place\n",
    "    shuffle!(v)\n",
    "    \n",
    "    return v\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b680d6ac-fe8d-4506-beef-6a267c60bf8f",
   "metadata": {},
   "source": [
    "2. Develop a new function called `crossvalidation` that receives:\n",
    "\n",
    "- A vector `targets::AbstractArray{Bool,1}` containing the desired outputs (ground truth labels),\n",
    "- An integer `k`, which is the number of folds for the partitioning.\n",
    "\n",
    "The function must return a vector of **integer values** of length **N** (where `N` is the number of elements in `targets`), indicating to which subset (fold) each instance belongs.\n",
    "\n",
    "The partitioning must be **stratified**, meaning that **the distribution of positive and negative instances is preserved across folds**. \n",
    "Follow these steps:\n",
    "\n",
    "    1. Create a vector of indices with as many elements as there are rows in `targets`.\n",
    "    2. Call the previously developed `crossvalidation` function, passing the **number of positive instances** and the value of `k`.\n",
    "    3. Assign the result of the previous step to the positions of the index vector that correspond to **positive instances**.\n",
    "  ### Question 5.2\n",
    "  > ❓ Could you combine steps 2 and 3 into a single line? (This is not required, but worth considering.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e0399b-6e0b-477b-ba85-fc348cf5bdd2",
   "metadata": {},
   "source": [
    "Yes. Steps 2 and 3 can be combined into a single line.  \n",
    "For example:\n",
    "\n",
    "```julia\n",
    "folds[idx_pos] = crossvalidation(length(idx_pos), k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3916d3f-a710-403f-a264-f341c6037c8b",
   "metadata": {},
   "source": [
    "    4. Repeat a similar operation for **negative instances**.\n",
    "    5. Return the index vector.\n",
    "\n",
    "Loops are **not allowed** in the implementation of this function. The function must return a **vector of integers** of length **N**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd4a8077-61ae-4e30-a541-f7785c708d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crossvalidation (generic function with 4 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Random\n",
    "\n",
    "\n",
    "function crossvalidation(targets::AbstractArray{Bool,1}, k::Int64)\n",
    "    N = length(targets)\n",
    "    folds = zeros(Int, N)\n",
    "\n",
    "    # Indices for each class\n",
    "    idx_pos = findall(identity, targets)  # true (positives)\n",
    "    idx_neg = findall(!, targets)         # false (negatives)\n",
    "\n",
    "    # Stratified assignment\n",
    "    folds[idx_pos] = crossvalidation(length(idx_pos), k)\n",
    "    folds[idx_neg] = crossvalidation(length(idx_neg), k)\n",
    "\n",
    "    return folds\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed76ab4-fc61-4d6e-9bb9-274c9d36fab2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. Develop a new function called `crossvalidation` that receives:\n",
    "\n",
    "- A matrix `targets::AbstractArray{Bool,2}`, where each row corresponds to one instance and each column to one class (multilabel format),\n",
    "- An integer `k`, indicating the number of folds (subsets) to divide the dataset into.\n",
    "\n",
    "The function must return a vector of integers of length `N` (where `N` is the number of rows in `targets`).  \n",
    "Each element in the vector indicates the subset (fold) that the corresponding instance should be assigned to.\n",
    "\n",
    "The partitioning must be **stratified**, meaning that the distribution of **each class across the folds is preserved**.\n",
    "Suggested Steps:\n",
    "\n",
    "    1. Create an index vector with as many elements as rows in `targets`.\n",
    "\n",
    "    2. Use a loop that **iterates over the classes** (i.e., the columns of `targets`).  \n",
    "     For each class:\n",
    "\n",
    "       i. Count the number of instances that belong to that class using `sum(targets[:, i])`.  \n",
    "       ii. Call the previously defined `crossvalidation` function with the number of instances and `k`.  \n",
    "       iii. Assign the result to the corresponding positions in the index vector where `targets[:, i]` is `true`.\n",
    "\n",
    "   ### Question 5.3\n",
    "   > ❓ Could you combine these three operations into a single line inside the loop? (Optional exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12cc193-80ae-45ca-94de-46550f892a14",
   "metadata": {},
   "source": [
    "Yes. The three operations (counting class members, generating folds for them, and assigning them)\n",
    "can be combined into a single line inside the loop:\n",
    "\n",
    "```julia\n",
    "folds[findall(targets[:, i])] = crossvalidation(sum(targets[:, i]), k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251ad9a5-6b74-4470-bbb7-19c0037f7da3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "    3. Return the index vector.\n",
    "\n",
    "  #### Important considerations\n",
    "\n",
    "   - This function is allowed to use **a single loop** that iterates over the classes.\n",
    "   - The result must be a vector of **integer values** with length equal to the number of instances `N`.\n",
    "\n",
    "  #### Class support per fold\n",
    "\n",
    "  It is essential to ensure that **each class has at least `k` instances**.\n",
    "\n",
    "  > A typical value is `k = 10`.\n",
    "\n",
    "### Question 5.4\n",
    "> ❓ What would happen if a class has fewer than `k` instances?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53345403-5571-476a-9849-c8ba655fd8a5",
   "metadata": {},
   "source": [
    "If a class has < k samples, you cannot place at least one instance of that class in every fold. Stratification for that class becomes impossible; some folds will have zero positives of that class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20df17c-db3b-4858-943c-3776bde06078",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Question 5.5\n",
    "> ❓ How would this affect the calculation of metrics such as sensitivity or specificity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abde175-4bcd-409a-aed2-869193c37438",
   "metadata": {},
   "source": [
    "When a test fold lacks positives (or negatives) for a class:\n",
    "\n",
    "* Sensitivity (TPR) is undefined (denominator = number of actual positives = 0) and is often skipped or set to NaN.\n",
    "\n",
    "* Specificity (TNR) is undefined when there are no negatives.\n",
    "This leads to unstable, high-variance estimates and may bias the aggregated metrics unless such folds are handled explicitly (e.g., skip those class–fold entries or reduce k)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeb3de8-1e3c-4fc9-b40c-3ef37bcacac8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If, for any reason, it is not possible to guarantee at least `k` samples per class, you may consider **reducing the value of `k`**.  \n",
    "In this case, consult your instructor to evaluate the implications and whether the trained models would still yield meaningful results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5491483d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crossvalidation (generic function with 4 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Random\n",
    "\n",
    "function crossvalidation(targets::AbstractArray{Bool,2}, k::Int64)\n",
    "    N, C = size(targets)\n",
    "    folds = zeros(Int, N)\n",
    "\n",
    "    # Ensure each class has at least k instances\n",
    "    class_counts = vec(sum(targets; dims=1))\n",
    "    if any(class_counts .< k)\n",
    "        error(\"Each class must have at least k instances. Found counts=$(class_counts), k=$(k).\")\n",
    "    end\n",
    "\n",
    "    # Single allowed loop: iterate over classes (columns)\n",
    "    for i in 1:C\n",
    "        idx = findall(@view targets[:, i])                 # instances that belong to class i\n",
    "        folds[idx] = crossvalidation(length(idx), k)       # assign folds for this class\n",
    "    end\n",
    "\n",
    "    # if any instance had no positive label in any class,\n",
    "    # assign it non-stratified so every row gets a fold.\n",
    "    if any(folds .== 0)\n",
    "        rem = findall(==(0), folds)\n",
    "        folds[rem] = crossvalidation(length(rem), k)\n",
    "    end\n",
    "\n",
    "    return folds\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc74a4b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "4. Implement a final version of the `crossvalidation` function, where:\n",
    "\n",
    "- The first argument is `targets::AbstractArray{<:Any,1}`, a vector containing **heterogeneous elements** (e.g., strings, symbols, integers, etc.),\n",
    "- The second argument is the usual integer `k` (number of folds),\n",
    "- The output is a vector of **integer values**, one per instance, indicating the fold assignment.\n",
    "\n",
    "To implement this function, follow these steps:\n",
    "\n",
    "   1. Call the `oneHotEncoding` function, passing `targets` as the input.  \n",
    "      This will convert the labels into a binary matrix (multilabel format).\n",
    "   2. Call the **previous version** of the `crossvalidation` function, passing the encoded matrix and the value `k`.\n",
    "\n",
    "   > **No loops are allowed** in this function.\n",
    "   > The returned vector must contain integers and have the same length as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bbc619c8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crossvalidation (generic function with 4 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Random\n",
    "\n",
    "function crossvalidation(targets::AbstractArray{<:Any,1}, k::Int64)\n",
    "    classes = unique(targets)\n",
    "    enc = Utils.oneHotEncoding(targets, classes)   # heterogeneous labels -> one-hot matrix\n",
    "    bool_enc = enc .!= 0             # ensure Bool matrix regardless of encoding type\n",
    "    return crossvalidation(bool_enc, k)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a868da-4c95-4182-b644-a69ecc90393c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "5. Finally, implement a function called `ANNCrossValidation` that trains an artificial neural network (ANN) multiple times following a **cross-validation strategy**, as described in this exercise.\n",
    "\n",
    " This function will receive both the usual inputs for training a neural network (as seen in previous exercises) and additional arguments for performing cross-validation with multiple executions per fold.\n",
    "\n",
    "   #### **Function parameters**\n",
    "\n",
    "   - `topology::AbstractArray{<:Int,1}` — Structure of the neural network.\n",
    "   - `dataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{<:Any,1}}` — A tuple with:\n",
    "     - A matrix of inputs (instances in rows),\n",
    "     - A vector of target labels (categorical). These must be converted into a Boolean matrix via `oneHotEncoding` inside this function.\n",
    "   - `crossValidationIndices::Array{Int64,1}` — Vector of cross-validation fold assignments for each instance, obtained via the `crossvalidation` function.\n",
    "\n",
    "   **Optional parameters** (with default values, as specified in the function signature):\n",
    "\n",
    "   - `numExecutions`: Number of training repetitions per fold.\n",
    "   - `transferFunctions`: Activation functions for each layer in the ANN.\n",
    "     ### Question 5.6\n",
    "     > ❓ Does it make sense to use a linear activation function in hidden layers?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c35491-2c6e-45b1-b303-d760a4c1a906",
   "metadata": {},
   "source": [
    "No. A stack of linear layers is equivalent to a single linear transformation; it cannot model nonlinear decision boundaries. Hidden layers should use nonlinear activations (e.g., ReLU, GELU, tanh) so the network can learn complex functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b490f182-f600-461c-b3ab-5f3bbac0002c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "   - `maxEpochs`: Maximum number of training iterations (epochs).\n",
    "   - `minLoss`: Minimum loss to stop training early.\n",
    "   - `learningRate`: Learning rate.\n",
    "   - `validationRatio`: Fraction of data used for validation in early stopping (can be 0).\n",
    "   - `maxEpochsVal`: Maximum number of epochs without validation improvement before stopping early.\n",
    "\n",
    "\n",
    " **Step-by-step implementation**\n",
    "\n",
    "   1. **Extract class labels**:\n",
    "      ```julia\n",
    "      classes = unique(targets)\n",
    "      ```\n",
    "   2. **One-hot encode** the categorical target labels using the `oneHotEncoding` function and the computed `classes`.\n",
    "      ### Question 5.7\n",
    "      > ❓ What could go wrong if the `classes` vector is not passed explicitly to `oneHotEncoding`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb8c201-4ef4-4c1d-aef0-49d2ce01ec4d",
   "metadata": {},
   "source": [
    "- **Inconsistent column ordering** across folds/executions, breaking metric aggregation and confusion matrices.  \n",
    "- **Dropped columns** when a class is absent in a subset, changing the one-hot width and misaligning per-class metrics.  \n",
    "\n",
    "Providing a **global, fixed `classes`** list ensures stable, consistent one-hot encoding in every fold/run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24113d84-7186-4ebb-afd9-6c983afcab23",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "   3. **Determine the number of folds** using:\n",
    "      ```julia\n",
    "      numFolds = maximum(crossValidationIndices)\n",
    "      ```\n",
    "   4. Create one vector per metric (`accuracy`, `error rate`, `sensitivity`, `specificity`, `PPV`, `NPV`, `F1`) to store fold results.\n",
    "\n",
    "   5. Initialize a **confusion matrix accumulator** (matrix of real numbers with all entries set to 0) for the **global test confusion matrix**.\n",
    "\n",
    "   **For each fold**:\n",
    "\n",
    "   - Extract `train` and `test` subsets for **inputs** and **outputs**, based on the cross-validation indices and current fold number.\n",
    "\n",
    "   - Since ANNs are **non-deterministic**, results from a single training per fold may not be representative.  \n",
    "     For this reason, train the ANN **multiple times per fold** (as specified in `numExecutions`).\n",
    "\n",
    "   **Inside each fold**:\n",
    "\n",
    "   1. Initialize vectors to store the metric results for each execution.  \n",
    "   2. Create a 3D array of size `(numClasses, numClasses, numExecutions)` to store test confusion matrices from each execution.\n",
    "\n",
    "   3. For each execution:\n",
    "\n",
    "      - If `validationRatio > 0`, split the training set into training and validation sets using `holdOut`.  \n",
    "        > ⚠️ You must adjust the ratio properly, since you’re applying it **on the training subset**, not the full dataset.\n",
    "        ### Question 5.8  \n",
    "        > ❓ How can you adapt the validation ratio accordingly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e257035-50fe-46bc-9134-0f28ffbf7ee1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb09aae0-53cc-44d5-9967-250e608332e2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "      - Train the network using `trainClassANN`.\n",
    "\n",
    "      - Evaluate it on the test set using `confusionMatrix`.\n",
    "\n",
    "      - Store the returned metrics and confusion matrix.\n",
    "\n",
    "   4. After all executions for this fold:\n",
    "\n",
    "      - Compute the **average** of each metric vector and store it in the global metric vectors.\n",
    "      - Compute the **mean confusion matrix** using:\n",
    "        ```julia\n",
    "        mean(confusionMatrices, dims=3)\n",
    "        ```\n",
    "        > Note: This returns a 3D array with one slice; you must extract the 2D matrix from it.\n",
    "\n",
    "      - Add the resulting matrix to the global confusion matrix.\n",
    "\n",
    "#### Return values\n",
    "\n",
    "Once all folds are completed:\n",
    "\n",
    "- For each metric, compute the **mean and standard deviation** across the folds using `mean(...)` and `std(...)`.\n",
    "- Return a tuple with 8 elements:\n",
    "  1. Accuracy (mean, std)\n",
    "  2. Error rate (mean, std)\n",
    "  3. Sensitivity (mean, std)\n",
    "  4. Specificity (mean, std)\n",
    "  5. PPV (mean, std)\n",
    "  6. NPV (mean, std)\n",
    "  7. F1-score (mean, std)\n",
    "  8. Global test confusion matrix\n",
    "\n",
    "This function will be called in the next exercise by a general-purpose validation function.  \n",
    "Unlike ANNs, **other ML models (e.g., SVM, kNN)** are **deterministic** — so they do not need the inner execution loop.  \n",
    "For them, training once per fold is sufficient to produce stable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "44bcc770",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ANNCrossValidation"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Random, Statistics\n",
    "\n",
    "# --- helper: scores -> one-hot by row-argmax ---\n",
    "_scores_to_onehot(scores::AbstractMatrix{<:Real}) = begin\n",
    "    N, C = size(scores)\n",
    "    Yhat = falses(N, C)\n",
    "    @inbounds for i in 1:N\n",
    "        j = findmax(view(scores, i, :))[2]\n",
    "        Yhat[i, j] = true\n",
    "    end\n",
    "    Yhat\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "ANNCrossValidation (adapted for Utils)\n",
    "\n",
    "Trains an ANN with k-fold cross-validation and multiple executions per fold,\n",
    "using the utility functions defined in `Utils`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "topology::AbstractVector{<:Int}\n",
    "    Network architecture (e.g. [4, 8, 3]).\n",
    "dataset::Tuple{AbstractMatrix{<:Real}, AbstractVector{<:Any}}\n",
    "    Tuple (X, targets) where X are input features and targets are class labels.\n",
    "crossValidationIndices::Vector{Int}\n",
    "    Vector of fold indices (1..k) for k-fold splitting.\n",
    "\n",
    "Keyword arguments\n",
    "-----------------\n",
    "numExecutions::Int = 50\n",
    "transferFunctions = nothing\n",
    "maxEpochs::Int = 200\n",
    "minLoss::Real = 0.0\n",
    "learningRate::Real = 1e-3\n",
    "validationRatio::Real = 0.0\n",
    "maxEpochsVal::Int = 25\n",
    "\n",
    "Returns\n",
    "-------\n",
    "(ntuple)\n",
    "1. (meanAcc,  stdAcc)\n",
    "2. (meanErr,  stdErr)\n",
    "3. (meanSens, stdSens)\n",
    "4. (meanSpec, stdSpec)\n",
    "5. (meanPPV,  stdPPV)\n",
    "6. (meanNPV,  stdNPV)\n",
    "7. (meanF1,   stdF1)\n",
    "8. globalConfusionMatrix::Matrix{Float64}\n",
    "\"\"\"\n",
    "function ANNCrossValidation(\n",
    "    topology::AbstractVector{<:Int},\n",
    "    dataset::Tuple{AbstractMatrix{<:Real}, AbstractVector{<:Any}},\n",
    "    crossValidationIndices::Vector{Int};\n",
    "    numExecutions::Int = 50,\n",
    "    transferFunctions = nothing,\n",
    "    maxEpochs::Int = 200,\n",
    "    minLoss::Real = 0.0,\n",
    "    learningRate::Real = 1e-3,\n",
    "    validationRatio::Real = 0.0,\n",
    "    maxEpochsVal::Int = 25\n",
    ")\n",
    "\n",
    "    # 1) Unpack and check dataset\n",
    "    X, targets = dataset\n",
    "    N = size(X, 1)\n",
    "    @assert length(targets) == N \"targets length must match number of rows in X\"\n",
    "    @assert length(crossValidationIndices) == N \"CV index length must match N\"\n",
    "    @assert numExecutions ≥ 1 \"numExecutions must be ≥ 1\"\n",
    "\n",
    "    # 2) One-hot encode labels using Utils\n",
    "    classes = unique(targets)\n",
    "    Yenc = Utils.oneHotEncoding(targets, classes)\n",
    "    Y = Yenc .!= 0\n",
    "    C = size(Y, 2)\n",
    "\n",
    "    # 3) Get number of folds\n",
    "    numFolds = maximum(crossValidationIndices)\n",
    "    @assert minimum(crossValidationIndices) ≥ 1 \"Fold indices must start at 1.\"\n",
    "    @assert all(in(1:numFolds), crossValidationIndices) \"Fold indices must be in 1..numFolds.\"\n",
    "\n",
    "    # 4) Initialize metric accumulators\n",
    "    acc_f  = zeros(Float64, numFolds)\n",
    "    err_f  = zeros(Float64, numFolds)\n",
    "    sens_f = zeros(Float64, numFolds)\n",
    "    spec_f = zeros(Float64, numFolds)\n",
    "    ppv_f  = zeros(Float64, numFolds)\n",
    "    npv_f  = zeros(Float64, numFolds)\n",
    "    f1_f   = zeros(Float64, numFolds)\n",
    "    globalCM = zeros(Float64, C, C)\n",
    "\n",
    "    # Adjust validation ratio within each training subset\n",
    "    val_in_train = validationRatio == 0 ? 0.0 :\n",
    "                   clamp(validationRatio / ((numFolds - 1) / numFolds), 0.0, 0.9)\n",
    "\n",
    "    # ======================\n",
    "    # Iterate over each fold\n",
    "    # ======================\n",
    "    for fold in 1:numFolds\n",
    "        test_mask  = crossValidationIndices .== fold\n",
    "        train_mask = .!test_mask\n",
    "\n",
    "        Xtr, Xte = X[train_mask, :], X[test_mask, :]\n",
    "        Ytr, Yte = Y[train_mask, :], Y[test_mask, :]\n",
    "\n",
    "        acc_e  = similar(acc_f, numExecutions)\n",
    "        err_e  = similar(err_f, numExecutions)\n",
    "        sens_e = similar(sens_f, numExecutions)\n",
    "        spec_e = similar(spec_f, numExecutions)\n",
    "        ppv_e  = similar(ppv_f, numExecutions)\n",
    "        npv_e  = similar(npv_f, numExecutions)\n",
    "        f1_e   = similar(f1_f, numExecutions)\n",
    "        cms = Array{Float64}(undef, C, C, numExecutions)\n",
    "\n",
    "        for run in 1:numExecutions\n",
    "            # (a) Optional hold-out validation within the training set\n",
    "            if val_in_train > 0\n",
    "                Ntr = size(Xtr, 1)\n",
    "                train_idx, val_idx = Utils.holdOut(Ntr, val_in_train)\n",
    "                Xtr_run, Ytr_run = Xtr[train_idx, :], Ytr[train_idx, :]\n",
    "                Xval,    Yval    = Xtr[val_idx, :],  Ytr[val_idx, :]\n",
    "\n",
    "                model, _, _, _ = Utils.trainClassANN(\n",
    "                    topology, (Xtr_run, Ytr_run);\n",
    "                    validationDataset = (Xval, Yval),\n",
    "                    transferFunctions = (transferFunctions === nothing ? fill(σ, length(topology)) : transferFunctions),\n",
    "                    maxEpochs         = maxEpochs,\n",
    "                    minLoss           = minLoss,\n",
    "                    learningRate      = learningRate,\n",
    "                    maxEpochsVal      = maxEpochsVal\n",
    "                )\n",
    "            else\n",
    "                model, _, _, _ = Utils.trainClassANN(\n",
    "                    topology, (Xtr, Ytr);\n",
    "                    transferFunctions = (transferFunctions === nothing ? fill(σ, length(topology)) : transferFunctions),\n",
    "                    maxEpochs         = maxEpochs,\n",
    "                    minLoss           = minLoss,\n",
    "                    learningRate      = learningRate,\n",
    "                    maxEpochsVal      = maxEpochsVal\n",
    "                )\n",
    "            end\n",
    "\n",
    "            # (b) Predict on the test set\n",
    "            scores = Array(model(Xte'))'     # convert Flux output to (Ntest × C)\n",
    "            Yhat   = _scores_to_onehot(scores)\n",
    "\n",
    "            # (c) Evaluate using Utils.confusionMatrix\n",
    "            acc, err, sens, spec, ppv, npv, f1, CM = Utils.confusionMatrix(Yhat, Yte)\n",
    "\n",
    "            acc_e[run]  = acc\n",
    "            err_e[run]  = err\n",
    "            sens_e[run] = sens\n",
    "            spec_e[run] = spec\n",
    "            ppv_e[run]  = ppv\n",
    "            npv_e[run]  = npv\n",
    "            f1_e[run]   = f1\n",
    "            cms[:, :, run] = float.(CM)\n",
    "        end\n",
    "\n",
    "        # (d) Average results per fold\n",
    "        acc_f[fold]  = mean(acc_e)\n",
    "        err_f[fold]  = mean(err_e)\n",
    "        sens_f[fold] = mean(sens_e)\n",
    "        spec_f[fold] = mean(spec_e)\n",
    "        ppv_f[fold]  = mean(ppv_e)\n",
    "        npv_f[fold]  = mean(npv_e)\n",
    "        f1_f[fold]   = mean(f1_e)\n",
    "\n",
    "        # (e) Aggregate confusion matrices\n",
    "        meanCM = dropdims(mean(cms; dims=3), dims=3)\n",
    "        globalCM .+= meanCM\n",
    "    end\n",
    "\n",
    "    # Final aggregated statistics\n",
    "    return (\n",
    "        (mean(acc_f),  std(acc_f)),\n",
    "        (mean(err_f),  std(err_f)),\n",
    "        (mean(sens_f), std(sens_f)),\n",
    "        (mean(spec_f), std(spec_f)),\n",
    "        (mean(ppv_f),  std(ppv_f)),\n",
    "        (mean(npv_f),  std(npv_f)),\n",
    "        (mean(f1_f),   std(f1_f)),\n",
    "        globalCM\n",
    "    )\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a201b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `modelCrossValidation` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `modelCrossValidation` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/mia/ml1/MIA_Repo/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X43sZmlsZQ==.jl:40"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Imports\n",
    "# =========================\n",
    "using DelimitedFiles\n",
    "using Random, Statistics\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Example with Iris Data\n",
    "# =========================\n",
    "\n",
    "# 1) Load the dataset (CSV with 5 columns: 4 numeric + 1 label)\n",
    "dataset = readdlm(\"iris/iris.data\", ',')\n",
    "\n",
    "# 2) Separate features (X) and class labels (targets)\n",
    "X = Float32.(dataset[:, 1:4])          # 150×4 matrix of features\n",
    "targets = Vector{String}(dataset[:, 5])  # 150 labels (Setosa, Versicolor, Virginica)\n",
    "\n",
    "# 3) Normalize each feature column to [0, 1]\n",
    "Xn = Utils.normalizeMinMax(X)\n",
    "@assert all(minimum(Xn, dims=1) .≈ 0f0)\n",
    "@assert all(maximum(Xn, dims=1) .≈ 1f0)\n",
    "\n",
    "# 4) Generate stratified fold indices for 5-fold cross-validation\n",
    "k = 5\n",
    "rng = Random.MersenneTwister(123)  # reproducibility for the fold assignment\n",
    "cv_idx = Utils.stratified_kfold_indices(targets, k; rng=rng)\n",
    "\n",
    "# 5) Define network topology and run cross-validation\n",
    "# Use hidden layers only; inputs/outputs are inferred by Utils.buildClassANN\n",
    "topology = [8, 6]  # e.g., two hidden layers\n",
    "\n",
    "result = ANNCrossValidation(\n",
    "    topology,\n",
    "    (Xn, targets),\n",
    "    cv_idx;\n",
    "    numExecutions   = 10,\n",
    "    maxEpochs       = 300,\n",
    "    learningRate    = 0.1,\n",
    "    validationRatio = 0.1,\n",
    "    maxEpochsVal    = 30\n",
    ")\n",
    "\n",
    "# 6) Unpack results and display metrics\n",
    "((meanAcc,  stdAcc),\n",
    " (meanErr,  stdErr),\n",
    " (meanSens, stdSens),\n",
    " (meanSpec, stdSpec),\n",
    " (meanPPV,  stdPPV),\n",
    " (meanNPV,  stdNPV),\n",
    " (meanF1,   stdF1),\n",
    " globalCM) = result\n",
    "\n",
    "println(\"=== K-Fold Cross-Validation on Iris Dataset (k = $k) ===\")\n",
    "println(\"Accuracy:  $(round(meanAcc, digits=4)) ± $(round(stdAcc, digits=4))\")\n",
    "println(\"Error:     $(round(meanErr, digits=4)) ± $(round(stdErr, digits=4))\")\n",
    "println(\"Sensitivity: $(round(meanSens, digits=4)) ± $(round(stdSens, digits=4))\")\n",
    "println(\"Specificity: $(round(meanSpec, digits=4)) ± $(round(stdSpec, digits=4))\")\n",
    "println(\"PPV:       $(round(meanPPV, digits=4)) ± $(round(stdPPV, digits=4))\")\n",
    "println(\"NPV:       $(round(meanNPV, digits=4)) ± $(round(stdNPV, digits=4))\")\n",
    "println(\"F1-Score:  $(round(meanF1, digits=4)) ± $(round(stdF1, digits=4))\")\n",
    "\n",
    "println(\"\\nGlobal Confusion Matrix (mean of folds):\")\n",
    "show(stdout, \"text/plain\", round.(globalCM, digits=2))\n",
    "println()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bf4ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.7",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
