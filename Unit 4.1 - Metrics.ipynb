{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57c56216",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Metrics\n",
    "\n",
    "We have used simple functions in the previous assignments, such as MSE in regression problems or accuracy in classification problems, to quantify the goodness-of-fit of the model. While in regression problems the functions are based on an error calculated in one way or another (mean error, mean square error, etc.), in classification problems other types of metrics can be derived depending on what the problem being solved is like. Many of these metrics, at least those that will be used in practice, are based on the calculation of the confusion matrix.\n",
    "\n",
    "A confusion matrix is a square matrix, with as many rows and columns as classes, showing the distribution of patterns in classes, and the classification performed by the model. Usually the rows show how the model has performed the classification, and the columns show the actual classification values, although this may vary depending on the source consulted.\n",
    "\n",
    "The simplest case corresponds to 2 classes, where one is considered \"negative\" and the other \"positive\". A two-class confusion matrix would be as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51abb7f4-08ac-4c27-9e62-08cf14906c63",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "| |                      | **Predicted**          |                        |\n",
    "|--------|----------------------|------------------------|------------------------|\n",
    "|  |                      | **Negative**           | **Positive**           |\n",
    "| **Real** | **Negative**  | VN (True Negative)     | FP (False Positive)    |\n",
    "|  | **Positive**  | FN (False Negative)    | VP (True Positive)     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f184c9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This confusion matrix contains 4 values, which can be divided\n",
    "\n",
    "- according to the output of the model: positive or negative.\n",
    "- depending on whether the model is wrong or not: true or false. \n",
    "\n",
    "Thus, these 4 values are called true negatives (TN), false positives (FP), false negatives (FN) and true positives (TP). For example, false negatives would be the number of patterns that the system has classified as negative, and has been wrong because they were actually positive.\n",
    "\n",
    "From this confusion matrix, different metrics can be calculated. Depending on the problem you are working on, it will be more interesting to follow one or the other. Some of the most most commonly used metrics are:\n",
    "\n",
    "- **Accuracy**. Ratio of patterns in which the prediction is correct. Calculated as $$\\frac{TN+TP}{TN+TP+FN+FP}$$\n",
    "- **Error rate**. Ratio of patterns in which the prediction is wrong. Calculated as $$\\frac{FP+FN}{TN+TP+FN+FP}$$\n",
    "- **Sensitivity** or **recall**. Indicates the probability that a positive classification result is obtained for a positive case. It is calculated as $$\\frac{TP}{FN+TP}$$\n",
    "    - In a medical test, the test sensitivity represents the probability that a sick (positive) subject will have a positive test result.\n",
    "- **Specificity**. Indicates the probability that a negative classification result is obatined for a negative case. It is calculated as $$\\frac{TN}{FP+TN}$$\n",
    "    - The specificity of a test represents the probability that a healthy (negative) subject will have a negative test result.\n",
    "- **Precision** or **positive predictive value**. Ratio of positive patterns that have been correctly classified. Calculated as $$\\frac{TP}{TP+FP}$$\n",
    "- **Negative predictive value**. Ratio of positive patterns that have been correctly classified. Calculated as $$\\frac{TN}{TN+FN}$$\n",
    "- **F-score**, **F1-score** or **F-measurement**. It is defined as the harmonic mean of precision and recall.\n",
    "\n",
    "It is worth clarifying that these metrics, as well as others seen in theory class (ROC curve, Kappa index) are used to assess already trained classifiers, not to perform the training process. To be trained, each model has its own function to quantify the error or goodness of fit, such as the cross-entropy function in the case of neural networks.\n",
    "\n",
    "The accuracy is probably the most commonly used value, as it indicates the success rate of the classifier in a simple way. However, depending on the problem you are working with, it may not be the most appropriate metric. For example, in a mass population-based test for a disease where it is known that most people do not have the disease, a model that classifies everyone as negative (healthy) will have a very high accuracy, even though the model does not actually do anything.\n",
    "\n",
    "For this reason, it is necessary to assess which metric or metrics are the most used for each kind of problem. In many problems where the different classes are of equal importance, the accuracy value may be enough. However, in other problems, it may be of more interest to evaluate the situations in which a positive response is or should be produced by the model, as it could indicate something critical, such as detecting a disease or raising some kind of alarm. For this reason, sensitivity and positive predictive value values are often taken into account in addition to accuracy. There is a more extensive discussion of this in the theory notes, but a possible informal guide might be the following:\n",
    "\n",
    "- If you want to minimise the number of positives incorrectly classified as negative (e.g. maximise the number of correctly diagnosed sick subjects, or maximise the number of alarms given for risky situations), the indicated metric is sensitivity (recall).\n",
    "- If one wishes to minimise the number of samples incorrectly classified as positives (false positives, e.g. sick subjects diagnosed as healthy, or situations where an alarm should not has been raised but it was), the indicated metric is the positive predictive value (precision).\n",
    "\n",
    "Therefore, the most appropriate metric depends entirely on the specific problem, according to the relative importance of the classifier output and its behaviour. In this type of problem, the F-score is a metric that may be more useful than accuracy.\n",
    "\n",
    "Another issue to be considered is the data imbalance. Accuracy is a metric that gives a \"global\" view, which can be misleading when the distribution in classes is unbalanced. In these cases, F-score is a better metric. Having unbalanced databases is very common, which provides an extra argument for using F-score rather than accuracy.\n",
    "\n",
    "Finally, if you have more than two classes, it is possible to build a confusion matrix in a similar way by having one row and column per class. For example:\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "03ce5ee0",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "             Prediction \n",
    "           +---+---+---+\n",
    "           | A | B | C |\n",
    "       +---+---+---+---+\n",
    "       | A |   |   |   |\n",
    "       +---+---+---+---+\n",
    "  Real | B |   |   |   |\n",
    "       +---+---+---+---+\n",
    "       | C |   |   |   |\n",
    "       +---+---+---+---+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f6a477",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In these cases, it is no longer possible to speak of positive or negative patterns, since there are more than two classes, nor to take values for sensitivity or positive predictive value. However, this confusion matrix can offer very interesting information when it comes to understanding how the model works, assessing which are the classes between which the model finds it easiest and most difficult to separate.\n",
    "\n",
    "### Question 4.1\n",
    "> ‚ùì If the pattern set has been divided into training and test subsets, which subset should be used to calculated the confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc6397f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The test subset should be used. we dont want to use this metrics to train our model. Only to evaluate its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8354cd47",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this assignment, you are asked to:\n",
    "\n",
    "1. Develop a function called `confusionMatrix` which takes two vectors of equal length (the number of patterns), the first one containing the outputs obtained by a model (`outputs`) and the second with the desired outputs (`targets`), both of type `AbstractArray{Bool,1}`. This function should return:\n",
    "    - Accuracy\n",
    "    - Error rate\n",
    "    - Sensitivity\n",
    "    - Specificity\n",
    "    - Positive predicitive value\n",
    "    - Negative predicitve value\n",
    "    - F-score\n",
    "    - Confusion matrix, as an object of type `Array{Int64,2}` with two rows and two columns\n",
    "    \n",
    "    As this function is being fed with boolean-valued vectors, it will be applicable to problems with two classes (positive and negative cases).\n",
    "\n",
    "    It is necessary to consider some particular situations when calculated the required classification metrics.\n",
    "    \n",
    "        - If every pattern is a true negative, neither the sensitivity nor the positive predictive value can be calculated. In this case the system works correctly, so these two metrics will be 1.\n",
    "        - Similarly, neither the specificity nor the negative predictive value can be obtained and if every pattern is a true positive, so both metrics have to be manually set to 1.\n",
    "        - If neither of these two cases has occurred and there is still any metric which cannot be calculated, it will take the value of 0. For example, if the sensitivity could not be calculated, it means that there was no pattern with a positive desired output.\n",
    "        - It both sensitivity and positive predictive values are equal to 0, the value of F-score cannot be obtained, and thus it will be 0.\n",
    "        \n",
    "    Do not use loops inside the developed function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03d7f264-a0f7-4c94-bf31-fdbe88d3771b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fscore_fn (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Metric helper functions ---\n",
    "\n",
    "# Sensitivity (Recall)\n",
    "function sensitivity_fn(TP, FN, FP, TN)\n",
    "    all_TN = TP == 0 && FP == 0 && FN == 0\n",
    "    if all_TN\n",
    "        return 1.0\n",
    "    elseif (TP + FN) == 0\n",
    "        return 0.0\n",
    "    else\n",
    "        return TP / (TP + FN)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Specificity\n",
    "function specificity_fn(TN, FP, TP, FN)\n",
    "    all_TP = TN == 0 && FP == 0 && FN == 0\n",
    "    if all_TP\n",
    "        return 1.0\n",
    "    elseif (TN + FP) == 0\n",
    "        return 0.0\n",
    "    else\n",
    "        return TN / (TN + FP)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Positive Predictive Value (Precision)\n",
    "function ppv_fn(TP, FP, FN, TN)\n",
    "    all_TN = TP == 0 && FP == 0 && FN == 0\n",
    "    if all_TN\n",
    "        return 1.0\n",
    "    elseif (TP + FP) == 0\n",
    "        return 0.0\n",
    "    else\n",
    "        return TP / (TP + FP)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Negative Predictive Value\n",
    "function npv_fn(TN, FN, TP, FP)\n",
    "    all_TP = TN == 0 && FP == 0 && FN == 0\n",
    "    if all_TP\n",
    "        return 1.0\n",
    "    elseif (TN + FN) == 0\n",
    "        return 0.0\n",
    "    else\n",
    "        return TN / (TN + FN)\n",
    "    end\n",
    "end\n",
    "\n",
    "# F-score\n",
    "function fscore_fn(TP, FP, FN, TN)\n",
    "    sens = sensitivity_fn(TP, FN, FP, TN)\n",
    "    ppv = ppv_fn(TP, FP, FN, TN)\n",
    "    if sens == 0 || ppv == 0\n",
    "        return 0.0\n",
    "    else\n",
    "        return 2 * (ppv * sens) / (ppv + sens)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd4df5f7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "confusionMatrix (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function confusionMatrix(outputs::AbstractArray{Bool,1}, targets::AbstractArray{Bool,1})\n",
    "    @assert length(outputs) == length(targets) \"Outputs and targets must have the same length\"\n",
    "\n",
    "    # Confusion matrix components\n",
    "    TP = sum(outputs .& targets)\n",
    "    TN = sum(.!outputs .& .!targets)\n",
    "    FP = sum(outputs .& .!targets)\n",
    "    FN = sum(.!outputs .& targets)\n",
    "\n",
    "    # Confusion matrix\n",
    "    conf_mat = [TP FP; FN TN]\n",
    "\n",
    "    total = TP + TN + FP + FN\n",
    "\n",
    "    # Compute metrics (each helper handles edge cases internally)\n",
    "    accuracy = (TP + TN) / total\n",
    "    error_rate = (FP + FN) / total\n",
    "    sens = sensitivity_fn(TP, FN, FP, TN)\n",
    "    spec = specificity_fn(TN, FP, TP, FN)\n",
    "    ppv = ppv_fn(TP, FP, FN, TN)\n",
    "    npv = npv_fn(TN, FN, TP, FP)\n",
    "    fscore = fscore_fn(TP, FP, FN, TN)\n",
    "\n",
    "    return (\n",
    "        Accuracy = accuracy,\n",
    "        ErrorRate = error_rate,\n",
    "        Sensitivity = sens,\n",
    "        Specificity = spec,\n",
    "        PPV = ppv,\n",
    "        NPV = npv,\n",
    "        Fscore = fscore,\n",
    "        ConfusionMatrix = conf_mat\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720b35e9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. Many models (e.g. artificial neural networks) do not return a categorial output for each pattern, but the probability that it is \"positive\". For this reason, it is requested to develop a function with the same name as the previous one, whose first parameter is not a vector of boolean values but a vector of real values (of type `AbstractArray{<:Real}`). It also receives an optional third parameter with a threshold, with a default value, which is used to apply the previous function and return, therefore, the same values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b7712ee",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "confusionMatrix (generic function with 2 methods)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function confusionMatrix(outputs::AbstractArray{<:Real,1},targets::AbstractArray{Bool,1}; threshold::Real=0.5)\n",
    "    # Convert probabilities to boolean predictions\n",
    "    bin_outputs = outputs .>= threshold\n",
    "    # Delegate to boolean version\n",
    "    return confusionMatrix(bin_outputs, targets)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1b8dad",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. Develop two functions with the same name, `printConfusionMatrix`, that receive the model outputs and the desired outputs, call the previous functions and display the results obtained, including the confusion matrix. One of these functions shall receive a vector of model classifications (`outputs`) of type `AbstractArray{Bool,1}`, while for the other one this parameter shall be a vector of real values (of type `AbstractArray{<:Real}`). These functions will make calls to the previous functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90e01e78",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "printConfusionMatrix (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function printConfusionMatrix(outputs::AbstractArray{Bool,1},targets::AbstractArray{Bool,1})\n",
    "    result = confusionMatrix(outputs, targets)\n",
    "\n",
    "    println(\"=== Confusion Matrix Results ===\")\n",
    "    println(\"Accuracy:               \", round(result.Accuracy, digits=4))\n",
    "    println(\"Error Rate:             \", round(result.ErrorRate, digits=4))\n",
    "    println(\"Sensitivity (Recall):   \", round(result.Sensitivity, digits=4))\n",
    "    println(\"Specificity:            \", round(result.Specificity, digits=4))\n",
    "    println(\"PPV (Precision):        \", round(result.PPV, digits=4))\n",
    "    println(\"NPV:                    \", round(result.NPV, digits=4))\n",
    "    println(\"F-Score:                \", round(result.Fscore, digits=4))\n",
    "    println(\"\\nConfusion Matrix (rows = true class, cols = predicted class):\")\n",
    "    println(\"                Predicted +    Predicted -\")\n",
    "    println(\"Actual +    \", result.ConfusionMatrix[1,1], \"              \", result.ConfusionMatrix[2,1])\n",
    "    println(\"Actual -    \", result.ConfusionMatrix[1,2], \"              \", result.ConfusionMatrix[2,2])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13c6f9c8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "printConfusionMatrix (generic function with 2 methods)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function printConfusionMatrix(outputs::AbstractArray{<:Real,1},targets::AbstractArray{Bool,1}; threshold::Real=0.5)\n",
    "    result = confusionMatrix(outputs, targets, treshold)\n",
    "\n",
    "    println(\"=== Confusion Matrix Results ===\")\n",
    "    println(\"Accuracy:               \", round(result.Accuracy, digits=4))\n",
    "    println(\"Error Rate:             \", round(result.ErrorRate, digits=4))\n",
    "    println(\"Sensitivity (Recall):   \", round(result.Sensitivity, digits=4))\n",
    "    println(\"Specificity:            \", round(result.Specificity, digits=4))\n",
    "    println(\"PPV (Precision):        \", round(result.PPV, digits=4))\n",
    "    println(\"NPV:                    \", round(result.NPV, digits=4))\n",
    "    println(\"F-Score:                \", round(result.Fscore, digits=4))\n",
    "    println(\"\\nConfusion Matrix (rows = true class, cols = predicted class):\")\n",
    "    println(\"                Predicted +    Predicted -\")\n",
    "    println(\"Actual +    \", result.ConfusionMatrix[1,1], \"              \", result.ConfusionMatrix[2,1])\n",
    "    println(\"Actual -    \", result.ConfusionMatrix[1,2], \"              \", result.ConfusionMatrix[2,2])\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.7",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
