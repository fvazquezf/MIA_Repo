{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5dd4668",
   "metadata": {},
   "source": [
    "# Ensemble Models\n",
    "\n",
    "One of the latest trends in artificial intelligence modelling can be summarised as \"knowledge of the whole or the crowd\". What this somewhat familiar phrase defines is the use of a multitude of so-called \"weak\" models in a meta-classifier. The aim is to generate a \"strong\" model based on the knowledge extracted by the \"weak\" models. For example, although it will be detailed later, multiple, much simpler Decision Trees are developed in a Random Forest. The combination of these ones in the Random Forest exceeds the performance of any of the individual models. The models that emerge in this way, as meta-classifiers or meta-regressors, are generically called **Ensemble models**.\n",
    "\n",
    "Is is worth mentioned that these models may not be limited only to decision trees, but may instead be composed of any type of machine learning model that has been seen previously. They can even be mixed models where not all models have been obtained in the same way, but can be created through the combined use of several techniques such as K-NN, SVM, etc. Thus, the first criteria to classifify the ensemble models would be if they are homogeneous or heterogeneous models. However this is not the only criteria to classifity the ensemble models, in this unit, we will explore various ways of generating the models and how to combine them later on. We will also take a closer look at two of the most common techniques within ensemble models such as Random Forest and _XGBoost_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10e861d",
   "metadata": {},
   "source": [
    "First of all, ensure that the required packages are installed. Therefore, you should only execute the folowing cell the fist time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0b736500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "# Ensure required packages (uncomment to install if needed)\n",
    "Pkg.add([\n",
    "    \"MLJ\", \n",
    "    \"MLJBase\", \n",
    "    \"MLJModels\", \n",
    "    \"MLJEnsembles\", \n",
    "    \"MLJLinearModels\", \n",
    "    \"DecisionTree\", \n",
    "    \"MLJDecisionTreeInterface\", \n",
    "    \"NaiveBayes\", \n",
    "    \"EvoTrees\", \n",
    "    \"CategoricalArrays\", \n",
    "    \"Random\",\n",
    "    \"LIBSVM\",           \n",
    "    \"Plots\",            \n",
    "    \"MLJModelInterface\", \n",
    "    \"CSV\",              \n",
    "    \"DataFrames\",       \n",
    "    \"UrlDownload\",      \n",
    "    \"XGBoost\"    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8f73db",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Unlike the first tutorials, where the iris flower problem has been used as a benchmark, in this tutorial we will use a different one. The problem is also included in the UCI repository, although it is also small, the number of variables increases significantly and therefore it will give us some more room to explore. Specifically, it is a classic machine learning problem, which is informally called Rock or Mine? It is a small database consisting of 111 patterns corresponding to rocks and 97 to water mines (simulated as metal cylinders). Each of the patterns consists of 60 numerical measurements corresponding to a section of the sonar sequences. These values are already between 0.0 and 1.0, although it is worth normalising them to be on the safe side. These measurements represent the energy value of different wavelength ranges for a certain period of time.\n",
    "\n",
    "We are going to use a couple of new packages in the process, more specificly, [DataFrames.jl](https://juliaai.github.io/DataScienceTutorials.jl/data/dataframe/) and [UrlDownload.jl](https://github.com/Arkoniak/UrlDownload.jl). Therefore, first thing first, ensure that the packages are correcly installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f0eb88ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Manifest.toml`\n",
      "WARNING: replacing module Utils.\n",
      "WARNING: could not import MLJBase.accuracy into Main\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg;\n",
    "Pkg.add(\"CSV\")\n",
    "Pkg.add(\"DataFrames\")\n",
    "Pkg.add(\"UrlDownload\")\n",
    "include(\"utils.jl\")\n",
    "using .Utils\n",
    "\n",
    "using MLJ\n",
    "using MLJBase: accuracy\n",
    "import Pkg;\n",
    "Pkg.add(\"MLJNaiveBayesInterface\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad05475",
   "metadata": {},
   "source": [
    "After that, the data will be downloaded if they are not already available, for which the following code can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "007c0232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   0%|                                         |  ETA: 1278824033.21 days\u001b[39m"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>61×7 DataFrame</span></div><div style = \"float: right;\"><span style = \"font-style: italic;\">36 rows omitted</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">variable</th><th style = \"text-align: left;\">mean</th><th style = \"text-align: left;\">min</th><th style = \"text-align: left;\">median</th><th style = \"text-align: left;\">max</th><th style = \"text-align: left;\">nmissing</th><th style = \"text-align: left;\">eltype</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Symbol\" style = \"text-align: left;\">Symbol</th><th title = \"Union{Nothing, Float64}\" style = \"text-align: left;\">Union…</th><th title = \"Any\" style = \"text-align: left;\">Any</th><th title = \"Union{Nothing, Float64}\" style = \"text-align: left;\">Union…</th><th title = \"Any\" style = \"text-align: left;\">Any</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"DataType\" style = \"text-align: left;\">DataType</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">Column1</td><td style = \"text-align: left;\">0.0291639</td><td style = \"text-align: left;\">0.0015</td><td style = \"text-align: left;\">0.0228</td><td style = \"text-align: left;\">0.1371</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">Column2</td><td style = \"text-align: left;\">0.0384365</td><td style = \"text-align: left;\">0.0006</td><td style = \"text-align: left;\">0.0308</td><td style = \"text-align: left;\">0.2339</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">Column3</td><td style = \"text-align: left;\">0.0438322</td><td style = \"text-align: left;\">0.0015</td><td style = \"text-align: left;\">0.0343</td><td style = \"text-align: left;\">0.3059</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: left;\">Column4</td><td style = \"text-align: left;\">0.0538923</td><td style = \"text-align: left;\">0.0058</td><td style = \"text-align: left;\">0.04405</td><td style = \"text-align: left;\">0.4264</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: left;\">Column5</td><td style = \"text-align: left;\">0.0752024</td><td style = \"text-align: left;\">0.0067</td><td style = \"text-align: left;\">0.0625</td><td style = \"text-align: left;\">0.401</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: left;\">Column6</td><td style = \"text-align: left;\">0.10457</td><td style = \"text-align: left;\">0.0102</td><td style = \"text-align: left;\">0.09215</td><td style = \"text-align: left;\">0.3823</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: left;\">Column7</td><td style = \"text-align: left;\">0.121747</td><td style = \"text-align: left;\">0.0033</td><td style = \"text-align: left;\">0.10695</td><td style = \"text-align: left;\">0.3729</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: left;\">Column8</td><td style = \"text-align: left;\">0.134799</td><td style = \"text-align: left;\">0.0055</td><td style = \"text-align: left;\">0.1121</td><td style = \"text-align: left;\">0.459</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: left;\">Column9</td><td style = \"text-align: left;\">0.178003</td><td style = \"text-align: left;\">0.0075</td><td style = \"text-align: left;\">0.15225</td><td style = \"text-align: left;\">0.6828</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: left;\">Column10</td><td style = \"text-align: left;\">0.208259</td><td style = \"text-align: left;\">0.0113</td><td style = \"text-align: left;\">0.1824</td><td style = \"text-align: left;\">0.7106</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: left;\">Column11</td><td style = \"text-align: left;\">0.236013</td><td style = \"text-align: left;\">0.0289</td><td style = \"text-align: left;\">0.2248</td><td style = \"text-align: left;\">0.7342</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: left;\">Column12</td><td style = \"text-align: left;\">0.250221</td><td style = \"text-align: left;\">0.0236</td><td style = \"text-align: left;\">0.24905</td><td style = \"text-align: left;\">0.706</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: left;\">Column13</td><td style = \"text-align: left;\">0.273305</td><td style = \"text-align: left;\">0.0184</td><td style = \"text-align: left;\">0.26395</td><td style = \"text-align: left;\">0.7131</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">50</td><td style = \"text-align: left;\">Column50</td><td style = \"text-align: left;\">0.020424</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">0.0179</td><td style = \"text-align: left;\">0.0825</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">51</td><td style = \"text-align: left;\">Column51</td><td style = \"text-align: left;\">0.0160687</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">0.0139</td><td style = \"text-align: left;\">0.1004</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">52</td><td style = \"text-align: left;\">Column52</td><td style = \"text-align: left;\">0.0134202</td><td style = \"text-align: left;\">0.0008</td><td style = \"text-align: left;\">0.0114</td><td style = \"text-align: left;\">0.0709</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">53</td><td style = \"text-align: left;\">Column53</td><td style = \"text-align: left;\">0.0107091</td><td style = \"text-align: left;\">0.0005</td><td style = \"text-align: left;\">0.00955</td><td style = \"text-align: left;\">0.039</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">54</td><td style = \"text-align: left;\">Column54</td><td style = \"text-align: left;\">0.0109409</td><td style = \"text-align: left;\">0.001</td><td style = \"text-align: left;\">0.0093</td><td style = \"text-align: left;\">0.0352</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">55</td><td style = \"text-align: left;\">Column55</td><td style = \"text-align: left;\">0.00929038</td><td style = \"text-align: left;\">0.0006</td><td style = \"text-align: left;\">0.0075</td><td style = \"text-align: left;\">0.0447</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">56</td><td style = \"text-align: left;\">Column56</td><td style = \"text-align: left;\">0.00822163</td><td style = \"text-align: left;\">0.0004</td><td style = \"text-align: left;\">0.00685</td><td style = \"text-align: left;\">0.0394</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">57</td><td style = \"text-align: left;\">Column57</td><td style = \"text-align: left;\">0.00782019</td><td style = \"text-align: left;\">0.0003</td><td style = \"text-align: left;\">0.00595</td><td style = \"text-align: left;\">0.0355</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">58</td><td style = \"text-align: left;\">Column58</td><td style = \"text-align: left;\">0.00794904</td><td style = \"text-align: left;\">0.0003</td><td style = \"text-align: left;\">0.0058</td><td style = \"text-align: left;\">0.044</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">59</td><td style = \"text-align: left;\">Column59</td><td style = \"text-align: left;\">0.00794135</td><td style = \"text-align: left;\">0.0001</td><td style = \"text-align: left;\">0.0064</td><td style = \"text-align: left;\">0.0364</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">60</td><td style = \"text-align: left;\">Column60</td><td style = \"text-align: left;\">0.00650721</td><td style = \"text-align: left;\">0.0006</td><td style = \"text-align: left;\">0.0053</td><td style = \"text-align: left;\">0.0439</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">61</td><td style = \"text-align: left;\">Column61</td><td style = \"font-style: italic; text-align: left;\"></td><td style = \"text-align: left;\">M</td><td style = \"font-style: italic; text-align: left;\"></td><td style = \"text-align: left;\">R</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">String1</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& variable & mean & min & median & max & nmissing & eltype\\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Union… & Any & Union… & Any & Int64 & DataType\\\\\n",
       "\t\\hline\n",
       "\t1 & Column1 & 0.0291639 & 0.0015 & 0.0228 & 0.1371 & 0 & Float64 \\\\\n",
       "\t2 & Column2 & 0.0384365 & 0.0006 & 0.0308 & 0.2339 & 0 & Float64 \\\\\n",
       "\t3 & Column3 & 0.0438322 & 0.0015 & 0.0343 & 0.3059 & 0 & Float64 \\\\\n",
       "\t4 & Column4 & 0.0538923 & 0.0058 & 0.04405 & 0.4264 & 0 & Float64 \\\\\n",
       "\t5 & Column5 & 0.0752024 & 0.0067 & 0.0625 & 0.401 & 0 & Float64 \\\\\n",
       "\t6 & Column6 & 0.10457 & 0.0102 & 0.09215 & 0.3823 & 0 & Float64 \\\\\n",
       "\t7 & Column7 & 0.121747 & 0.0033 & 0.10695 & 0.3729 & 0 & Float64 \\\\\n",
       "\t8 & Column8 & 0.134799 & 0.0055 & 0.1121 & 0.459 & 0 & Float64 \\\\\n",
       "\t9 & Column9 & 0.178003 & 0.0075 & 0.15225 & 0.6828 & 0 & Float64 \\\\\n",
       "\t10 & Column10 & 0.208259 & 0.0113 & 0.1824 & 0.7106 & 0 & Float64 \\\\\n",
       "\t11 & Column11 & 0.236013 & 0.0289 & 0.2248 & 0.7342 & 0 & Float64 \\\\\n",
       "\t12 & Column12 & 0.250221 & 0.0236 & 0.24905 & 0.706 & 0 & Float64 \\\\\n",
       "\t13 & Column13 & 0.273305 & 0.0184 & 0.26395 & 0.7131 & 0 & Float64 \\\\\n",
       "\t14 & Column14 & 0.296568 & 0.0273 & 0.2811 & 0.997 & 0 & Float64 \\\\\n",
       "\t15 & Column15 & 0.320201 & 0.0031 & 0.2817 & 1.0 & 0 & Float64 \\\\\n",
       "\t16 & Column16 & 0.378487 & 0.0162 & 0.3047 & 0.9988 & 0 & Float64 \\\\\n",
       "\t17 & Column17 & 0.415983 & 0.0349 & 0.3084 & 1.0 & 0 & Float64 \\\\\n",
       "\t18 & Column18 & 0.452318 & 0.0375 & 0.3683 & 1.0 & 0 & Float64 \\\\\n",
       "\t19 & Column19 & 0.504812 & 0.0494 & 0.43495 & 1.0 & 0 & Float64 \\\\\n",
       "\t20 & Column20 & 0.563047 & 0.0656 & 0.5425 & 1.0 & 0 & Float64 \\\\\n",
       "\t21 & Column21 & 0.60906 & 0.0512 & 0.6177 & 1.0 & 0 & Float64 \\\\\n",
       "\t22 & Column22 & 0.624275 & 0.0219 & 0.6649 & 1.0 & 0 & Float64 \\\\\n",
       "\t23 & Column23 & 0.646975 & 0.0563 & 0.6997 & 1.0 & 0 & Float64 \\\\\n",
       "\t24 & Column24 & 0.672654 & 0.0239 & 0.6985 & 1.0 & 0 & Float64 \\\\\n",
       "\t25 & Column25 & 0.675424 & 0.024 & 0.7211 & 1.0 & 0 & Float64 \\\\\n",
       "\t26 & Column26 & 0.699866 & 0.0921 & 0.7545 & 1.0 & 0 & Float64 \\\\\n",
       "\t27 & Column27 & 0.702155 & 0.0481 & 0.7456 & 1.0 & 0 & Float64 \\\\\n",
       "\t28 & Column28 & 0.694024 & 0.0284 & 0.7319 & 1.0 & 0 & Float64 \\\\\n",
       "\t29 & Column29 & 0.642074 & 0.0144 & 0.6808 & 1.0 & 0 & Float64 \\\\\n",
       "\t30 & Column30 & 0.580928 & 0.0613 & 0.60715 & 1.0 & 0 & Float64 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m61×7 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m variable \u001b[0m\u001b[1m mean       \u001b[0m\u001b[1m min    \u001b[0m\u001b[1m median  \u001b[0m\u001b[1m max    \u001b[0m\u001b[1m nmissing \u001b[0m\u001b[1m eltype   \u001b[0m\n",
       "     │\u001b[90m Symbol   \u001b[0m\u001b[90m Union…     \u001b[0m\u001b[90m Any    \u001b[0m\u001b[90m Union…  \u001b[0m\u001b[90m Any    \u001b[0m\u001b[90m Int64    \u001b[0m\u001b[90m DataType \u001b[0m\n",
       "─────┼───────────────────────────────────────────────────────────────────\n",
       "   1 │ Column1   0.0291639   0.0015  0.0228   0.1371         0  Float64\n",
       "   2 │ Column2   0.0384365   0.0006  0.0308   0.2339         0  Float64\n",
       "   3 │ Column3   0.0438322   0.0015  0.0343   0.3059         0  Float64\n",
       "   4 │ Column4   0.0538923   0.0058  0.04405  0.4264         0  Float64\n",
       "   5 │ Column5   0.0752024   0.0067  0.0625   0.401          0  Float64\n",
       "   6 │ Column6   0.10457     0.0102  0.09215  0.3823         0  Float64\n",
       "   7 │ Column7   0.121747    0.0033  0.10695  0.3729         0  Float64\n",
       "   8 │ Column8   0.134799    0.0055  0.1121   0.459          0  Float64\n",
       "   9 │ Column9   0.178003    0.0075  0.15225  0.6828         0  Float64\n",
       "  10 │ Column10  0.208259    0.0113  0.1824   0.7106         0  Float64\n",
       "  11 │ Column11  0.236013    0.0289  0.2248   0.7342         0  Float64\n",
       "  ⋮  │    ⋮          ⋮         ⋮        ⋮       ⋮        ⋮         ⋮\n",
       "  52 │ Column52  0.0134202   0.0008  0.0114   0.0709         0  Float64\n",
       "  53 │ Column53  0.0107091   0.0005  0.00955  0.039          0  Float64\n",
       "  54 │ Column54  0.0109409   0.001   0.0093   0.0352         0  Float64\n",
       "  55 │ Column55  0.00929038  0.0006  0.0075   0.0447         0  Float64\n",
       "  56 │ Column56  0.00822163  0.0004  0.00685  0.0394         0  Float64\n",
       "  57 │ Column57  0.00782019  0.0003  0.00595  0.0355         0  Float64\n",
       "  58 │ Column58  0.00794904  0.0003  0.0058   0.044          0  Float64\n",
       "  59 │ Column59  0.00794135  0.0001  0.0064   0.0364         0  Float64\n",
       "  60 │ Column60  0.00650721  0.0006  0.0053   0.0439         0  Float64\n",
       "  61 │ Column61 \u001b[90m            \u001b[0m M      \u001b[90m         \u001b[0m R              0  String1\n",
       "\u001b[36m                                                          40 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using UrlDownload\n",
    "using DataFrames\n",
    "using CSV\n",
    "using CategoricalArrays\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\"\n",
    "data = urldownload(url, true, format=:CSV, header=false) |> DataFrame\n",
    "describe(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ebf504",
   "metadata": {},
   "source": [
    "As it can be seen in the previos line, we have downloaded de data and pipe it, with the operator `|>`, into the function DataFrame. This is going to create an structure simular to a database table which is particular convinient to check for missing values or the ranges of the different variables. In fact, the library makes it particularly easy to deal with missing values with functions to fullfill or remove the samples with non-valid measures. However it is too long to see every single variable on the output report, if some queries are made we can identify  that here is no missing values. Additionally no variable is over 1.0 but some of them are not normalized. A similar structure can be found in other languages, like R or Python.\n",
    "\n",
    "As an example, of this process lets make the an additional column in order to convert to categorical the las column 60 which has a **M** for each Mine and an **R** for each sample of rock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4a07ecb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>208×62 DataFrame</span></div><div style = \"float: right;\"><span style = \"font-style: italic;\">183 rows omitted</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">Column1</th><th style = \"text-align: left;\">Column2</th><th style = \"text-align: left;\">Column3</th><th style = \"text-align: left;\">Column4</th><th style = \"text-align: left;\">Column5</th><th style = \"text-align: left;\">Column6</th><th style = \"text-align: left;\">Column7</th><th style = \"text-align: left;\">Column8</th><th style = \"text-align: left;\">Column9</th><th style = \"text-align: left;\">Column10</th><th style = \"text-align: left;\">Column11</th><th style = \"text-align: left;\">Column12</th><th style = \"text-align: left;\">Column13</th><th style = \"text-align: left;\">Column14</th><th style = \"text-align: left;\">Column15</th><th style = \"text-align: left;\">Column16</th><th style = \"text-align: left;\">Column17</th><th style = \"text-align: left;\">Column18</th><th style = \"text-align: left;\">Column19</th><th style = \"text-align: left;\">Column20</th><th style = \"text-align: left;\">Column21</th><th style = \"text-align: left;\">Column22</th><th style = \"text-align: left;\">Column23</th><th style = \"text-align: left;\">Column24</th><th style = \"text-align: left;\">Column25</th><th style = \"text-align: left;\">Column26</th><th style = \"text-align: left;\">Column27</th><th style = \"text-align: left;\">Column28</th><th style = \"text-align: left;\">Column29</th><th style = \"text-align: left;\">Column30</th><th style = \"text-align: left;\">Column31</th><th style = \"text-align: left;\">Column32</th><th style = \"text-align: left;\">Column33</th><th style = \"text-align: left;\">Column34</th><th style = \"text-align: left;\">Column35</th><th style = \"text-align: left;\">Column36</th><th style = \"text-align: left;\">Column37</th><th style = \"text-align: left;\">Column38</th><th style = \"text-align: left;\">Column39</th><th style = \"text-align: left;\">Column40</th><th style = \"text-align: left;\">Column41</th><th style = \"text-align: left;\">Column42</th><th style = \"text-align: left;\">Column43</th><th style = \"text-align: left;\">Column44</th><th style = \"text-align: left;\">Column45</th><th style = \"text-align: left;\">Column46</th><th style = \"text-align: left;\">Column47</th><th style = \"text-align: left;\">Column48</th><th style = \"text-align: left;\">Column49</th><th style = \"text-align: left;\">Column50</th><th style = \"text-align: left;\">Column51</th><th style = \"text-align: left;\">Column52</th><th style = \"text-align: left;\">Column53</th><th style = \"text-align: left;\">Column54</th><th style = \"text-align: left;\">Column55</th><th style = \"text-align: left;\">Column56</th><th style = \"text-align: left;\">Column57</th><th style = \"text-align: left;\">Column58</th><th style = \"text-align: left;\">Column59</th><th style = \"text-align: left;\">Column60</th><th style = \"text-align: left;\">Column61</th><th style = \"text-align: left;\">Mine</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"String1\" style = \"text-align: left;\">String1</th><th title = \"Bool\" style = \"text-align: left;\">Bool</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">0.02</td><td style = \"text-align: right;\">0.0371</td><td style = \"text-align: right;\">0.0428</td><td style = \"text-align: right;\">0.0207</td><td style = \"text-align: right;\">0.0954</td><td style = \"text-align: right;\">0.0986</td><td style = \"text-align: right;\">0.1539</td><td style = \"text-align: right;\">0.1601</td><td style = \"text-align: right;\">0.3109</td><td style = \"text-align: right;\">0.2111</td><td style = \"text-align: right;\">0.1609</td><td style = \"text-align: right;\">0.1582</td><td style = \"text-align: right;\">0.2238</td><td style = \"text-align: right;\">0.0645</td><td style = \"text-align: right;\">0.066</td><td style = \"text-align: right;\">0.2273</td><td style = \"text-align: right;\">0.31</td><td style = \"text-align: right;\">0.2999</td><td style = \"text-align: right;\">0.5078</td><td style = \"text-align: right;\">0.4797</td><td style = \"text-align: right;\">0.5783</td><td style = \"text-align: right;\">0.5071</td><td style = \"text-align: right;\">0.4328</td><td style = \"text-align: right;\">0.555</td><td style = \"text-align: right;\">0.6711</td><td style = \"text-align: right;\">0.6415</td><td style = \"text-align: right;\">0.7104</td><td style = \"text-align: right;\">0.808</td><td style = \"text-align: right;\">0.6791</td><td style = \"text-align: right;\">0.3857</td><td style = \"text-align: right;\">0.1307</td><td style = \"text-align: right;\">0.2604</td><td style = \"text-align: right;\">0.5121</td><td style = \"text-align: right;\">0.7547</td><td style = \"text-align: right;\">0.8537</td><td style = \"text-align: right;\">0.8507</td><td style = \"text-align: right;\">0.6692</td><td style = \"text-align: right;\">0.6097</td><td style = \"text-align: right;\">0.4943</td><td style = \"text-align: right;\">0.2744</td><td style = \"text-align: right;\">0.051</td><td style = \"text-align: right;\">0.2834</td><td style = \"text-align: right;\">0.2825</td><td style = \"text-align: right;\">0.4256</td><td style = \"text-align: right;\">0.2641</td><td style = \"text-align: right;\">0.1386</td><td style = \"text-align: right;\">0.1051</td><td style = \"text-align: right;\">0.1343</td><td style = \"text-align: right;\">0.0383</td><td style = \"text-align: right;\">0.0324</td><td style = \"text-align: right;\">0.0232</td><td style = \"text-align: right;\">0.0027</td><td style = \"text-align: right;\">0.0065</td><td style = \"text-align: right;\">0.0159</td><td style = \"text-align: right;\">0.0072</td><td style = \"text-align: right;\">0.0167</td><td style = \"text-align: right;\">0.018</td><td style = \"text-align: right;\">0.0084</td><td style = \"text-align: right;\">0.009</td><td style = \"text-align: right;\">0.0032</td><td style = \"text-align: left;\">R</td><td style = \"text-align: right;\">false</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">0.0453</td><td style = \"text-align: right;\">0.0523</td><td style = \"text-align: right;\">0.0843</td><td style = \"text-align: right;\">0.0689</td><td style = \"text-align: right;\">0.1183</td><td style = \"text-align: right;\">0.2583</td><td style = \"text-align: right;\">0.2156</td><td style = \"text-align: right;\">0.3481</td><td style = \"text-align: right;\">0.3337</td><td style = \"text-align: right;\">0.2872</td><td style = \"text-align: right;\">0.4918</td><td style = \"text-align: right;\">0.6552</td><td style = \"text-align: right;\">0.6919</td><td style = \"text-align: right;\">0.7797</td><td style = \"text-align: right;\">0.7464</td><td style = \"text-align: right;\">0.9444</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.8874</td><td style = \"text-align: right;\">0.8024</td><td style = \"text-align: right;\">0.7818</td><td style = \"text-align: right;\">0.5212</td><td style = \"text-align: right;\">0.4052</td><td style = \"text-align: right;\">0.3957</td><td style = \"text-align: right;\">0.3914</td><td style = \"text-align: right;\">0.325</td><td style = \"text-align: right;\">0.32</td><td style = \"text-align: right;\">0.3271</td><td style = \"text-align: right;\">0.2767</td><td style = \"text-align: right;\">0.4423</td><td style = \"text-align: right;\">0.2028</td><td style = \"text-align: right;\">0.3788</td><td style = \"text-align: right;\">0.2947</td><td style = \"text-align: right;\">0.1984</td><td style = \"text-align: right;\">0.2341</td><td style = \"text-align: right;\">0.1306</td><td style = \"text-align: right;\">0.4182</td><td style = \"text-align: right;\">0.3835</td><td style = \"text-align: right;\">0.1057</td><td style = \"text-align: right;\">0.184</td><td style = \"text-align: right;\">0.197</td><td style = \"text-align: right;\">0.1674</td><td style = \"text-align: right;\">0.0583</td><td style = \"text-align: right;\">0.1401</td><td style = \"text-align: right;\">0.1628</td><td style = \"text-align: right;\">0.0621</td><td style = \"text-align: right;\">0.0203</td><td style = \"text-align: right;\">0.053</td><td style = \"text-align: right;\">0.0742</td><td style = \"text-align: right;\">0.0409</td><td style = \"text-align: right;\">0.0061</td><td style = \"text-align: right;\">0.0125</td><td style = \"text-align: right;\">0.0084</td><td style = \"text-align: right;\">0.0089</td><td style = \"text-align: right;\">0.0048</td><td style = \"text-align: right;\">0.0094</td><td style = \"text-align: right;\">0.0191</td><td style = \"text-align: right;\">0.014</td><td style = \"text-align: right;\">0.0049</td><td style = \"text-align: right;\">0.0052</td><td style = \"text-align: right;\">0.0044</td><td style = \"text-align: left;\">R</td><td style = \"text-align: right;\">false</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">0.0262</td><td style = \"text-align: right;\">0.0582</td><td style = \"text-align: right;\">0.1099</td><td style = \"text-align: right;\">0.1083</td><td style = \"text-align: right;\">0.0974</td><td style = \"text-align: right;\">0.228</td><td style = \"text-align: right;\">0.2431</td><td style = \"text-align: right;\">0.3771</td><td style = \"text-align: right;\">0.5598</td><td style = \"text-align: right;\">0.6194</td><td style = \"text-align: right;\">0.6333</td><td style = \"text-align: right;\">0.706</td><td style = \"text-align: right;\">0.5544</td><td style = \"text-align: right;\">0.532</td><td style = \"text-align: right;\">0.6479</td><td style = \"text-align: right;\">0.6931</td><td style = \"text-align: right;\">0.6759</td><td style = \"text-align: right;\">0.7551</td><td style = \"text-align: right;\">0.8929</td><td style = \"text-align: right;\">0.8619</td><td style = \"text-align: right;\">0.7974</td><td style = \"text-align: right;\">0.6737</td><td style = \"text-align: right;\">0.4293</td><td style = \"text-align: right;\">0.3648</td><td style = \"text-align: right;\">0.5331</td><td style = \"text-align: right;\">0.2413</td><td style = \"text-align: right;\">0.507</td><td style = \"text-align: right;\">0.8533</td><td style = \"text-align: right;\">0.6036</td><td style = \"text-align: right;\">0.8514</td><td style = \"text-align: right;\">0.8512</td><td style = \"text-align: right;\">0.5045</td><td style = \"text-align: right;\">0.1862</td><td style = \"text-align: right;\">0.2709</td><td style = \"text-align: right;\">0.4232</td><td style = \"text-align: right;\">0.3043</td><td style = \"text-align: right;\">0.6116</td><td style = \"text-align: right;\">0.6756</td><td style = \"text-align: right;\">0.5375</td><td style = \"text-align: right;\">0.4719</td><td style = \"text-align: right;\">0.4647</td><td style = \"text-align: right;\">0.2587</td><td style = \"text-align: right;\">0.2129</td><td style = \"text-align: right;\">0.2222</td><td style = \"text-align: right;\">0.2111</td><td style = \"text-align: right;\">0.0176</td><td style = \"text-align: right;\">0.1348</td><td style = \"text-align: right;\">0.0744</td><td style = \"text-align: right;\">0.013</td><td style = \"text-align: right;\">0.0106</td><td style = \"text-align: right;\">0.0033</td><td style = \"text-align: right;\">0.0232</td><td style = \"text-align: right;\">0.0166</td><td style = \"text-align: right;\">0.0095</td><td style = \"text-align: right;\">0.018</td><td style = \"text-align: right;\">0.0244</td><td style = \"text-align: right;\">0.0316</td><td style = \"text-align: right;\">0.0164</td><td style = \"text-align: right;\">0.0095</td><td style = \"text-align: right;\">0.0078</td><td style = \"text-align: left;\">R</td><td style = \"text-align: right;\">false</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">0.01</td><td style = \"text-align: right;\">0.0171</td><td style = \"text-align: right;\">0.0623</td><td style = \"text-align: right;\">0.0205</td><td style = \"text-align: right;\">0.0205</td><td style = \"text-align: right;\">0.0368</td><td style = \"text-align: right;\">0.1098</td><td style = \"text-align: right;\">0.1276</td><td style = \"text-align: right;\">0.0598</td><td style = \"text-align: right;\">0.1264</td><td style = \"text-align: right;\">0.0881</td><td style = \"text-align: right;\">0.1992</td><td style = \"text-align: right;\">0.0184</td><td style = \"text-align: right;\">0.2261</td><td style = \"text-align: right;\">0.1729</td><td style = \"text-align: right;\">0.2131</td><td style = \"text-align: right;\">0.0693</td><td style = \"text-align: right;\">0.2281</td><td style = \"text-align: right;\">0.406</td><td style = \"text-align: right;\">0.3973</td><td style = \"text-align: right;\">0.2741</td><td style = \"text-align: right;\">0.369</td><td style = \"text-align: right;\">0.5556</td><td style = \"text-align: right;\">0.4846</td><td style = \"text-align: right;\">0.314</td><td style = \"text-align: right;\">0.5334</td><td style = \"text-align: right;\">0.5256</td><td style = \"text-align: right;\">0.252</td><td style = \"text-align: right;\">0.209</td><td style = \"text-align: right;\">0.3559</td><td style = \"text-align: right;\">0.626</td><td style = \"text-align: right;\">0.734</td><td style = \"text-align: right;\">0.612</td><td style = \"text-align: right;\">0.3497</td><td style = \"text-align: right;\">0.3953</td><td style = \"text-align: right;\">0.3012</td><td style = \"text-align: right;\">0.5408</td><td style = \"text-align: right;\">0.8814</td><td style = \"text-align: right;\">0.9857</td><td style = \"text-align: right;\">0.9167</td><td style = \"text-align: right;\">0.6121</td><td style = \"text-align: right;\">0.5006</td><td style = \"text-align: right;\">0.321</td><td style = \"text-align: right;\">0.3202</td><td style = \"text-align: right;\">0.4295</td><td style = \"text-align: right;\">0.3654</td><td style = \"text-align: right;\">0.2655</td><td style = \"text-align: right;\">0.1576</td><td style = \"text-align: right;\">0.0681</td><td style = \"text-align: right;\">0.0294</td><td style = \"text-align: right;\">0.0241</td><td style = \"text-align: right;\">0.0121</td><td style = \"text-align: right;\">0.0036</td><td style = \"text-align: right;\">0.015</td><td style = \"text-align: right;\">0.0085</td><td style = \"text-align: right;\">0.0073</td><td style = \"text-align: right;\">0.005</td><td style = \"text-align: right;\">0.0044</td><td style = \"text-align: right;\">0.004</td><td style = \"text-align: right;\">0.0117</td><td style = \"text-align: left;\">R</td><td style = \"text-align: right;\">false</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">0.0762</td><td style = \"text-align: right;\">0.0666</td><td style = \"text-align: right;\">0.0481</td><td style = \"text-align: right;\">0.0394</td><td style = \"text-align: right;\">0.059</td><td style = \"text-align: right;\">0.0649</td><td style = \"text-align: right;\">0.1209</td><td style = \"text-align: right;\">0.2467</td><td style = \"text-align: right;\">0.3564</td><td style = \"text-align: right;\">0.4459</td><td style = \"text-align: right;\">0.4152</td><td style = \"text-align: right;\">0.3952</td><td style = \"text-align: right;\">0.4256</td><td style = \"text-align: right;\">0.4135</td><td style = \"text-align: right;\">0.4528</td><td style = \"text-align: right;\">0.5326</td><td style = \"text-align: right;\">0.7306</td><td style = \"text-align: right;\">0.6193</td><td style = \"text-align: right;\">0.2032</td><td style = \"text-align: right;\">0.4636</td><td style = \"text-align: right;\">0.4148</td><td style = \"text-align: right;\">0.4292</td><td style = \"text-align: right;\">0.573</td><td style = \"text-align: right;\">0.5399</td><td style = \"text-align: right;\">0.3161</td><td style = \"text-align: right;\">0.2285</td><td style = \"text-align: right;\">0.6995</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.7262</td><td style = \"text-align: right;\">0.4724</td><td style = \"text-align: right;\">0.5103</td><td style = \"text-align: right;\">0.5459</td><td style = \"text-align: right;\">0.2881</td><td style = \"text-align: right;\">0.0981</td><td style = \"text-align: right;\">0.1951</td><td style = \"text-align: right;\">0.4181</td><td style = \"text-align: right;\">0.4604</td><td style = \"text-align: right;\">0.3217</td><td style = \"text-align: right;\">0.2828</td><td style = \"text-align: right;\">0.243</td><td style = \"text-align: right;\">0.1979</td><td style = \"text-align: right;\">0.2444</td><td style = \"text-align: right;\">0.1847</td><td style = \"text-align: right;\">0.0841</td><td style = \"text-align: right;\">0.0692</td><td style = \"text-align: right;\">0.0528</td><td style = \"text-align: right;\">0.0357</td><td style = \"text-align: right;\">0.0085</td><td style = \"text-align: right;\">0.023</td><td style = \"text-align: right;\">0.0046</td><td style = \"text-align: right;\">0.0156</td><td style = \"text-align: right;\">0.0031</td><td style = \"text-align: right;\">0.0054</td><td style = \"text-align: right;\">0.0105</td><td style = \"text-align: right;\">0.011</td><td style = \"text-align: right;\">0.0015</td><td style = \"text-align: right;\">0.0072</td><td style = \"text-align: right;\">0.0048</td><td style = \"text-align: right;\">0.0107</td><td style = \"text-align: right;\">0.0094</td><td style = \"text-align: left;\">R</td><td style = \"text-align: right;\">false</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: right;\">0.0286</td><td style = \"text-align: right;\">0.0453</td><td style = \"text-align: right;\">0.0277</td><td style = \"text-align: right;\">0.0174</td><td style = \"text-align: right;\">0.0384</td><td style = \"text-align: right;\">0.099</td><td style = \"text-align: right;\">0.1201</td><td style = \"text-align: right;\">0.1833</td><td style = \"text-align: right;\">0.2105</td><td style = \"text-align: right;\">0.3039</td><td style = \"text-align: right;\">0.2988</td><td style = \"text-align: right;\">0.425</td><td style = \"text-align: right;\">0.6343</td><td style = \"text-align: right;\">0.8198</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.9988</td><td style = \"text-align: right;\">0.9508</td><td style = \"text-align: right;\">0.9025</td><td style = \"text-align: right;\">0.7234</td><td style = \"text-align: right;\">0.5122</td><td style = \"text-align: right;\">0.2074</td><td style = \"text-align: right;\">0.3985</td><td style = \"text-align: right;\">0.589</td><td style = \"text-align: right;\">0.2872</td><td style = \"text-align: right;\">0.2043</td><td style = \"text-align: right;\">0.5782</td><td style = \"text-align: right;\">0.5389</td><td style = \"text-align: right;\">0.375</td><td style = \"text-align: right;\">0.3411</td><td style = \"text-align: right;\">0.5067</td><td style = \"text-align: right;\">0.558</td><td style = \"text-align: right;\">0.4778</td><td style = \"text-align: right;\">0.3299</td><td style = \"text-align: right;\">0.2198</td><td style = \"text-align: right;\">0.1407</td><td style = \"text-align: right;\">0.2856</td><td style = \"text-align: right;\">0.3807</td><td style = \"text-align: right;\">0.4158</td><td style = \"text-align: right;\">0.4054</td><td style = \"text-align: right;\">0.3296</td><td style = \"text-align: right;\">0.2707</td><td style = \"text-align: right;\">0.265</td><td style = \"text-align: right;\">0.0723</td><td style = \"text-align: right;\">0.1238</td><td style = \"text-align: right;\">0.1192</td><td style = \"text-align: right;\">0.1089</td><td style = \"text-align: right;\">0.0623</td><td style = \"text-align: right;\">0.0494</td><td style = \"text-align: right;\">0.0264</td><td style = \"text-align: right;\">0.0081</td><td style = \"text-align: right;\">0.0104</td><td style = \"text-align: right;\">0.0045</td><td style = \"text-align: right;\">0.0014</td><td style = \"text-align: right;\">0.0038</td><td style = \"text-align: right;\">0.0013</td><td style = \"text-align: right;\">0.0089</td><td style = \"text-align: right;\">0.0057</td><td style = \"text-align: right;\">0.0027</td><td style = \"text-align: right;\">0.0051</td><td style = \"text-align: right;\">0.0062</td><td style = \"text-align: left;\">R</td><td style = \"text-align: right;\">false</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: right;\">0.0317</td><td style = \"text-align: right;\">0.0956</td><td style = \"text-align: right;\">0.1321</td><td style = \"text-align: right;\">0.1408</td><td style = \"text-align: right;\">0.1674</td><td style = \"text-align: right;\">0.171</td><td style = \"text-align: right;\">0.0731</td><td style = \"text-align: right;\">0.1401</td><td style = \"text-align: right;\">0.2083</td><td style = \"text-align: right;\">0.3513</td><td style = \"text-align: right;\">0.1786</td><td style = \"text-align: right;\">0.0658</td><td style = \"text-align: right;\">0.0513</td><td style = \"text-align: right;\">0.3752</td><td style = \"text-align: right;\">0.5419</td><td style = \"text-align: right;\">0.544</td><td style = \"text-align: right;\">0.515</td><td style = \"text-align: right;\">0.4262</td><td style = \"text-align: right;\">0.2024</td><td style = \"text-align: right;\">0.4233</td><td style = \"text-align: right;\">0.7723</td><td style = \"text-align: right;\">0.9735</td><td style = \"text-align: right;\">0.939</td><td style = \"text-align: right;\">0.5559</td><td style = \"text-align: right;\">0.5268</td><td style = \"text-align: right;\">0.6826</td><td style = \"text-align: right;\">0.5713</td><td style = \"text-align: right;\">0.5429</td><td style = \"text-align: right;\">0.2177</td><td style = \"text-align: right;\">0.2149</td><td style = \"text-align: right;\">0.5811</td><td style = \"text-align: right;\">0.6323</td><td style = \"text-align: right;\">0.2965</td><td style = \"text-align: right;\">0.1873</td><td style = \"text-align: right;\">0.2969</td><td style = \"text-align: right;\">0.5163</td><td style = \"text-align: right;\">0.6153</td><td style = \"text-align: right;\">0.4283</td><td style = \"text-align: right;\">0.5479</td><td style = \"text-align: right;\">0.6133</td><td style = \"text-align: right;\">0.5017</td><td style = \"text-align: right;\">0.2377</td><td style = \"text-align: right;\">0.1957</td><td style = \"text-align: right;\">0.1749</td><td style = \"text-align: right;\">0.1304</td><td style = \"text-align: right;\">0.0597</td><td style = \"text-align: right;\">0.1124</td><td style = \"text-align: right;\">0.1047</td><td style = \"text-align: right;\">0.0507</td><td style = \"text-align: right;\">0.0159</td><td style = \"text-align: right;\">0.0195</td><td style = \"text-align: right;\">0.0201</td><td style = \"text-align: right;\">0.0248</td><td style = \"text-align: right;\">0.0131</td><td style = \"text-align: right;\">0.007</td><td style = \"text-align: right;\">0.0138</td><td style = \"text-align: right;\">0.0092</td><td style = \"text-align: right;\">0.0143</td><td style = \"text-align: right;\">0.0036</td><td style = \"text-align: right;\">0.0103</td><td style = \"text-align: left;\">R</td><td style = \"text-align: right;\">false</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: right;\">0.0519</td><td style = \"text-align: right;\">0.0548</td><td style = \"text-align: right;\">0.0842</td><td style = \"text-align: right;\">0.0319</td><td style = \"text-align: right;\">0.1158</td><td style = \"text-align: right;\">0.0922</td><td style = \"text-align: right;\">0.1027</td><td style = \"text-align: right;\">0.0613</td><td style = \"text-align: right;\">0.1465</td><td style = \"text-align: right;\">0.2838</td><td style = \"text-align: right;\">0.2802</td><td style = \"text-align: right;\">0.3086</td><td style = \"text-align: right;\">0.2657</td><td style = \"text-align: right;\">0.3801</td><td style = \"text-align: right;\">0.5626</td><td style = \"text-align: right;\">0.4376</td><td style = \"text-align: right;\">0.2617</td><td style = \"text-align: right;\">0.1199</td><td style = \"text-align: right;\">0.6676</td><td style = \"text-align: right;\">0.9402</td><td style = \"text-align: right;\">0.7832</td><td style = \"text-align: right;\">0.5352</td><td style = \"text-align: right;\">0.6809</td><td style = \"text-align: right;\">0.9174</td><td style = \"text-align: right;\">0.7613</td><td style = \"text-align: right;\">0.822</td><td style = \"text-align: right;\">0.8872</td><td style = \"text-align: right;\">0.6091</td><td style = \"text-align: right;\">0.2967</td><td style = \"text-align: right;\">0.1103</td><td style = \"text-align: right;\">0.1318</td><td style = \"text-align: right;\">0.0624</td><td style = \"text-align: right;\">0.099</td><td style = \"text-align: right;\">0.4006</td><td style = \"text-align: right;\">0.3666</td><td style = \"text-align: right;\">0.105</td><td style = \"text-align: right;\">0.1915</td><td style = \"text-align: right;\">0.393</td><td style = \"text-align: right;\">0.4288</td><td style = \"text-align: right;\">0.2546</td><td style = \"text-align: right;\">0.1151</td><td style = \"text-align: right;\">0.2196</td><td style = \"text-align: right;\">0.1879</td><td style = \"text-align: right;\">0.1437</td><td style = \"text-align: right;\">0.2146</td><td style = \"text-align: right;\">0.236</td><td style = \"text-align: right;\">0.1125</td><td style = \"text-align: right;\">0.0254</td><td style = \"text-align: right;\">0.0285</td><td style = \"text-align: right;\">0.0178</td><td style = \"text-align: right;\">0.0052</td><td style = \"text-align: right;\">0.0081</td><td style = \"text-align: right;\">0.012</td><td style = \"text-align: right;\">0.0045</td><td style = \"text-align: right;\">0.0121</td><td style = \"text-align: right;\">0.0097</td><td style = \"text-align: right;\">0.0085</td><td style = \"text-align: right;\">0.0047</td><td style = \"text-align: right;\">0.0048</td><td style = \"text-align: right;\">0.0053</td><td style = \"text-align: left;\">R</td><td style = \"text-align: right;\">false</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: right;\">0.0223</td><td style = \"text-align: right;\">0.0375</td><td style = \"text-align: right;\">0.0484</td><td style = \"text-align: right;\">0.0475</td><td style = \"text-align: right;\">0.0647</td><td style = \"text-align: right;\">0.0591</td><td style = \"text-align: right;\">0.0753</td><td style = \"text-align: right;\">0.0098</td><td style = \"text-align: right;\">0.0684</td><td style = \"text-align: right;\">0.1487</td><td style = \"text-align: right;\">0.1156</td><td style = \"text-align: right;\">0.1654</td><td style = \"text-align: right;\">0.3833</td><td style = \"text-align: right;\">0.3598</td><td style = \"text-align: right;\">0.1713</td><td style = \"text-align: right;\">0.1136</td><td style = \"text-align: right;\">0.0349</td><td style = \"text-align: right;\">0.3796</td><td style = \"text-align: right;\">0.7401</td><td style = \"text-align: right;\">0.9925</td><td style = \"text-align: right;\">0.9802</td><td style = \"text-align: right;\">0.889</td><td style = \"text-align: right;\">0.6712</td><td style = \"text-align: right;\">0.4286</td><td style = \"text-align: right;\">0.3374</td><td style = \"text-align: right;\">0.7366</td><td style = \"text-align: right;\">0.9611</td><td style = \"text-align: right;\">0.7353</td><td style = \"text-align: right;\">0.4856</td><td style = \"text-align: right;\">0.1594</td><td style = \"text-align: right;\">0.3007</td><td style = \"text-align: right;\">0.4096</td><td style = \"text-align: right;\">0.317</td><td style = \"text-align: right;\">0.3305</td><td style = \"text-align: right;\">0.3408</td><td style = \"text-align: right;\">0.2186</td><td style = \"text-align: right;\">0.2463</td><td style = \"text-align: right;\">0.2726</td><td style = \"text-align: right;\">0.168</td><td style = \"text-align: right;\">0.2792</td><td style = \"text-align: right;\">0.2558</td><td style = \"text-align: right;\">0.174</td><td style = \"text-align: right;\">0.2121</td><td style = \"text-align: right;\">0.1099</td><td style = \"text-align: right;\">0.0985</td><td style = \"text-align: right;\">0.1271</td><td style = \"text-align: right;\">0.1459</td><td style = \"text-align: right;\">0.1164</td><td style = \"text-align: right;\">0.0777</td><td style = \"text-align: right;\">0.0439</td><td style = \"text-align: right;\">0.0061</td><td style = \"text-align: right;\">0.0145</td><td style = \"text-align: right;\">0.0128</td><td style = \"text-align: right;\">0.0145</td><td style = \"text-align: right;\">0.0058</td><td style = \"text-align: right;\">0.0049</td><td style = \"text-align: right;\">0.0065</td><td style = \"text-align: right;\">0.0093</td><td style = \"text-align: right;\">0.0059</td><td style = \"text-align: right;\">0.0022</td><td style = \"text-align: left;\">R</td><td style = \"text-align: right;\">false</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: right;\">0.0164</td><td style = \"text-align: right;\">0.0173</td><td style = \"text-align: right;\">0.0347</td><td style = \"text-align: right;\">0.007</td><td style = \"text-align: right;\">0.0187</td><td style = \"text-align: right;\">0.0671</td><td style = \"text-align: right;\">0.1056</td><td style = \"text-align: right;\">0.0697</td><td style = \"text-align: right;\">0.0962</td><td style = \"text-align: right;\">0.0251</td><td style = \"text-align: right;\">0.0801</td><td style = \"text-align: right;\">0.1056</td><td style = \"text-align: right;\">0.1266</td><td style = \"text-align: right;\">0.089</td><td style = \"text-align: right;\">0.0198</td><td style = \"text-align: right;\">0.1133</td><td style = \"text-align: right;\">0.2826</td><td style = \"text-align: right;\">0.3234</td><td style = \"text-align: right;\">0.3238</td><td style = \"text-align: right;\">0.4333</td><td style = \"text-align: right;\">0.6068</td><td style = \"text-align: right;\">0.7652</td><td style = \"text-align: right;\">0.9203</td><td style = \"text-align: right;\">0.9719</td><td style = \"text-align: right;\">0.9207</td><td style = \"text-align: right;\">0.7545</td><td style = \"text-align: right;\">0.8289</td><td style = \"text-align: right;\">0.8907</td><td style = \"text-align: right;\">0.7309</td><td style = \"text-align: right;\">0.6896</td><td style = \"text-align: right;\">0.5829</td><td style = \"text-align: right;\">0.4935</td><td style = \"text-align: right;\">0.3101</td><td style = \"text-align: right;\">0.0306</td><td style = \"text-align: right;\">0.0244</td><td style = \"text-align: right;\">0.1108</td><td style = \"text-align: right;\">0.1594</td><td style = \"text-align: right;\">0.1371</td><td style = \"text-align: right;\">0.0696</td><td style = \"text-align: right;\">0.0452</td><td style = \"text-align: right;\">0.062</td><td style = \"text-align: right;\">0.1421</td><td style = \"text-align: right;\">0.1597</td><td style = \"text-align: right;\">0.1384</td><td style = \"text-align: right;\">0.0372</td><td style = \"text-align: right;\">0.0688</td><td style = \"text-align: right;\">0.0867</td><td style = \"text-align: right;\">0.0513</td><td style = \"text-align: right;\">0.0092</td><td style = \"text-align: right;\">0.0198</td><td style = \"text-align: right;\">0.0118</td><td style = \"text-align: right;\">0.009</td><td style = \"text-align: right;\">0.0223</td><td style = \"text-align: right;\">0.0179</td><td style = \"text-align: right;\">0.0084</td><td style = \"text-align: right;\">0.0068</td><td style = \"text-align: right;\">0.0032</td><td style = \"text-align: right;\">0.0035</td><td style = \"text-align: right;\">0.0056</td><td style = \"text-align: right;\">0.004</td><td style = \"text-align: left;\">R</td><td style = \"text-align: right;\">false</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: right;\">0.0039</td><td style = \"text-align: right;\">0.0063</td><td style = \"text-align: right;\">0.0152</td><td style = \"text-align: right;\">0.0336</td><td style = \"text-align: right;\">0.031</td><td style = \"text-align: right;\">0.0284</td><td style = \"text-align: right;\">0.0396</td><td style = \"text-align: right;\">0.0272</td><td style = \"text-align: right;\">0.0323</td><td style = \"text-align: right;\">0.0452</td><td style = \"text-align: right;\">0.0492</td><td style = \"text-align: right;\">0.0996</td><td style = \"text-align: right;\">0.1424</td><td style = \"text-align: right;\">0.1194</td><td style = \"text-align: right;\">0.0628</td><td style = \"text-align: right;\">0.0907</td><td style = \"text-align: right;\">0.1177</td><td style = \"text-align: right;\">0.1429</td><td style = \"text-align: right;\">0.1223</td><td style = \"text-align: right;\">0.1104</td><td style = \"text-align: right;\">0.1847</td><td style = \"text-align: right;\">0.3715</td><td style = \"text-align: right;\">0.4382</td><td style = \"text-align: right;\">0.5707</td><td style = \"text-align: right;\">0.6654</td><td style = \"text-align: right;\">0.7476</td><td style = \"text-align: right;\">0.7654</td><td style = \"text-align: right;\">0.8555</td><td style = \"text-align: right;\">0.972</td><td style = \"text-align: right;\">0.9221</td><td style = \"text-align: right;\">0.7502</td><td style = \"text-align: right;\">0.7209</td><td style = \"text-align: right;\">0.7757</td><td style = \"text-align: right;\">0.6055</td><td style = \"text-align: right;\">0.5021</td><td style = \"text-align: right;\">0.4499</td><td style = \"text-align: right;\">0.3947</td><td style = \"text-align: right;\">0.4281</td><td style = \"text-align: right;\">0.4427</td><td style = \"text-align: right;\">0.3749</td><td style = \"text-align: right;\">0.1972</td><td style = \"text-align: right;\">0.0511</td><td style = \"text-align: right;\">0.0793</td><td style = \"text-align: right;\">0.1269</td><td style = \"text-align: right;\">0.1533</td><td style = \"text-align: right;\">0.069</td><td style = \"text-align: right;\">0.0402</td><td style = \"text-align: right;\">0.0534</td><td style = \"text-align: right;\">0.0228</td><td style = \"text-align: right;\">0.0073</td><td style = \"text-align: right;\">0.0062</td><td style = \"text-align: right;\">0.0062</td><td style = \"text-align: right;\">0.012</td><td style = \"text-align: right;\">0.0052</td><td style = \"text-align: right;\">0.0056</td><td style = \"text-align: right;\">0.0093</td><td style = \"text-align: right;\">0.0042</td><td style = \"text-align: right;\">0.0003</td><td style = \"text-align: right;\">0.0053</td><td style = \"text-align: right;\">0.0036</td><td style = \"text-align: left;\">R</td><td style = \"text-align: right;\">false</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: right;\">0.0123</td><td style = \"text-align: right;\">0.0309</td><td style = \"text-align: right;\">0.0169</td><td style = \"text-align: right;\">0.0313</td><td style = \"text-align: right;\">0.0358</td><td style = \"text-align: right;\">0.0102</td><td style = \"text-align: right;\">0.0182</td><td style = \"text-align: right;\">0.0579</td><td style = \"text-align: right;\">0.1122</td><td style = \"text-align: right;\">0.0835</td><td style = \"text-align: right;\">0.0548</td><td style = \"text-align: right;\">0.0847</td><td style = \"text-align: right;\">0.2026</td><td style = \"text-align: right;\">0.2557</td><td style = \"text-align: right;\">0.187</td><td style = \"text-align: right;\">0.2032</td><td style = \"text-align: right;\">0.1463</td><td style = \"text-align: right;\">0.2849</td><td style = \"text-align: right;\">0.5824</td><td style = \"text-align: right;\">0.7728</td><td style = \"text-align: right;\">0.7852</td><td style = \"text-align: right;\">0.8515</td><td style = \"text-align: right;\">0.5312</td><td style = \"text-align: right;\">0.3653</td><td style = \"text-align: right;\">0.5973</td><td style = \"text-align: right;\">0.8275</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.8673</td><td style = \"text-align: right;\">0.6301</td><td style = \"text-align: right;\">0.4591</td><td style = \"text-align: right;\">0.394</td><td style = \"text-align: right;\">0.2576</td><td style = \"text-align: right;\">0.2817</td><td style = \"text-align: right;\">0.2641</td><td style = \"text-align: right;\">0.2757</td><td style = \"text-align: right;\">0.2698</td><td style = \"text-align: right;\">0.3994</td><td style = \"text-align: right;\">0.4576</td><td style = \"text-align: right;\">0.394</td><td style = \"text-align: right;\">0.2522</td><td style = \"text-align: right;\">0.1782</td><td style = \"text-align: right;\">0.1354</td><td style = \"text-align: right;\">0.0516</td><td style = \"text-align: right;\">0.0337</td><td style = \"text-align: right;\">0.0894</td><td style = \"text-align: right;\">0.0861</td><td style = \"text-align: right;\">0.0872</td><td style = \"text-align: right;\">0.0445</td><td style = \"text-align: right;\">0.0134</td><td style = \"text-align: right;\">0.0217</td><td style = \"text-align: right;\">0.0188</td><td style = \"text-align: right;\">0.0133</td><td style = \"text-align: right;\">0.0265</td><td style = \"text-align: right;\">0.0224</td><td style = \"text-align: right;\">0.0074</td><td style = \"text-align: right;\">0.0118</td><td style = \"text-align: right;\">0.0026</td><td style = \"text-align: right;\">0.0092</td><td style = \"text-align: right;\">0.0009</td><td style = \"text-align: right;\">0.0044</td><td style = \"text-align: left;\">R</td><td style = \"text-align: right;\">false</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: right;\">0.0079</td><td style = \"text-align: right;\">0.0086</td><td style = \"text-align: right;\">0.0055</td><td style = \"text-align: right;\">0.025</td><td style = \"text-align: right;\">0.0344</td><td style = \"text-align: right;\">0.0546</td><td style = \"text-align: right;\">0.0528</td><td style = \"text-align: right;\">0.0958</td><td style = \"text-align: right;\">0.1009</td><td style = \"text-align: right;\">0.124</td><td style = \"text-align: right;\">0.1097</td><td style = \"text-align: right;\">0.1215</td><td style = \"text-align: right;\">0.1874</td><td style = \"text-align: right;\">0.3383</td><td style = \"text-align: right;\">0.3227</td><td style = \"text-align: right;\">0.2723</td><td style = \"text-align: right;\">0.3943</td><td style = \"text-align: right;\">0.6432</td><td style = \"text-align: right;\">0.7271</td><td style = \"text-align: right;\">0.8673</td><td style = \"text-align: right;\">0.9674</td><td style = \"text-align: right;\">0.9847</td><td style = \"text-align: right;\">0.948</td><td style = \"text-align: right;\">0.8036</td><td style = \"text-align: right;\">0.6833</td><td style = \"text-align: right;\">0.5136</td><td style = \"text-align: right;\">0.309</td><td style = \"text-align: right;\">0.0832</td><td style = \"text-align: right;\">0.4019</td><td style = \"text-align: right;\">0.2344</td><td style = \"text-align: right;\">0.1905</td><td style = \"text-align: right;\">0.1235</td><td style = \"text-align: right;\">0.1717</td><td style = \"text-align: right;\">0.2351</td><td style = \"text-align: right;\">0.2489</td><td style = \"text-align: right;\">0.3649</td><td style = \"text-align: right;\">0.3382</td><td style = \"text-align: right;\">0.1589</td><td style = \"text-align: right;\">0.0989</td><td style = \"text-align: right;\">0.1089</td><td style = \"text-align: right;\">0.1043</td><td style = \"text-align: right;\">0.0839</td><td style = \"text-align: right;\">0.1391</td><td style = \"text-align: right;\">0.0819</td><td style = \"text-align: right;\">0.0678</td><td style = \"text-align: right;\">0.0663</td><td style = \"text-align: right;\">0.1202</td><td style = \"text-align: right;\">0.0692</td><td style = \"text-align: right;\">0.0152</td><td style = \"text-align: right;\">0.0266</td><td style = \"text-align: right;\">0.0174</td><td style = \"text-align: right;\">0.0176</td><td style = \"text-align: right;\">0.0127</td><td style = \"text-align: right;\">0.0088</td><td style = \"text-align: right;\">0.0098</td><td style = \"text-align: right;\">0.0019</td><td style = \"text-align: right;\">0.0059</td><td style = \"text-align: right;\">0.0058</td><td style = \"text-align: right;\">0.0059</td><td style = \"text-align: right;\">0.0032</td><td style = \"text-align: left;\">R</td><td style = \"text-align: right;\">false</td></tr><tr><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">197</td><td style = \"text-align: right;\">0.005</td><td style = \"text-align: right;\">0.0017</td><td style = \"text-align: right;\">0.027</td><td style = \"text-align: right;\">0.045</td><td style = \"text-align: right;\">0.0958</td><td style = \"text-align: right;\">0.083</td><td style = \"text-align: right;\">0.0879</td><td style = \"text-align: right;\">0.122</td><td style = \"text-align: right;\">0.1977</td><td style = \"text-align: right;\">0.2282</td><td style = \"text-align: right;\">0.2521</td><td style = \"text-align: right;\">0.3484</td><td style = \"text-align: right;\">0.3309</td><td style = \"text-align: right;\">0.2614</td><td style = \"text-align: right;\">0.1782</td><td style = \"text-align: right;\">0.2055</td><td style = \"text-align: right;\">0.2298</td><td style = \"text-align: right;\">0.3545</td><td style = \"text-align: right;\">0.6218</td><td style = \"text-align: right;\">0.7265</td><td style = \"text-align: right;\">0.8346</td><td style = \"text-align: right;\">0.8268</td><td style = \"text-align: right;\">0.8366</td><td style = \"text-align: right;\">0.9408</td><td style = \"text-align: right;\">0.951</td><td style = \"text-align: right;\">0.9801</td><td style = \"text-align: right;\">0.9974</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.9036</td><td style = \"text-align: right;\">0.6409</td><td style = \"text-align: right;\">0.3857</td><td style = \"text-align: right;\">0.2908</td><td style = \"text-align: right;\">0.204</td><td style = \"text-align: right;\">0.1653</td><td style = \"text-align: right;\">0.1769</td><td style = \"text-align: right;\">0.114</td><td style = \"text-align: right;\">0.074</td><td style = \"text-align: right;\">0.0941</td><td style = \"text-align: right;\">0.0621</td><td style = \"text-align: right;\">0.0426</td><td style = \"text-align: right;\">0.0572</td><td style = \"text-align: right;\">0.1068</td><td style = \"text-align: right;\">0.1909</td><td style = \"text-align: right;\">0.2229</td><td style = \"text-align: right;\">0.2203</td><td style = \"text-align: right;\">0.2265</td><td style = \"text-align: right;\">0.1766</td><td style = \"text-align: right;\">0.1097</td><td style = \"text-align: right;\">0.0558</td><td style = \"text-align: right;\">0.0142</td><td style = \"text-align: right;\">0.0281</td><td style = \"text-align: right;\">0.0165</td><td style = \"text-align: right;\">0.0056</td><td style = \"text-align: right;\">0.001</td><td style = \"text-align: right;\">0.0027</td><td style = \"text-align: right;\">0.0062</td><td style = \"text-align: right;\">0.0024</td><td style = \"text-align: right;\">0.0063</td><td style = \"text-align: right;\">0.0017</td><td style = \"text-align: right;\">0.0028</td><td style = \"text-align: left;\">M</td><td style = \"text-align: right;\">true</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">198</td><td style = \"text-align: right;\">0.0366</td><td style = \"text-align: right;\">0.0421</td><td style = \"text-align: right;\">0.0504</td><td style = \"text-align: right;\">0.025</td><td style = \"text-align: right;\">0.0596</td><td style = \"text-align: right;\">0.0252</td><td style = \"text-align: right;\">0.0958</td><td style = \"text-align: right;\">0.0991</td><td style = \"text-align: right;\">0.1419</td><td style = \"text-align: right;\">0.1847</td><td style = \"text-align: right;\">0.2222</td><td style = \"text-align: right;\">0.2648</td><td style = \"text-align: right;\">0.2508</td><td style = \"text-align: right;\">0.2291</td><td style = \"text-align: right;\">0.1555</td><td style = \"text-align: right;\">0.1863</td><td style = \"text-align: right;\">0.2387</td><td style = \"text-align: right;\">0.3345</td><td style = \"text-align: right;\">0.5233</td><td style = \"text-align: right;\">0.6684</td><td style = \"text-align: right;\">0.7766</td><td style = \"text-align: right;\">0.7928</td><td style = \"text-align: right;\">0.794</td><td style = \"text-align: right;\">0.9129</td><td style = \"text-align: right;\">0.9498</td><td style = \"text-align: right;\">0.9835</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.9471</td><td style = \"text-align: right;\">0.8237</td><td style = \"text-align: right;\">0.6252</td><td style = \"text-align: right;\">0.4181</td><td style = \"text-align: right;\">0.3209</td><td style = \"text-align: right;\">0.2658</td><td style = \"text-align: right;\">0.2196</td><td style = \"text-align: right;\">0.1588</td><td style = \"text-align: right;\">0.0561</td><td style = \"text-align: right;\">0.0948</td><td style = \"text-align: right;\">0.17</td><td style = \"text-align: right;\">0.1215</td><td style = \"text-align: right;\">0.1282</td><td style = \"text-align: right;\">0.0386</td><td style = \"text-align: right;\">0.1329</td><td style = \"text-align: right;\">0.2331</td><td style = \"text-align: right;\">0.2468</td><td style = \"text-align: right;\">0.196</td><td style = \"text-align: right;\">0.1985</td><td style = \"text-align: right;\">0.157</td><td style = \"text-align: right;\">0.0921</td><td style = \"text-align: right;\">0.0549</td><td style = \"text-align: right;\">0.0194</td><td style = \"text-align: right;\">0.0166</td><td style = \"text-align: right;\">0.0132</td><td style = \"text-align: right;\">0.0027</td><td style = \"text-align: right;\">0.0022</td><td style = \"text-align: right;\">0.0059</td><td style = \"text-align: right;\">0.0016</td><td style = \"text-align: right;\">0.0025</td><td style = \"text-align: right;\">0.0017</td><td style = \"text-align: right;\">0.0027</td><td style = \"text-align: right;\">0.0027</td><td style = \"text-align: left;\">M</td><td style = \"text-align: right;\">true</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">199</td><td style = \"text-align: right;\">0.0238</td><td style = \"text-align: right;\">0.0318</td><td style = \"text-align: right;\">0.0422</td><td style = \"text-align: right;\">0.0399</td><td style = \"text-align: right;\">0.0788</td><td style = \"text-align: right;\">0.0766</td><td style = \"text-align: right;\">0.0881</td><td style = \"text-align: right;\">0.1143</td><td style = \"text-align: right;\">0.1594</td><td style = \"text-align: right;\">0.2048</td><td style = \"text-align: right;\">0.2652</td><td style = \"text-align: right;\">0.31</td><td style = \"text-align: right;\">0.2381</td><td style = \"text-align: right;\">0.1918</td><td style = \"text-align: right;\">0.143</td><td style = \"text-align: right;\">0.1735</td><td style = \"text-align: right;\">0.1781</td><td style = \"text-align: right;\">0.2852</td><td style = \"text-align: right;\">0.5036</td><td style = \"text-align: right;\">0.6166</td><td style = \"text-align: right;\">0.7616</td><td style = \"text-align: right;\">0.8125</td><td style = \"text-align: right;\">0.7793</td><td style = \"text-align: right;\">0.8788</td><td style = \"text-align: right;\">0.8813</td><td style = \"text-align: right;\">0.947</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.9739</td><td style = \"text-align: right;\">0.8446</td><td style = \"text-align: right;\">0.6151</td><td style = \"text-align: right;\">0.4302</td><td style = \"text-align: right;\">0.3165</td><td style = \"text-align: right;\">0.2869</td><td style = \"text-align: right;\">0.2017</td><td style = \"text-align: right;\">0.1206</td><td style = \"text-align: right;\">0.0271</td><td style = \"text-align: right;\">0.058</td><td style = \"text-align: right;\">0.1262</td><td style = \"text-align: right;\">0.1072</td><td style = \"text-align: right;\">0.1082</td><td style = \"text-align: right;\">0.036</td><td style = \"text-align: right;\">0.1197</td><td style = \"text-align: right;\">0.2061</td><td style = \"text-align: right;\">0.2054</td><td style = \"text-align: right;\">0.1878</td><td style = \"text-align: right;\">0.2047</td><td style = \"text-align: right;\">0.1716</td><td style = \"text-align: right;\">0.1069</td><td style = \"text-align: right;\">0.0477</td><td style = \"text-align: right;\">0.017</td><td style = \"text-align: right;\">0.0186</td><td style = \"text-align: right;\">0.0096</td><td style = \"text-align: right;\">0.0071</td><td style = \"text-align: right;\">0.0084</td><td style = \"text-align: right;\">0.0038</td><td style = \"text-align: right;\">0.0026</td><td style = \"text-align: right;\">0.0028</td><td style = \"text-align: right;\">0.0013</td><td style = \"text-align: right;\">0.0035</td><td style = \"text-align: right;\">0.006</td><td style = \"text-align: left;\">M</td><td style = \"text-align: right;\">true</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">200</td><td style = \"text-align: right;\">0.0116</td><td style = \"text-align: right;\">0.0744</td><td style = \"text-align: right;\">0.0367</td><td style = \"text-align: right;\">0.0225</td><td style = \"text-align: right;\">0.0076</td><td style = \"text-align: right;\">0.0545</td><td style = \"text-align: right;\">0.111</td><td style = \"text-align: right;\">0.1069</td><td style = \"text-align: right;\">0.1708</td><td style = \"text-align: right;\">0.2271</td><td style = \"text-align: right;\">0.3171</td><td style = \"text-align: right;\">0.2882</td><td style = \"text-align: right;\">0.2657</td><td style = \"text-align: right;\">0.2307</td><td style = \"text-align: right;\">0.1889</td><td style = \"text-align: right;\">0.1791</td><td style = \"text-align: right;\">0.2298</td><td style = \"text-align: right;\">0.3715</td><td style = \"text-align: right;\">0.6223</td><td style = \"text-align: right;\">0.726</td><td style = \"text-align: right;\">0.7934</td><td style = \"text-align: right;\">0.8045</td><td style = \"text-align: right;\">0.8067</td><td style = \"text-align: right;\">0.9173</td><td style = \"text-align: right;\">0.9327</td><td style = \"text-align: right;\">0.9562</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.9818</td><td style = \"text-align: right;\">0.8684</td><td style = \"text-align: right;\">0.6381</td><td style = \"text-align: right;\">0.3997</td><td style = \"text-align: right;\">0.3242</td><td style = \"text-align: right;\">0.2835</td><td style = \"text-align: right;\">0.2413</td><td style = \"text-align: right;\">0.2321</td><td style = \"text-align: right;\">0.126</td><td style = \"text-align: right;\">0.0693</td><td style = \"text-align: right;\">0.0701</td><td style = \"text-align: right;\">0.1439</td><td style = \"text-align: right;\">0.1475</td><td style = \"text-align: right;\">0.0438</td><td style = \"text-align: right;\">0.0469</td><td style = \"text-align: right;\">0.1476</td><td style = \"text-align: right;\">0.1742</td><td style = \"text-align: right;\">0.1555</td><td style = \"text-align: right;\">0.1651</td><td style = \"text-align: right;\">0.1181</td><td style = \"text-align: right;\">0.072</td><td style = \"text-align: right;\">0.0321</td><td style = \"text-align: right;\">0.0056</td><td style = \"text-align: right;\">0.0202</td><td style = \"text-align: right;\">0.0141</td><td style = \"text-align: right;\">0.0103</td><td style = \"text-align: right;\">0.01</td><td style = \"text-align: right;\">0.0034</td><td style = \"text-align: right;\">0.0026</td><td style = \"text-align: right;\">0.0037</td><td style = \"text-align: right;\">0.0044</td><td style = \"text-align: right;\">0.0057</td><td style = \"text-align: right;\">0.0035</td><td style = \"text-align: left;\">M</td><td style = \"text-align: right;\">true</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">201</td><td style = \"text-align: right;\">0.0131</td><td style = \"text-align: right;\">0.0387</td><td style = \"text-align: right;\">0.0329</td><td style = \"text-align: right;\">0.0078</td><td style = \"text-align: right;\">0.0721</td><td style = \"text-align: right;\">0.1341</td><td style = \"text-align: right;\">0.1626</td><td style = \"text-align: right;\">0.1902</td><td style = \"text-align: right;\">0.261</td><td style = \"text-align: right;\">0.3193</td><td style = \"text-align: right;\">0.3468</td><td style = \"text-align: right;\">0.3738</td><td style = \"text-align: right;\">0.3055</td><td style = \"text-align: right;\">0.1926</td><td style = \"text-align: right;\">0.1385</td><td style = \"text-align: right;\">0.2122</td><td style = \"text-align: right;\">0.2758</td><td style = \"text-align: right;\">0.4576</td><td style = \"text-align: right;\">0.6487</td><td style = \"text-align: right;\">0.7154</td><td style = \"text-align: right;\">0.801</td><td style = \"text-align: right;\">0.7924</td><td style = \"text-align: right;\">0.8793</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.9865</td><td style = \"text-align: right;\">0.9474</td><td style = \"text-align: right;\">0.9474</td><td style = \"text-align: right;\">0.9315</td><td style = \"text-align: right;\">0.8326</td><td style = \"text-align: right;\">0.6213</td><td style = \"text-align: right;\">0.3772</td><td style = \"text-align: right;\">0.2822</td><td style = \"text-align: right;\">0.2042</td><td style = \"text-align: right;\">0.219</td><td style = \"text-align: right;\">0.2223</td><td style = \"text-align: right;\">0.1327</td><td style = \"text-align: right;\">0.0521</td><td style = \"text-align: right;\">0.0618</td><td style = \"text-align: right;\">0.1416</td><td style = \"text-align: right;\">0.146</td><td style = \"text-align: right;\">0.0846</td><td style = \"text-align: right;\">0.1055</td><td style = \"text-align: right;\">0.1639</td><td style = \"text-align: right;\">0.1916</td><td style = \"text-align: right;\">0.2085</td><td style = \"text-align: right;\">0.2335</td><td style = \"text-align: right;\">0.1964</td><td style = \"text-align: right;\">0.13</td><td style = \"text-align: right;\">0.0633</td><td style = \"text-align: right;\">0.0183</td><td style = \"text-align: right;\">0.0137</td><td style = \"text-align: right;\">0.015</td><td style = \"text-align: right;\">0.0076</td><td style = \"text-align: right;\">0.0032</td><td style = \"text-align: right;\">0.0037</td><td style = \"text-align: right;\">0.0071</td><td style = \"text-align: right;\">0.004</td><td style = \"text-align: right;\">0.0009</td><td style = \"text-align: right;\">0.0015</td><td style = \"text-align: right;\">0.0085</td><td style = \"text-align: left;\">M</td><td style = \"text-align: right;\">true</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">202</td><td style = \"text-align: right;\">0.0335</td><td style = \"text-align: right;\">0.0258</td><td style = \"text-align: right;\">0.0398</td><td style = \"text-align: right;\">0.057</td><td style = \"text-align: right;\">0.0529</td><td style = \"text-align: right;\">0.1091</td><td style = \"text-align: right;\">0.1709</td><td style = \"text-align: right;\">0.1684</td><td style = \"text-align: right;\">0.1865</td><td style = \"text-align: right;\">0.266</td><td style = \"text-align: right;\">0.3188</td><td style = \"text-align: right;\">0.3553</td><td style = \"text-align: right;\">0.3116</td><td style = \"text-align: right;\">0.1965</td><td style = \"text-align: right;\">0.178</td><td style = \"text-align: right;\">0.2794</td><td style = \"text-align: right;\">0.287</td><td style = \"text-align: right;\">0.3969</td><td style = \"text-align: right;\">0.5599</td><td style = \"text-align: right;\">0.6936</td><td style = \"text-align: right;\">0.7969</td><td style = \"text-align: right;\">0.7452</td><td style = \"text-align: right;\">0.8203</td><td style = \"text-align: right;\">0.9261</td><td style = \"text-align: right;\">0.881</td><td style = \"text-align: right;\">0.8814</td><td style = \"text-align: right;\">0.9301</td><td style = \"text-align: right;\">0.9955</td><td style = \"text-align: right;\">0.8576</td><td style = \"text-align: right;\">0.6069</td><td style = \"text-align: right;\">0.3934</td><td style = \"text-align: right;\">0.2464</td><td style = \"text-align: right;\">0.1645</td><td style = \"text-align: right;\">0.114</td><td style = \"text-align: right;\">0.0956</td><td style = \"text-align: right;\">0.008</td><td style = \"text-align: right;\">0.0702</td><td style = \"text-align: right;\">0.0936</td><td style = \"text-align: right;\">0.0894</td><td style = \"text-align: right;\">0.1127</td><td style = \"text-align: right;\">0.0873</td><td style = \"text-align: right;\">0.102</td><td style = \"text-align: right;\">0.1964</td><td style = \"text-align: right;\">0.2256</td><td style = \"text-align: right;\">0.1814</td><td style = \"text-align: right;\">0.2012</td><td style = \"text-align: right;\">0.1688</td><td style = \"text-align: right;\">0.1037</td><td style = \"text-align: right;\">0.0501</td><td style = \"text-align: right;\">0.0136</td><td style = \"text-align: right;\">0.013</td><td style = \"text-align: right;\">0.012</td><td style = \"text-align: right;\">0.0039</td><td style = \"text-align: right;\">0.0053</td><td style = \"text-align: right;\">0.0062</td><td style = \"text-align: right;\">0.0046</td><td style = \"text-align: right;\">0.0045</td><td style = \"text-align: right;\">0.0022</td><td style = \"text-align: right;\">0.0005</td><td style = \"text-align: right;\">0.0031</td><td style = \"text-align: left;\">M</td><td style = \"text-align: right;\">true</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">203</td><td style = \"text-align: right;\">0.0272</td><td style = \"text-align: right;\">0.0378</td><td style = \"text-align: right;\">0.0488</td><td style = \"text-align: right;\">0.0848</td><td style = \"text-align: right;\">0.1127</td><td style = \"text-align: right;\">0.1103</td><td style = \"text-align: right;\">0.1349</td><td style = \"text-align: right;\">0.2337</td><td style = \"text-align: right;\">0.3113</td><td style = \"text-align: right;\">0.3997</td><td style = \"text-align: right;\">0.3941</td><td style = \"text-align: right;\">0.3309</td><td style = \"text-align: right;\">0.2926</td><td style = \"text-align: right;\">0.176</td><td style = \"text-align: right;\">0.1739</td><td style = \"text-align: right;\">0.2043</td><td style = \"text-align: right;\">0.2088</td><td style = \"text-align: right;\">0.2678</td><td style = \"text-align: right;\">0.2434</td><td style = \"text-align: right;\">0.1839</td><td style = \"text-align: right;\">0.2802</td><td style = \"text-align: right;\">0.6172</td><td style = \"text-align: right;\">0.8015</td><td style = \"text-align: right;\">0.8313</td><td style = \"text-align: right;\">0.844</td><td style = \"text-align: right;\">0.8494</td><td style = \"text-align: right;\">0.9168</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.7896</td><td style = \"text-align: right;\">0.5371</td><td style = \"text-align: right;\">0.6472</td><td style = \"text-align: right;\">0.6505</td><td style = \"text-align: right;\">0.4959</td><td style = \"text-align: right;\">0.2175</td><td style = \"text-align: right;\">0.099</td><td style = \"text-align: right;\">0.0434</td><td style = \"text-align: right;\">0.1708</td><td style = \"text-align: right;\">0.1979</td><td style = \"text-align: right;\">0.188</td><td style = \"text-align: right;\">0.1108</td><td style = \"text-align: right;\">0.1702</td><td style = \"text-align: right;\">0.0585</td><td style = \"text-align: right;\">0.0638</td><td style = \"text-align: right;\">0.1391</td><td style = \"text-align: right;\">0.0638</td><td style = \"text-align: right;\">0.0581</td><td style = \"text-align: right;\">0.0641</td><td style = \"text-align: right;\">0.1044</td><td style = \"text-align: right;\">0.0732</td><td style = \"text-align: right;\">0.0275</td><td style = \"text-align: right;\">0.0146</td><td style = \"text-align: right;\">0.0091</td><td style = \"text-align: right;\">0.0045</td><td style = \"text-align: right;\">0.0043</td><td style = \"text-align: right;\">0.0043</td><td style = \"text-align: right;\">0.0098</td><td style = \"text-align: right;\">0.0054</td><td style = \"text-align: right;\">0.0051</td><td style = \"text-align: right;\">0.0065</td><td style = \"text-align: right;\">0.0103</td><td style = \"text-align: left;\">M</td><td style = \"text-align: right;\">true</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">204</td><td style = \"text-align: right;\">0.0187</td><td style = \"text-align: right;\">0.0346</td><td style = \"text-align: right;\">0.0168</td><td style = \"text-align: right;\">0.0177</td><td style = \"text-align: right;\">0.0393</td><td style = \"text-align: right;\">0.163</td><td style = \"text-align: right;\">0.2028</td><td style = \"text-align: right;\">0.1694</td><td style = \"text-align: right;\">0.2328</td><td style = \"text-align: right;\">0.2684</td><td style = \"text-align: right;\">0.3108</td><td style = \"text-align: right;\">0.2933</td><td style = \"text-align: right;\">0.2275</td><td style = \"text-align: right;\">0.0994</td><td style = \"text-align: right;\">0.1801</td><td style = \"text-align: right;\">0.22</td><td style = \"text-align: right;\">0.2732</td><td style = \"text-align: right;\">0.2862</td><td style = \"text-align: right;\">0.2034</td><td style = \"text-align: right;\">0.174</td><td style = \"text-align: right;\">0.413</td><td style = \"text-align: right;\">0.6879</td><td style = \"text-align: right;\">0.812</td><td style = \"text-align: right;\">0.8453</td><td style = \"text-align: right;\">0.8919</td><td style = \"text-align: right;\">0.93</td><td style = \"text-align: right;\">0.9987</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.8104</td><td style = \"text-align: right;\">0.6199</td><td style = \"text-align: right;\">0.6041</td><td style = \"text-align: right;\">0.5547</td><td style = \"text-align: right;\">0.416</td><td style = \"text-align: right;\">0.1472</td><td style = \"text-align: right;\">0.0849</td><td style = \"text-align: right;\">0.0608</td><td style = \"text-align: right;\">0.0969</td><td style = \"text-align: right;\">0.1411</td><td style = \"text-align: right;\">0.1676</td><td style = \"text-align: right;\">0.12</td><td style = \"text-align: right;\">0.1201</td><td style = \"text-align: right;\">0.1036</td><td style = \"text-align: right;\">0.1977</td><td style = \"text-align: right;\">0.1339</td><td style = \"text-align: right;\">0.0902</td><td style = \"text-align: right;\">0.1085</td><td style = \"text-align: right;\">0.1521</td><td style = \"text-align: right;\">0.1363</td><td style = \"text-align: right;\">0.0858</td><td style = \"text-align: right;\">0.029</td><td style = \"text-align: right;\">0.0203</td><td style = \"text-align: right;\">0.0116</td><td style = \"text-align: right;\">0.0098</td><td style = \"text-align: right;\">0.0199</td><td style = \"text-align: right;\">0.0033</td><td style = \"text-align: right;\">0.0101</td><td style = \"text-align: right;\">0.0065</td><td style = \"text-align: right;\">0.0115</td><td style = \"text-align: right;\">0.0193</td><td style = \"text-align: right;\">0.0157</td><td style = \"text-align: left;\">M</td><td style = \"text-align: right;\">true</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">205</td><td style = \"text-align: right;\">0.0323</td><td style = \"text-align: right;\">0.0101</td><td style = \"text-align: right;\">0.0298</td><td style = \"text-align: right;\">0.0564</td><td style = \"text-align: right;\">0.076</td><td style = \"text-align: right;\">0.0958</td><td style = \"text-align: right;\">0.099</td><td style = \"text-align: right;\">0.1018</td><td style = \"text-align: right;\">0.103</td><td style = \"text-align: right;\">0.2154</td><td style = \"text-align: right;\">0.3085</td><td style = \"text-align: right;\">0.3425</td><td style = \"text-align: right;\">0.299</td><td style = \"text-align: right;\">0.1402</td><td style = \"text-align: right;\">0.1235</td><td style = \"text-align: right;\">0.1534</td><td style = \"text-align: right;\">0.1901</td><td style = \"text-align: right;\">0.2429</td><td style = \"text-align: right;\">0.212</td><td style = \"text-align: right;\">0.2395</td><td style = \"text-align: right;\">0.3272</td><td style = \"text-align: right;\">0.5949</td><td style = \"text-align: right;\">0.8302</td><td style = \"text-align: right;\">0.9045</td><td style = \"text-align: right;\">0.9888</td><td style = \"text-align: right;\">0.9912</td><td style = \"text-align: right;\">0.9448</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.9092</td><td style = \"text-align: right;\">0.7412</td><td style = \"text-align: right;\">0.7691</td><td style = \"text-align: right;\">0.7117</td><td style = \"text-align: right;\">0.5304</td><td style = \"text-align: right;\">0.2131</td><td style = \"text-align: right;\">0.0928</td><td style = \"text-align: right;\">0.1297</td><td style = \"text-align: right;\">0.1159</td><td style = \"text-align: right;\">0.1226</td><td style = \"text-align: right;\">0.1768</td><td style = \"text-align: right;\">0.0345</td><td style = \"text-align: right;\">0.1562</td><td style = \"text-align: right;\">0.0824</td><td style = \"text-align: right;\">0.1149</td><td style = \"text-align: right;\">0.1694</td><td style = \"text-align: right;\">0.0954</td><td style = \"text-align: right;\">0.008</td><td style = \"text-align: right;\">0.079</td><td style = \"text-align: right;\">0.1255</td><td style = \"text-align: right;\">0.0647</td><td style = \"text-align: right;\">0.0179</td><td style = \"text-align: right;\">0.0051</td><td style = \"text-align: right;\">0.0061</td><td style = \"text-align: right;\">0.0093</td><td style = \"text-align: right;\">0.0135</td><td style = \"text-align: right;\">0.0063</td><td style = \"text-align: right;\">0.0063</td><td style = \"text-align: right;\">0.0034</td><td style = \"text-align: right;\">0.0032</td><td style = \"text-align: right;\">0.0062</td><td style = \"text-align: right;\">0.0067</td><td style = \"text-align: left;\">M</td><td style = \"text-align: right;\">true</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">206</td><td style = \"text-align: right;\">0.0522</td><td style = \"text-align: right;\">0.0437</td><td style = \"text-align: right;\">0.018</td><td style = \"text-align: right;\">0.0292</td><td style = \"text-align: right;\">0.0351</td><td style = \"text-align: right;\">0.1171</td><td style = \"text-align: right;\">0.1257</td><td style = \"text-align: right;\">0.1178</td><td style = \"text-align: right;\">0.1258</td><td style = \"text-align: right;\">0.2529</td><td style = \"text-align: right;\">0.2716</td><td style = \"text-align: right;\">0.2374</td><td style = \"text-align: right;\">0.1878</td><td style = \"text-align: right;\">0.0983</td><td style = \"text-align: right;\">0.0683</td><td style = \"text-align: right;\">0.1503</td><td style = \"text-align: right;\">0.1723</td><td style = \"text-align: right;\">0.2339</td><td style = \"text-align: right;\">0.1962</td><td style = \"text-align: right;\">0.1395</td><td style = \"text-align: right;\">0.3164</td><td style = \"text-align: right;\">0.5888</td><td style = \"text-align: right;\">0.7631</td><td style = \"text-align: right;\">0.8473</td><td style = \"text-align: right;\">0.9424</td><td style = \"text-align: right;\">0.9986</td><td style = \"text-align: right;\">0.9699</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.863</td><td style = \"text-align: right;\">0.6979</td><td style = \"text-align: right;\">0.7717</td><td style = \"text-align: right;\">0.7305</td><td style = \"text-align: right;\">0.5197</td><td style = \"text-align: right;\">0.1786</td><td style = \"text-align: right;\">0.1098</td><td style = \"text-align: right;\">0.1446</td><td style = \"text-align: right;\">0.1066</td><td style = \"text-align: right;\">0.144</td><td style = \"text-align: right;\">0.1929</td><td style = \"text-align: right;\">0.0325</td><td style = \"text-align: right;\">0.149</td><td style = \"text-align: right;\">0.0328</td><td style = \"text-align: right;\">0.0537</td><td style = \"text-align: right;\">0.1309</td><td style = \"text-align: right;\">0.091</td><td style = \"text-align: right;\">0.0757</td><td style = \"text-align: right;\">0.1059</td><td style = \"text-align: right;\">0.1005</td><td style = \"text-align: right;\">0.0535</td><td style = \"text-align: right;\">0.0235</td><td style = \"text-align: right;\">0.0155</td><td style = \"text-align: right;\">0.016</td><td style = \"text-align: right;\">0.0029</td><td style = \"text-align: right;\">0.0051</td><td style = \"text-align: right;\">0.0062</td><td style = \"text-align: right;\">0.0089</td><td style = \"text-align: right;\">0.014</td><td style = \"text-align: right;\">0.0138</td><td style = \"text-align: right;\">0.0077</td><td style = \"text-align: right;\">0.0031</td><td style = \"text-align: left;\">M</td><td style = \"text-align: right;\">true</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">207</td><td style = \"text-align: right;\">0.0303</td><td style = \"text-align: right;\">0.0353</td><td style = \"text-align: right;\">0.049</td><td style = \"text-align: right;\">0.0608</td><td style = \"text-align: right;\">0.0167</td><td style = \"text-align: right;\">0.1354</td><td style = \"text-align: right;\">0.1465</td><td style = \"text-align: right;\">0.1123</td><td style = \"text-align: right;\">0.1945</td><td style = \"text-align: right;\">0.2354</td><td style = \"text-align: right;\">0.2898</td><td style = \"text-align: right;\">0.2812</td><td style = \"text-align: right;\">0.1578</td><td style = \"text-align: right;\">0.0273</td><td style = \"text-align: right;\">0.0673</td><td style = \"text-align: right;\">0.1444</td><td style = \"text-align: right;\">0.207</td><td style = \"text-align: right;\">0.2645</td><td style = \"text-align: right;\">0.2828</td><td style = \"text-align: right;\">0.4293</td><td style = \"text-align: right;\">0.5685</td><td style = \"text-align: right;\">0.699</td><td style = \"text-align: right;\">0.7246</td><td style = \"text-align: right;\">0.7622</td><td style = \"text-align: right;\">0.9242</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.9979</td><td style = \"text-align: right;\">0.8297</td><td style = \"text-align: right;\">0.7032</td><td style = \"text-align: right;\">0.7141</td><td style = \"text-align: right;\">0.6893</td><td style = \"text-align: right;\">0.4961</td><td style = \"text-align: right;\">0.2584</td><td style = \"text-align: right;\">0.0969</td><td style = \"text-align: right;\">0.0776</td><td style = \"text-align: right;\">0.0364</td><td style = \"text-align: right;\">0.1572</td><td style = \"text-align: right;\">0.1823</td><td style = \"text-align: right;\">0.1349</td><td style = \"text-align: right;\">0.0849</td><td style = \"text-align: right;\">0.0492</td><td style = \"text-align: right;\">0.1367</td><td style = \"text-align: right;\">0.1552</td><td style = \"text-align: right;\">0.1548</td><td style = \"text-align: right;\">0.1319</td><td style = \"text-align: right;\">0.0985</td><td style = \"text-align: right;\">0.1258</td><td style = \"text-align: right;\">0.0954</td><td style = \"text-align: right;\">0.0489</td><td style = \"text-align: right;\">0.0241</td><td style = \"text-align: right;\">0.0042</td><td style = \"text-align: right;\">0.0086</td><td style = \"text-align: right;\">0.0046</td><td style = \"text-align: right;\">0.0126</td><td style = \"text-align: right;\">0.0036</td><td style = \"text-align: right;\">0.0035</td><td style = \"text-align: right;\">0.0034</td><td style = \"text-align: right;\">0.0079</td><td style = \"text-align: right;\">0.0036</td><td style = \"text-align: right;\">0.0048</td><td style = \"text-align: left;\">M</td><td style = \"text-align: right;\">true</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">208</td><td style = \"text-align: right;\">0.026</td><td style = \"text-align: right;\">0.0363</td><td style = \"text-align: right;\">0.0136</td><td style = \"text-align: right;\">0.0272</td><td style = \"text-align: right;\">0.0214</td><td style = \"text-align: right;\">0.0338</td><td style = \"text-align: right;\">0.0655</td><td style = \"text-align: right;\">0.14</td><td style = \"text-align: right;\">0.1843</td><td style = \"text-align: right;\">0.2354</td><td style = \"text-align: right;\">0.272</td><td style = \"text-align: right;\">0.2442</td><td style = \"text-align: right;\">0.1665</td><td style = \"text-align: right;\">0.0336</td><td style = \"text-align: right;\">0.1302</td><td style = \"text-align: right;\">0.1708</td><td style = \"text-align: right;\">0.2177</td><td style = \"text-align: right;\">0.3175</td><td style = \"text-align: right;\">0.3714</td><td style = \"text-align: right;\">0.4552</td><td style = \"text-align: right;\">0.57</td><td style = \"text-align: right;\">0.7397</td><td style = \"text-align: right;\">0.8062</td><td style = \"text-align: right;\">0.8837</td><td style = \"text-align: right;\">0.9432</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.9375</td><td style = \"text-align: right;\">0.7603</td><td style = \"text-align: right;\">0.7123</td><td style = \"text-align: right;\">0.8358</td><td style = \"text-align: right;\">0.7622</td><td style = \"text-align: right;\">0.4567</td><td style = \"text-align: right;\">0.1715</td><td style = \"text-align: right;\">0.1549</td><td style = \"text-align: right;\">0.1641</td><td style = \"text-align: right;\">0.1869</td><td style = \"text-align: right;\">0.2655</td><td style = \"text-align: right;\">0.1713</td><td style = \"text-align: right;\">0.0959</td><td style = \"text-align: right;\">0.0768</td><td style = \"text-align: right;\">0.0847</td><td style = \"text-align: right;\">0.2076</td><td style = \"text-align: right;\">0.2505</td><td style = \"text-align: right;\">0.1862</td><td style = \"text-align: right;\">0.1439</td><td style = \"text-align: right;\">0.147</td><td style = \"text-align: right;\">0.0991</td><td style = \"text-align: right;\">0.0041</td><td style = \"text-align: right;\">0.0154</td><td style = \"text-align: right;\">0.0116</td><td style = \"text-align: right;\">0.0181</td><td style = \"text-align: right;\">0.0146</td><td style = \"text-align: right;\">0.0129</td><td style = \"text-align: right;\">0.0047</td><td style = \"text-align: right;\">0.0039</td><td style = \"text-align: right;\">0.0061</td><td style = \"text-align: right;\">0.004</td><td style = \"text-align: right;\">0.0036</td><td style = \"text-align: right;\">0.0061</td><td style = \"text-align: right;\">0.0115</td><td style = \"text-align: left;\">M</td><td style = \"text-align: right;\">true</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccccc}\n",
       "\t& Column1 & Column2 & Column3 & Column4 & Column5 & Column6 & Column7 & Column8 & Column9 & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 0.02 & 0.0371 & 0.0428 & 0.0207 & 0.0954 & 0.0986 & 0.1539 & 0.1601 & 0.3109 & $\\dots$ \\\\\n",
       "\t2 & 0.0453 & 0.0523 & 0.0843 & 0.0689 & 0.1183 & 0.2583 & 0.2156 & 0.3481 & 0.3337 & $\\dots$ \\\\\n",
       "\t3 & 0.0262 & 0.0582 & 0.1099 & 0.1083 & 0.0974 & 0.228 & 0.2431 & 0.3771 & 0.5598 & $\\dots$ \\\\\n",
       "\t4 & 0.01 & 0.0171 & 0.0623 & 0.0205 & 0.0205 & 0.0368 & 0.1098 & 0.1276 & 0.0598 & $\\dots$ \\\\\n",
       "\t5 & 0.0762 & 0.0666 & 0.0481 & 0.0394 & 0.059 & 0.0649 & 0.1209 & 0.2467 & 0.3564 & $\\dots$ \\\\\n",
       "\t6 & 0.0286 & 0.0453 & 0.0277 & 0.0174 & 0.0384 & 0.099 & 0.1201 & 0.1833 & 0.2105 & $\\dots$ \\\\\n",
       "\t7 & 0.0317 & 0.0956 & 0.1321 & 0.1408 & 0.1674 & 0.171 & 0.0731 & 0.1401 & 0.2083 & $\\dots$ \\\\\n",
       "\t8 & 0.0519 & 0.0548 & 0.0842 & 0.0319 & 0.1158 & 0.0922 & 0.1027 & 0.0613 & 0.1465 & $\\dots$ \\\\\n",
       "\t9 & 0.0223 & 0.0375 & 0.0484 & 0.0475 & 0.0647 & 0.0591 & 0.0753 & 0.0098 & 0.0684 & $\\dots$ \\\\\n",
       "\t10 & 0.0164 & 0.0173 & 0.0347 & 0.007 & 0.0187 & 0.0671 & 0.1056 & 0.0697 & 0.0962 & $\\dots$ \\\\\n",
       "\t11 & 0.0039 & 0.0063 & 0.0152 & 0.0336 & 0.031 & 0.0284 & 0.0396 & 0.0272 & 0.0323 & $\\dots$ \\\\\n",
       "\t12 & 0.0123 & 0.0309 & 0.0169 & 0.0313 & 0.0358 & 0.0102 & 0.0182 & 0.0579 & 0.1122 & $\\dots$ \\\\\n",
       "\t13 & 0.0079 & 0.0086 & 0.0055 & 0.025 & 0.0344 & 0.0546 & 0.0528 & 0.0958 & 0.1009 & $\\dots$ \\\\\n",
       "\t14 & 0.009 & 0.0062 & 0.0253 & 0.0489 & 0.1197 & 0.1589 & 0.1392 & 0.0987 & 0.0955 & $\\dots$ \\\\\n",
       "\t15 & 0.0124 & 0.0433 & 0.0604 & 0.0449 & 0.0597 & 0.0355 & 0.0531 & 0.0343 & 0.1052 & $\\dots$ \\\\\n",
       "\t16 & 0.0298 & 0.0615 & 0.065 & 0.0921 & 0.1615 & 0.2294 & 0.2176 & 0.2033 & 0.1459 & $\\dots$ \\\\\n",
       "\t17 & 0.0352 & 0.0116 & 0.0191 & 0.0469 & 0.0737 & 0.1185 & 0.1683 & 0.1541 & 0.1466 & $\\dots$ \\\\\n",
       "\t18 & 0.0192 & 0.0607 & 0.0378 & 0.0774 & 0.1388 & 0.0809 & 0.0568 & 0.0219 & 0.1037 & $\\dots$ \\\\\n",
       "\t19 & 0.027 & 0.0092 & 0.0145 & 0.0278 & 0.0412 & 0.0757 & 0.1026 & 0.1138 & 0.0794 & $\\dots$ \\\\\n",
       "\t20 & 0.0126 & 0.0149 & 0.0641 & 0.1732 & 0.2565 & 0.2559 & 0.2947 & 0.411 & 0.4983 & $\\dots$ \\\\\n",
       "\t21 & 0.0473 & 0.0509 & 0.0819 & 0.1252 & 0.1783 & 0.307 & 0.3008 & 0.2362 & 0.383 & $\\dots$ \\\\\n",
       "\t22 & 0.0664 & 0.0575 & 0.0842 & 0.0372 & 0.0458 & 0.0771 & 0.0771 & 0.113 & 0.2353 & $\\dots$ \\\\\n",
       "\t23 & 0.0099 & 0.0484 & 0.0299 & 0.0297 & 0.0652 & 0.1077 & 0.2363 & 0.2385 & 0.0075 & $\\dots$ \\\\\n",
       "\t24 & 0.0115 & 0.015 & 0.0136 & 0.0076 & 0.0211 & 0.1058 & 0.1023 & 0.044 & 0.0931 & $\\dots$ \\\\\n",
       "\t25 & 0.0293 & 0.0644 & 0.039 & 0.0173 & 0.0476 & 0.0816 & 0.0993 & 0.0315 & 0.0736 & $\\dots$ \\\\\n",
       "\t26 & 0.0201 & 0.0026 & 0.0138 & 0.0062 & 0.0133 & 0.0151 & 0.0541 & 0.021 & 0.0505 & $\\dots$ \\\\\n",
       "\t27 & 0.0151 & 0.032 & 0.0599 & 0.105 & 0.1163 & 0.1734 & 0.1679 & 0.1119 & 0.0889 & $\\dots$ \\\\\n",
       "\t28 & 0.0177 & 0.03 & 0.0288 & 0.0394 & 0.063 & 0.0526 & 0.0688 & 0.0633 & 0.0624 & $\\dots$ \\\\\n",
       "\t29 & 0.01 & 0.0275 & 0.019 & 0.0371 & 0.0416 & 0.0201 & 0.0314 & 0.0651 & 0.1896 & $\\dots$ \\\\\n",
       "\t30 & 0.0189 & 0.0308 & 0.0197 & 0.0622 & 0.008 & 0.0789 & 0.144 & 0.1451 & 0.1789 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m208×62 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Column1 \u001b[0m\u001b[1m Column2 \u001b[0m\u001b[1m Column3 \u001b[0m\u001b[1m Column4 \u001b[0m\u001b[1m Column5 \u001b[0m\u001b[1m Column6 \u001b[0m\u001b[1m Column7 \u001b[0m\u001b[1m Column8 \u001b[0m\u001b[1m\u001b[0m ⋯\n",
       "     │\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │  0.02     0.0371   0.0428   0.0207   0.0954   0.0986   0.1539   0.1601  ⋯\n",
       "   2 │  0.0453   0.0523   0.0843   0.0689   0.1183   0.2583   0.2156   0.3481\n",
       "   3 │  0.0262   0.0582   0.1099   0.1083   0.0974   0.228    0.2431   0.3771\n",
       "   4 │  0.01     0.0171   0.0623   0.0205   0.0205   0.0368   0.1098   0.1276\n",
       "   5 │  0.0762   0.0666   0.0481   0.0394   0.059    0.0649   0.1209   0.2467  ⋯\n",
       "   6 │  0.0286   0.0453   0.0277   0.0174   0.0384   0.099    0.1201   0.1833\n",
       "   7 │  0.0317   0.0956   0.1321   0.1408   0.1674   0.171    0.0731   0.1401\n",
       "   8 │  0.0519   0.0548   0.0842   0.0319   0.1158   0.0922   0.1027   0.0613\n",
       "   9 │  0.0223   0.0375   0.0484   0.0475   0.0647   0.0591   0.0753   0.0098  ⋯\n",
       "  10 │  0.0164   0.0173   0.0347   0.007    0.0187   0.0671   0.1056   0.0697\n",
       "  11 │  0.0039   0.0063   0.0152   0.0336   0.031    0.0284   0.0396   0.0272\n",
       "  ⋮  │    ⋮        ⋮        ⋮        ⋮        ⋮        ⋮        ⋮        ⋮     ⋱\n",
       " 199 │  0.0238   0.0318   0.0422   0.0399   0.0788   0.0766   0.0881   0.1143\n",
       " 200 │  0.0116   0.0744   0.0367   0.0225   0.0076   0.0545   0.111    0.1069  ⋯\n",
       " 201 │  0.0131   0.0387   0.0329   0.0078   0.0721   0.1341   0.1626   0.1902\n",
       " 202 │  0.0335   0.0258   0.0398   0.057    0.0529   0.1091   0.1709   0.1684\n",
       " 203 │  0.0272   0.0378   0.0488   0.0848   0.1127   0.1103   0.1349   0.2337\n",
       " 204 │  0.0187   0.0346   0.0168   0.0177   0.0393   0.163    0.2028   0.1694  ⋯\n",
       " 205 │  0.0323   0.0101   0.0298   0.0564   0.076    0.0958   0.099    0.1018\n",
       " 206 │  0.0522   0.0437   0.018    0.0292   0.0351   0.1171   0.1257   0.1178\n",
       " 207 │  0.0303   0.0353   0.049    0.0608   0.0167   0.1354   0.1465   0.1123\n",
       " 208 │  0.026    0.0363   0.0136   0.0272   0.0214   0.0338   0.0655   0.14    ⋯\n",
       "\u001b[36m                                                 54 columns and 187 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insertcols!(data, :Mine => data[:, 61].==\"M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efa393f",
   "metadata": {},
   "source": [
    "Once the data is loaded in the DataFrame for the checking proposes and that any posible process has been applied on the data. As in previous tutorials, the data has to be put on a Matrix form, such as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "590ea8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = Matrix(data[!, 1:60]);\n",
    "output_data = data[!, :Mine];\n",
    "\n",
    "@assert input_data isa Matrix\n",
    "@assert output_data isa BitVector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a148c7",
   "metadata": {},
   "source": [
    "It is worth to mention that in a DataFrame when a set of lines is queried such as in the case of the `X`, the results is also a DataFrame. Therefore, in order to applied the remaining operations it is needed to applied the `Matrix` function to retrive a matrix where the previous operations can be used as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d272d3b3",
   "metadata": {},
   "source": [
    "### Question 7.1\n",
    "\n",
    "> ❓ Now, the data is loaded and converted to the usual types. Now you should be able to apply in the next section and make asplit of the dataset in two subset, test and training, and apply the corresponding normalization. Put the code on the following section to perform both operations. *Tip: Due to the preparation for MLJ models, read the notes at the end of the document*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f87e4831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tables.MatrixTable{Matrix{Float64}} with 21 rows, 60 columns, and schema:\n",
       " :x1   Float64\n",
       " :x2   Float64\n",
       " :x3   Float64\n",
       " :x4   Float64\n",
       " :x5   Float64\n",
       " :x6   Float64\n",
       " :x7   Float64\n",
       " :x8   Float64\n",
       " :x9   Float64\n",
       " :x10  Float64\n",
       " :x11  Float64\n",
       " :x12  Float64\n",
       " :x13  Float64\n",
       " ⋮     \n",
       " :x49  Float64\n",
       " :x50  Float64\n",
       " :x51  Float64\n",
       " :x52  Float64\n",
       " :x53  Float64\n",
       " :x54  Float64\n",
       " :x55  Float64\n",
       " :x56  Float64\n",
       " :x57  Float64\n",
       " :x58  Float64\n",
       " :x59  Float64\n",
       " :x60  Float64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx, test_idx = Utils.holdOut(size(input_data, 1), 0.1)\n",
    "\n",
    "train_input = input_data[train_idx, :]\n",
    "test_input  = input_data[test_idx, :]\n",
    "\n",
    "train_output = categorical(output_data[train_idx])\n",
    "test_output  = categorical(output_data[test_idx])\n",
    "\n",
    "μ = mean(train_input, dims=1) \n",
    "σ = std(train_input, dims=1)\n",
    "\n",
    "σ[σ .== 0] .= 1.0\n",
    "\n",
    "train_input_standardized = (train_input .- μ) ./ σ\n",
    "test_input_standardized  = (test_input  .- μ) ./ σ\n",
    "\n",
    "train_input = MLJ.table(train_input_standardized)\n",
    "test_input  = MLJ.table(test_input_standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcbe5ef",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "As mentioned above, ensembles are a set of \"weaker\" classifiers that allow us to later overcome their limits by joining them together. That is why, before starting with ensembles, it will be necessary to have some reference models that will later be joined together in a meta-classifier. In the following example, some simple models, imlemented with `MLJ` library, are trained: an SVM with RBF kernel, a Linear Regression, a Naïve Bayes and a Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "04b0a9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import MLJLIBSVMInterface ✔\n",
      "import MLJLinearModels ✔\n",
      "import MLJDecisionTreeInterface ✔\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFor silent loading, specify `verbosity=0`. \n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFor silent loading, specify `verbosity=0`. \n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFor silent loading, specify `verbosity=0`. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{Any, Any}()"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load models (MLJ will prompt to add missing packages the first time you run these)\n",
    "SVC = @load ProbabilisticSVC pkg=LIBSVM\n",
    "LogisticClassifier = @load LogisticClassifier pkg=MLJLinearModels\n",
    "DecisionTreeClassifier = @load DecisionTreeClassifier pkg=DecisionTree\n",
    "# GaussianNBClassifier = @load GaussianNBClassifier pkg=NaiveBayes\n",
    "\n",
    "#Define the models to train\n",
    "models = Dict(\n",
    "    \"SVM\" => SVC(),\n",
    "    \"LR\"  => LogisticClassifier(),\n",
    "    \"DT\"  => DecisionTreeClassifier(max_depth=4),\n",
    "    # \"NB\"  => GaussianNBClassifier(),\n",
    ")\n",
    "\n",
    "base_models=  [ model for (name, model) in models]\n",
    "\n",
    "machines = Dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c3a595d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: 90.47619047619048 %\n",
      "LR: 95.23809523809523 %\n",
      "DT: 76.19047619047619 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(ProbabilisticSVC(kernel = RadialBasis, …), …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(LogisticClassifier(lambda = 2.220446049250313e-16, …), …).\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSolver: MLJLinearModels.LBFGS{Optim.Options{Float64, Nothing}, @NamedTuple{}}\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m  optim_options: Optim.Options{Float64, Nothing}\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m  lbfgs_options: @NamedTuple{} NamedTuple()\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(DecisionTreeClassifier(max_depth = 4, …), …).\n"
     ]
    }
   ],
   "source": [
    "# Perform the training for each model and calculate the test values (accuracy)\n",
    "for (name, model) in models\n",
    "    machines[name] = machine(model, train_input, train_output) |> fit!\n",
    "    acc = MLJ.accuracy(predict_mode(machines[name], test_input), test_output)\n",
    "    println(\"$name: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0ecb8c",
   "metadata": {},
   "source": [
    "## Combining weak models in an ensemble\n",
    "\n",
    "When it comes to combining the models, there are different strategies depending on the task of the model, i.e. whether we are classifying or regressing. In this particular case we are going to focus on classification, although for regression it would be similar, but the continuous nature of the values should be taken into account when combining the outputs.\n",
    "\n",
    "Regarding the combination of the classification, there are mainly two ways to combine the outputs of several classifiers. These combinations are called Majority voting and Weighted majority voting, also known as Soft Voting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e3e9f1a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLJModelInterface.metadata_model"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MLJ\n",
    "using MLJBase\n",
    "using MLJModelInterface\n",
    "\n",
    "# ===================================================\n",
    "# DEFINITION OF VOTINGCLASSIFIER COMPATIBLE WITH MLJ\n",
    "# ===================================================\n",
    "\n",
    "\"\"\"\n",
    "    VotingClassifier <: Probabilistic\n",
    "\n",
    "An ensemble classifier that combines predictions from multiple base models using voting strategies.\n",
    "\n",
    "# Fields\n",
    "- `models::Vector{Probabilistic}`: Vector of base probabilistic models to be combined\n",
    "- `voting::Symbol`: Voting strategy, either `:hard` (majority vote) or `:soft` (averaged probabilities)\n",
    "- `weights::Union{Nothing, Vector{Float64}}`: Optional weights for each model. If `nothing`, all models have equal weight. Weights are automatically normalized to sum to 1.0.\n",
    "\n",
    "# Examples\n",
    "```julia\n",
    "# Equal weights (default)\n",
    "voting_clf = VotingClassifier(\n",
    "    models=[LogisticClassifier(), DecisionTreeClassifier()],\n",
    "    voting=:soft\n",
    ")\n",
    "\n",
    "# Custom weights (will be normalized automatically)\n",
    "voting_clf = VotingClassifier(\n",
    "    models=[LogisticClassifier(), DecisionTreeClassifier(), RandomForestClassifier()],\n",
    "    voting=:hard,\n",
    "    weights=[5, 3, 2]  # Will be normalized to [0.5, 0.3, 0.2]\n",
    ")\n",
    "```\n",
    "\"\"\"\n",
    "mutable struct VotingClassifier <: Probabilistic   # Models must be probabilistic, inherited from MLJBase\n",
    "    models::Vector{Probabilistic}\n",
    "    voting::Symbol  # :hard or :soft\n",
    "    weights::Union{Nothing, Vector{Float64}}\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    VotingClassifier(; models=Probabilistic[], voting=:hard, weights=nothing)\n",
    "\n",
    "Constructor for VotingClassifier.\n",
    "\n",
    "# Arguments\n",
    "- `models::Vector{Probabilistic}=Probabilistic[]`: Base models to combine\n",
    "- `voting::Symbol=:hard`: Voting strategy (`:hard` or `:soft`)\n",
    "- `weights::Union{Nothing, Vector{<:Real}}=nothing`: Weights for each model. Automatically normalized to sum to 1.0.\n",
    "\n",
    "# Throws\n",
    "- `AssertionError`: If voting is not `:hard` or `:soft`\n",
    "- `AssertionError`: If weights length doesn't match models length\n",
    "- `AssertionError`: If all weights are zero or negative\n",
    "\"\"\"\n",
    "function VotingClassifier(; models=Probabilistic[], voting=:hard, weights=nothing)\n",
    "    @assert voting in [:hard, :soft] \"The only possible labels are :hard or :soft\"\n",
    "    \n",
    "    normalized_weights = nothing\n",
    "    if weights !== nothing\n",
    "        @assert length(weights) == length(models) \"Number of weights must match number of models\"\n",
    "        @assert all(w >= 0 for w in weights) \"All weights must be non-negative\"\n",
    "        \n",
    "        # Normalize weights to sum to 1.0\n",
    "        normalized_weights = Float64.(weights) ./ sum(weights)\n",
    "    end\n",
    "    \n",
    "    return VotingClassifier(models, voting, normalized_weights)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    MLJModelInterface.fit(model::VotingClassifier, verbosity::Int, X, y)\n",
    "\n",
    "Fit the VotingClassifier by training each base model on the provided data.\n",
    "\n",
    "# Arguments\n",
    "- `model::VotingClassifier`: The voting classifier instance\n",
    "- `verbosity::Int`: Verbosity level for training output\n",
    "- `X`: Training features (table format)\n",
    "- `y`: Training target (categorical vector)\n",
    "\n",
    "# Returns\n",
    "- `fitresults`: Vector of trained machines (one per base model)\n",
    "- `cache`: Nothing (no caching implemented)\n",
    "- `report`: Named tuple with training information (number of models, voting strategy, and normalized weights)\n",
    "\"\"\"\n",
    "function MLJModelInterface.fit(model::VotingClassifier, verbosity::Int, X, y)\n",
    "    # Train each base model\n",
    "    fitresults = []\n",
    "    for base_model in model.models\n",
    "        model_copy = deepcopy(base_model)\n",
    "        mach = machine(model_copy, X, y)\n",
    "        fit!(mach, verbosity=0)\n",
    "        push!(fitresults, mach)\n",
    "    end\n",
    "    \n",
    "    # Save necessary information\n",
    "    cache = nothing\n",
    "    report = (n_models=length(model.models), voting=model.voting, weights=model.weights)\n",
    "    \n",
    "    return fitresults, cache, report\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    MLJModelInterface.predict_mode(model::VotingClassifier, fitresult, Xnew)\n",
    "\n",
    "Predict class labels using hard voting (majority vote with optional weights).\n",
    "\n",
    "# Arguments\n",
    "- `model::VotingClassifier`: The voting classifier instance\n",
    "- `fitresult`: Vector of trained machines from fit\n",
    "- `Xnew`: New data to predict on\n",
    "\n",
    "# Returns\n",
    "- Categorical vector of predicted class labels based on (weighted) majority voting\n",
    "\n",
    "# Details\n",
    "Each base model votes for a class. If weights are provided, each vote is multiplied by its \n",
    "corresponding weight. The class with the highest (weighted) vote count is selected.\n",
    "\"\"\"\n",
    "function MLJModelInterface.predict_mode(model::VotingClassifier, fitresult, Xnew)\n",
    "    machines = fitresult\n",
    "    \n",
    "    # Get predictions from all models\n",
    "    predictions = [predict_mode(mach, Xnew) for mach in machines]\n",
    "    \n",
    "    # Get all unique classes\n",
    "    # all_classes = unique(vcat([unique(p) for p in predictions]...))\n",
    "    all_classes = MLJBase.classes(predictions[1])\n",
    "    n_samples = length(predictions[1])\n",
    "    n_models = length(machines)\n",
    "    \n",
    "    # Determine weights (equal if not specified)\n",
    "    weights = model.weights === nothing ? fill(1.0/n_models, n_models) : model.weights\n",
    "    \n",
    "    # Weighted voting\n",
    "    ensemble_pred = Vector{eltype(predictions[1])}(undef, n_samples)\n",
    "    \n",
    "    for i in 1:n_samples\n",
    "        # Count weighted votes for each class\n",
    "        vote_counts = Dict{eltype(predictions[1]), Float64}()\n",
    "        for class in all_classes\n",
    "            vote_counts[class] = 0.0\n",
    "        end\n",
    "        \n",
    "        for (j, pred) in enumerate(predictions)\n",
    "            vote_counts[pred[i]] += weights[j]\n",
    "        end\n",
    "        \n",
    "        # Select class with maximum weighted votes\n",
    "        ensemble_pred[i] = argmax(vote_counts)\n",
    "    end\n",
    "    \n",
    "    # return categorical(ensemble_pred)\n",
    "    return categorical(MLJBase.unwrap.(ensemble_pred))\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    MLJModelInterface.predict(model::VotingClassifier, fitresult, Xnew)\n",
    "\n",
    "Predict class probabilities using the specified voting strategy.\n",
    "\n",
    "# Arguments\n",
    "- `model::VotingClassifier`: The voting classifier instance\n",
    "- `fitresult`: Vector of trained machines from fit\n",
    "- `Xnew`: New data to predict on\n",
    "\n",
    "# Returns\n",
    "- Vector of `UnivariateFinite` distributions representing class probabilities\n",
    "\n",
    "# Details\n",
    "- For `:hard` voting: Returns deterministic predictions wrapped in UnivariateFinite (with optional weights)\n",
    "- For `:soft` voting: Averages probability distributions from all base models using weights\n",
    "\"\"\"\n",
    "function MLJModelInterface.predict(model::VotingClassifier, fitresult, Xnew)\n",
    "    machines = fitresult\n",
    "    \n",
    "    result = if model.voting == :hard\n",
    "        # For hard voting, return deterministic predictions\n",
    "        UnivariateFinite(predict_mode(model, fitresult, Xnew))\n",
    "    else\n",
    "        # Soft voting: weighted average of probabilities\n",
    "        all_predictions = [predict(mach, Xnew) for mach in machines]\n",
    "        \n",
    "        # Get class levels\n",
    "        first_pred = all_predictions[1][1]\n",
    "        class_levels = MLJBase.classes(first_pred)\n",
    "        n_classes = length(class_levels)\n",
    "        n_samples = length(all_predictions[1])\n",
    "        n_models = length(machines)\n",
    "        \n",
    "        # Determine weights (equal if not specified)\n",
    "        weights = model.weights === nothing ? fill(1.0/n_models, n_models) : model.weights\n",
    "        \n",
    "        # Weighted average of probabilities\n",
    "        avg_probs = zeros(n_samples, n_classes)\n",
    "        for (model_idx, preds) in enumerate(all_predictions)\n",
    "            for i in 1:n_samples\n",
    "                for (j, level) in enumerate(class_levels)\n",
    "                    avg_probs[i, j] += weights[model_idx] * pdf(preds[i], level)\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        # Create UnivariateFinite distributions with weighted averaged probabilities\n",
    "        [UnivariateFinite(class_levels, avg_probs[i, :]) for i in 1:n_samples]\n",
    "    end\n",
    "    \n",
    "    return result\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Model metadata registration for VotingClassifier.\n",
    "\n",
    "Specifies input/output types and capabilities for MLJ integration.\n",
    "\"\"\"\n",
    "MLJModelInterface.metadata_model(VotingClassifier,\n",
    "    input_scitype=Table(Continuous),\n",
    "    target_scitype=AbstractVector{<:Finite},\n",
    "    supports_weights=false,\n",
    "    load_path=\"VotingClassifier\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced2b2cf",
   "metadata": {},
   "source": [
    "## Majority Voting\n",
    "Although also known as Hard Voting, as the name suggests, they are based on selecting the most voted option among the predicted ones among the different models. Each model casts a deterministic vote or prediction. The final class or prediction is the one that receives the most votes or the average among the results. It’s equivalent to a “democratic election” where each model/expert has one vote, and the most-voted option wins. In this way, the problem could be solved taking into account different results or points of view on the problem. \n",
    "\n",
    "### Example\n",
    "\n",
    "With 3 classifiers predicting a pattern:\n",
    "\n",
    "* SVM predicts: **Mine**\n",
    "\n",
    "* Logistic Regression predicts: **Rock**\n",
    "\n",
    "* Naive Bayes predicts: **Rock**\n",
    "\n",
    "**Result:** Rock (2 votes vs 1 vote) ✓\n",
    "\n",
    "See an example in the code below of constructing such a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ed08738e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import MLJLIBSVMInterface ✔\n",
      "import MLJLinearModels ✔\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFor silent loading, specify `verbosity=0`. \n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFor silent loading, specify `verbosity=0`. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{Any, Any}()"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load models (MLJ will prompt to add missing packages the first time you run these)\n",
    "SVC = @load ProbabilisticSVC pkg=LIBSVM\n",
    "LogisticClassifier = @load LogisticClassifier pkg=MLJLinearModels\n",
    "# GaussianNBClassifier = @load GaussianNBClassifier pkg=NaiveBayes\n",
    "\n",
    "#Define the models to train\n",
    "models = Dict(\n",
    "    \"SVM\" => SVC(),\n",
    "    \"LR\"  => LogisticClassifier(),\n",
    "    # \"NB\"  => GaussianNBClassifier(),\n",
    ")\n",
    "base_models = Probabilistic[model for (name, model) in models]\n",
    "\n",
    "machines = Dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "15c2cea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: 85.71428571428571 %\n",
      "LR: 95.23809523809523 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(ProbabilisticSVC(kernel = RadialBasis, …), …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(LogisticClassifier(lambda = 2.220446049250313e-16, …), …).\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSolver: MLJLinearModels.LBFGS{Optim.Options{Float64, Nothing}, @NamedTuple{}}\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m  optim_options: Optim.Options{Float64, Nothing}\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m  lbfgs_options: @NamedTuple{} NamedTuple()\n"
     ]
    }
   ],
   "source": [
    "# Perform the training for each model and calculate the test values (accuracy)\n",
    "for (name, model) in models\n",
    "    machines[name] = machine(model, train_input, train_output) |> fit!\n",
    "    acc = MLJ.accuracy(predict_mode(machines[name], test_input), test_output)\n",
    "    println(\"$name: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "72a2da84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(VotingClassifier(models = Probabilistic[ProbabilisticSVC(kernel = RadialBasis, …), LogisticClassifier(lambda = 2.220446049250313e-16, …)], …), …).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: 85.71428571428571 %\n",
      "LR: 95.23809523809523 %\n",
      "Ensemble (Hard Voting): 95.23809523809523 %\n"
     ]
    }
   ],
   "source": [
    "#Define the metaclassifier based on the base_models\n",
    "models[\"Ensemble (Hard Voting)\"] = VotingClassifier(models = base_models, voting=:hard)\n",
    "machines[\"Ensemble (Hard Voting)\"] = machine(models[\"Ensemble (Hard Voting)\"], train_input, train_output) |> fit!\n",
    "\n",
    "for (name, machine) in machines\n",
    "    acc = MLJ.accuracy(predict_mode(machine, test_input), test_output)\n",
    "    println(\"$name: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c3971c",
   "metadata": {},
   "source": [
    "The main problem is that we rely equally on all models when deciding on the response class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d50e61",
   "metadata": {},
   "source": [
    "## Soft Voting (Weighted Probability Voting)\n",
    "\n",
    "As mentioned in the previous section, one of the problems of the classical *emsemble* model is that all outcomes are weighted equally and in each of the \"weak\" models only the most voted option is taken into account. To solve this, **Soft Voting** propsose the use of the **probabilities** that each classifier assigns to each class, instead of just the predicted class. The final result is obtained by averaging these probabilities and selecting the class with the highest average probability.\n",
    "\n",
    "### Example without weights (all models equally important)\n",
    "\n",
    "| Classifier    | P(Mine) | P(Rock) |\n",
    "|--------------|---------|---------|\n",
    "| SVM          | 0.9     | 0.1     |\n",
    "| LR           | 0.3     | 0.7     |\n",
    "| NB           | 0.2     | 0.8     |\n",
    "| **Average ** | **0.47**| **0.53**|\n",
    "\n",
    "**Calculation:**\n",
    "- P(Mine) = (0.9 + 0.3 + 0.2) / 3 = 0.47\n",
    "- P(Rock) = (0.1 + 0.7 + 0.8) / 3 = 0.53\n",
    "\n",
    "**Result** Rock (highest average probability) ✓\n",
    "\n",
    "**Advantage over Hard Voting:** Even though SVM is very confident about Mine (0.9), the other two models are quite confident about Rock (0.7 and 0.8). Soft Voting captures this confidence information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f7b01ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: 85.71428571428571 %\n",
      "LR: 95.23809523809523 %\n",
      "Ensemble (Hard Voting): 95.23809523809523 %\n",
      "Ensemble (Soft Voting - Equal): 95.23809523809523 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(VotingClassifier(models = Probabilistic[ProbabilisticSVC(kernel = RadialBasis, …), LogisticClassifier(lambda = 2.220446049250313e-16, …)], …), …).\n"
     ]
    }
   ],
   "source": [
    "#Define the metaclassifier based on the base_models\n",
    "models[\"Ensemble (Soft Voting - Equal)\"] = VotingClassifier( models = base_models, voting = :soft, weights = nothing) # All models equally weighted\n",
    "machines[\"Ensemble (Soft Voting - Equal)\"] = machine(models[\"Ensemble (Soft Voting - Equal)\"], train_input, train_output) |> fit!\n",
    "for (name, machine) in machines\n",
    "    acc = MLJ.accuracy(predict_mode(machine, test_input), test_output)\n",
    "    println(\"$name: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf78117f",
   "metadata": {},
   "source": [
    "#### Weigthed Soft Voting \n",
    "Although it could improve because the confidence is taken into account, the main issue is the equality among the models. To solve this, one of the proposals is the use of a weighting in the decision.In many cases, we know that some models perform better than others. For example, if the SVM has an accuracy of 85% and the others are around 70%, we should give more importance to the SVM. This is achieved through weights. In the soft voting, the weights multiply each model’s probabilities before averaging them. Mathematically: \n",
    "\n",
    "$$P(clase) = \\frac{\\sum_{i=1}^{n} w_i \\cdot P_i(clase)}{\\sum_{i=1}^{n} w_i}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $w_i$ = weight of model $i$\n",
    "\n",
    "* $P_i(class)$ = probability assigned by model $i$ to that class\n",
    "\n",
    "* $n$ = number of models\n",
    "\n",
    "Going along with the same example, imagine that we want to increase the importance of the SVM\n",
    "### Example with weights [2, 1, 1] (double weight for SVM)\n",
    "\n",
    "| Classifier   | Weight | P(Mine) | P(Rock)| Mine Contribution | Rock Contribution |\n",
    "|--------------|-------|---------|---------|-------------------|-------------------|\n",
    "| SVM          | 2     | 0.9     | 0.1     | 2 × 0.9 = 1.8     | 2 × 0.1 = 0.2     |\n",
    "| LR           | 1     | 0.3     | 0.7     | 1 × 0.3 = 0.3     | 1 × 0.7 = 0.7     |\n",
    "| NB           | 1     | 0.2     | 0.8     | 1 × 0.2 = 0.2     | 1 × 0.8 = 0.8     |\n",
    "| **Sum**      | 4     |         |         | **2.3**           | **1.7**           |\n",
    "| **Weighted Avg.** |  |         |         | **0.575**         | **0.425** |\n",
    "\n",
    "**Calculation:**\n",
    "- P(Mine) = (1.8 + 0.3 + 0.2) / 4 = 2.3 / 4 = 0.575\n",
    "- P(Rock) = (0.2 + 0.7 + 0.8) / 4 = 1.7 / 4 = 0.425\n",
    "\n",
    "**Result:** Mine (highest weighted probabilit) ✓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "260da3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: 85.71428571428571 %\n",
      "LR: 95.23809523809523 %\n",
      "Ensembles (Soft Voting - Weighted): 95.23809523809523 %\n",
      "Ensemble (Hard Voting): 95.23809523809523 %\n",
      "Ensemble (Soft Voting - Equal): 95.23809523809523 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(VotingClassifier(models = Probabilistic[ProbabilisticSVC(kernel = RadialBasis, …), LogisticClassifier(lambda = 2.220446049250313e-16, …)], …), …).\n"
     ]
    }
   ],
   "source": [
    "models[\"Ensemble (Soft Voting - Weighted)\"] = VotingClassifier(models = base_models, voting=:soft,weights=[1,2])\n",
    "machines[\"Ensembles (Soft Voting - Weighted)\"] = machine(models[\"Ensemble (Soft Voting - Weighted)\"],train_input, train_output) |> fit!\n",
    "\n",
    "for (name, machine) in machines\n",
    "    acc = MLJ.accuracy(predict_mode(machine, test_input), test_output)\n",
    "    println(\"$name: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa26ee5d",
   "metadata": {},
   "source": [
    "## When to Use Each Strategy\n",
    "### Hard Voting\n",
    "\n",
    "- Models that only output categorical predictions (no probabilities)\n",
    "\n",
    "- When all models are equally reliable\n",
    "\n",
    "- Simpler and faster\n",
    "\n",
    "### Soft Voting (no weights)\n",
    "\n",
    "- Models that output probabilities\n",
    "\n",
    "- When all models perform similarly\n",
    "\n",
    "- Captures confidence in each prediction\n",
    "\n",
    "### Weighted Soft Voting\n",
    "\n",
    "- When some models are clearly better\n",
    "\n",
    "- When you want to give more importance to specific models\n",
    "\n",
    "- Weights can be based on:\n",
    "\n",
    "    - Validation accuracy\n",
    "    - Known model expertise\n",
    "    - F1-score or another relevant metric\n",
    "\n",
    "In order to chose the weights there are several strategies being the most importan: \n",
    "\n",
    "    1. Manual (based on prior knowledge)\n",
    "    2. Based on validation accuracy\n",
    "    3. Optimization via grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12be268b",
   "metadata": {},
   "source": [
    "### Question 7.2\n",
    "> ❓ We have perform every single test with a hold-out strategy, however, as it was appointed in a previous session, the application of a cross-validation approach is prefered to cut the dependency on the selection of the samples. In this case you could think that there are two different approaches one is apply the cross-validation to each model, choose the better one and combine those in a single ensemble. The other way arround would be applying the cross-validation at ensemble level before training the models. Which one is correct and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2fcbb5",
   "metadata": {},
   "source": [
    "It's better to take the whole ensemble as the model, and cross-validate at that level. If we train each model individually we are going to have the best result for each one working alone. But we will have no clue on how that translates into the ensemble. We want to optimize the prediction of the whole system, not on each model individually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f2b304",
   "metadata": {},
   "source": [
    "### Stacking\n",
    "\n",
    "This last approach to combining the models can be considered as a variant of Soft Voting. As mentioned in that section, soft voting allows the weights of each of the models to be fixed and this can be adjusted with a decaying gradient technique. Stacking is usually identified as creating a classification technique superior to a linear regression (which is what Soft Voting does) such as an ANN to combine the models.\n",
    "\n",
    "Thus, as has been done previously, the outputs of the different techniques could be taken and used as inputs to another classification model, allowing for the adjustment of the weights and the non-linear combinations of the responses of each one.\n",
    "\n",
    "You can see an example or this in the following code, which uses the implementation on `MLJ` whcih uses an SVC as compbining model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "128ac43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(ProbabilisticStack(metalearner = ProbabilisticSVC(kernel = RadialBasis, …), …), …).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:svm, …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:lr, …).\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSolver: MLJLinearModels.LBFGS{Optim.Options{Float64, Nothing}, @NamedTuple{}}\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m  optim_options: Optim.Options{Float64, Nothing}\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m  lbfgs_options: @NamedTuple{} NamedTuple()\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:dt, …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:svm, …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:lr, …).\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSolver: MLJLinearModels.LBFGS{Optim.Options{Float64, Nothing}, @NamedTuple{}}\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m  optim_options: Optim.Options{Float64, Nothing}\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m  lbfgs_options: @NamedTuple{} NamedTuple()\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:dt, …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:svm, …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:lr, …).\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSolver: MLJLinearModels.LBFGS{Optim.Options{Float64, Nothing}, @NamedTuple{}}\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m  optim_options: Optim.Options{Float64, Nothing}\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m  lbfgs_options: @NamedTuple{} NamedTuple()\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:dt, …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:svm, …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:lr, …).\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSolver: MLJLinearModels.LBFGS{Optim.Options{Float64, Nothing}, @NamedTuple{}}\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m  optim_options: Optim.Options{Float64, Nothing}\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m  lbfgs_options: @NamedTuple{} NamedTuple()\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:dt, …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:svm, …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:lr, …).\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSolver: MLJLinearModels.LBFGS{Optim.Options{Float64, Nothing}, @NamedTuple{}}\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m  optim_options: Optim.Options{Float64, Nothing}\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m  lbfgs_options: @NamedTuple{} NamedTuple()\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:dt, …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:svm, …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:lr, …).\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSolver: MLJLinearModels.LBFGS{Optim.Options{Float64, Nothing}, @NamedTuple{}}\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m  optim_options: Optim.Options{Float64, Nothing}\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m  lbfgs_options: @NamedTuple{} NamedTuple()\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:dt, …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:metalearner, …).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trained Machine; does not cache data\n",
       "  model: ProbabilisticStack(metalearner = ProbabilisticSVC(kernel = RadialBasis, …), …)\n",
       "  args: \n",
       "    1:\tSource @478 ⏎ Table{AbstractVector{Continuous}}\n",
       "    2:\tSource @258 ⏎ AbstractVector{Multiclass{2}}\n"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the base models again to ensure we have a clean set\n",
    "SVC = @load ProbabilisticSVC pkg=LIBSVM verbosity=0\n",
    "LogisticClassifier = @load LogisticClassifier pkg=MLJLinearModels verbosity=0\n",
    "DecisionTreeClassifier = @load DecisionTreeClassifier pkg=DecisionTree verbosity=0\n",
    "# GaussianNBClassifier = @load GaussianNBClassifier pkg=NaiveBayes verbosity=0\n",
    "\n",
    "# Create a NamedTuple of the base models for stacking\n",
    "base_models_for_stack = (;\n",
    "    :svm => SVC(),\n",
    "    :lr  => LogisticClassifier(),\n",
    "    :dt  => DecisionTreeClassifier(max_depth=4),\n",
    "    # :nb  => GaussianNBClassifier()\n",
    ")\n",
    "\n",
    "# Build the staking model\n",
    "models[\"Ensemble (Stacking)\"] = Stack(; \n",
    "    metalearner = SVC(),\n",
    "    resampling = CV(nfolds=5, shuffle=true, rng=123),\n",
    "    measures = log_loss,\n",
    "    base_models_for_stack...  \n",
    ")\n",
    "\n",
    "machines[\"Ensemble (Stacking)\"] = machine(models[\"Ensemble (Stacking)\"], train_input, train_output) |> fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ed598b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: 85.71428571428571 %\n",
      "LR: 95.23809523809523 %\n",
      "Ensembles (Soft Voting - Weighted): 95.23809523809523 %\n",
      "Ensemble (Hard Voting): 95.23809523809523 %\n",
      "Ensemble (Soft Voting - Equal): 95.23809523809523 %\n",
      "Ensemble (Stacking): 95.23809523809523 %\n"
     ]
    }
   ],
   "source": [
    "for (name, machine) in machines\n",
    "    acc = MLJ.accuracy(predict_mode(machine, test_input), test_output)\n",
    "    println(\"$name: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9273829",
   "metadata": {},
   "source": [
    "## Model creation\n",
    "\n",
    "One of the key elements that has not yet been addressed is the creation of the models that will compose the meta-classifier. So far, the approach that has been followed is not very adequate as the input dataset for all models is the same. This has the effect of an obvious lack of diversity in the models since whichever model we create, it will have the same information or \"point of view\" as the others. However, this is not the usual practice. Instead, the set of input patterns is usually divided into smaller sets with which to train one or more techniques in order to reduce the computational cost on the one hand, and to increase the diversity of the models on the other. It is necessary to remember at this point that \"weak\" models do not have to be perfect in all classes and do not even have to cover all possibilities, only models that are quick to train and offer a more or less consistent output.\n",
    "\n",
    "As for the way in which to partition the data for the creation of the models, most approaches usually consider two main approaches known as *Bagging* and *Boosting*. In the following, these two approaches will be briefly described.\n",
    "\n",
    "### Bagging or boostrap aggregation\n",
    "The technique known as _Bagging_ or selection with replacement was proposed by Breitman in 1996. It is based on the development of multiple models which can be trained in parallel. The key element of these models is that each model is trained on a subset of the training set. This subset of data is drawn randomly with replacement. This last point is particularly important because once an example has been selected from the possibilities, it is placed back among the possibilities so that it can be selected either in the subset being built, or in the subsets of the other models, i.e. non-disjoint sets of examples are created.\n",
    "\n",
    "![Bagging Example](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Ensemble_Bagging.svg/440px-Ensemble_Bagging.svg.png)\n",
    "\n",
    "The result is that \"experts\" are created on specialised data and depending on the partition. While common, or more frequent, data is correctly covered by all models, it is also true that less frequent data tends not to be in all partitions and may not be covered in all cases. Thus, you would get models that would be more specialised in certain data or have a different point of view, that would be experts in a particular region of the search space.\n",
    "\n",
    "Although it will be discussed in more detail later, a well-known technique that uses this approach for the construction of its \"weak\" models is RandomForest. It builds the decision trees that make up the metaclassifier in this way. Any classifier can be used as the basis of a *Bagging* with the class [EnsembleModel](https://juliaai.github.io/MLJ.jl/stable/models/EnsembleModel_MLJEnsembles/#EnsembleModel_MLJEnsembles). \n",
    "\n",
    "For example, in the following code, 10 SVM for classication has been chosen as weak models. Each of those models habe been trained on only 50% of the training patterns, and therefore the variance among them should be increased.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "bd7750d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble-building in parallel on 1 threads.\n",
      "\n",
      "SVM: 85.71428571428571 %\n",
      "LR: 95.23809523809523 %\n",
      "Ensembles (Soft Voting - Weighted): 95.23809523809523 %\n",
      "Ensemble (Hard Voting): 95.23809523809523 %\n",
      "Ensemble (Soft Voting - Equal): 95.23809523809523 %\n",
      "Ensemble (Stacking): 95.23809523809523 %\n",
      "Bagging (SVC): 71.42857142857143 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(ProbabilisticEnsembleModel(model = ProbabilisticSVC(kernel = RadialBasis, …), …), …).\n"
     ]
    }
   ],
   "source": [
    "# Add a Bagging model using SVC as base model\n",
    "using MLJEnsembles: EnsembleModel, CPUThreads\n",
    "\n",
    "models[\"Bagging (SVC)\"] = EnsembleModel(\n",
    "    model = SVC(),              # or ProbabilisticSVC()\n",
    "    n = 10,                     # number of base models\n",
    "    bagging_fraction = 0.50,    # fraction of examples per base model\n",
    "    rng = 123,                  \n",
    "    acceleration = CPUThreads() # uses Threads to speed up the training due to the independence of base models\n",
    ")\n",
    "\n",
    "machines[\"Bagging (SVC)\"] = machine(models[\"Bagging (SVC)\"], train_input, train_output) |> fit!\n",
    "\n",
    "for (name, machine) in machines\n",
    "    acc = MLJ.accuracy(predict_mode(machine, test_input), test_output)\n",
    "    println(\"$name: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5787a56",
   "metadata": {},
   "source": [
    "As an alternative to extracting complete examples, a vertical partition of the training _dataset_ could be performed, thus extracting features. To implement this alternative, in the `EnsembleModel` function, the parameter *bagging_fraction* must be defined. This approach is used when the number of features is particularly high in order to create simpler models that do not use all the information that is often redundant. It should be noted that this feature extraction procedure for models is done without replacement, i.e. features extracted for one classifier are not re-entered into the list of possibilities until the set for the next classifier is created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0484f377",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "\n",
    "The other major family of techniques for ensemble metamodelling is what is known as *Boosting*. In this case, the approach is slightly different, since the aim is to create a chain of classifiers. The key idea is that every successive classifier becomes **more specialized in the patterns that previous models misclassified**. Therefore, as in the previous case, a subset of patterns is selected from the original set. However, this process is done sequentially and without replacement. This causes the new learners to focus increasingly on the difficult cases, gradually producing a stronger and more accurate composite model. Thus, as in *Bagging*, the underlying idea of this approach is that not all models have to have all patterns as a basis, but unlike _Bagging_, this process is linear because of the dependency in the construction of the models. In the end, the outputs of the individual models are combined through a **weighted majority vote**, where each classifier’s weight reflects its performance during training.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Ensemble_Boosting.svg/1920px-Ensemble_Boosting.svg.png\" alt=\"Boosting examples\" width=\"600\"/>\n",
    "\n",
    "#### AdaBoost\n",
    "\n",
    "The **AdaBoost** algorithm starts by assigning equal weights to all instances in the training set. A simple classifier (called a *stump*, which is a tree of only one level) is trained, and its performance is evaluated. Instances that are misclassified are given higher weights, so that the next classifier focuses more on those difficult cases.  This iterative process continues, each time updating the weights and creating a new weak learner that complements the previous ones. In AdaBoost, the weighting of both instances and classifiers is based on an **exponential loss function**, which penalizes misclassifications exponentially. The final ensemble prediction is obtained through a **weighted majority vote** among all weak classifiers. In MLJ, this behaviour is implemented by the `AdaBoostStumpClassifier`, provided by the `DecisionTree.jl` package.\n",
    "\n",
    "\n",
    "#### Gradient Boosting\n",
    "\n",
    "**Gradient Boosting** follows a different principle: instead of reweighting instances explicitly, it uses a **gradient descent approach** to minimize a loss function. Each new tree in the sequence is trained to predict the **residual errors** (or gradients) of the previous ensemble, gradually refining the model. In the case of classification, each decision tree models the **logistic likelihood** of the data, and its predictions are combined to estimate the class probabilities. The final decision is based on the sum of these probabilities across all trees. In MLJ, this can be implemented using the `EvoTreeClassifier` (from `EvoTrees.jl`), which is conceptually similar to scikit-learn’s `GradientBoostingClassifier`, but is written entirely in Julia and supports both CPU and GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "830a90e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import MLJDecisionTreeInterface ✔\n",
      "import EvoTrees ✔\n",
      "SVM: 85.71428571428571 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: could not import MLJBase.accuracy into Main\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFor silent loading, specify `verbosity=0`. \n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFor silent loading, specify `verbosity=0`. \n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(AdaBoostStumpClassifier(n_iter = 30, …), …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mThe following kwargs are not supported and will be ignored: [:loss].\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(EvoTrees.EvoTreeClassifier\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - loss: mlogloss\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - metric: mlogloss\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - nrounds: 30\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - bagging_size: 1\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - early_stopping_rounds: 9223372036854775807\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - L2: 1.0\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - lambda: 0.0\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - gamma: 0.0\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - eta: 1.0\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - max_depth: 2\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - min_weight: 1.0\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - rowsample: 1.0\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - colsample: 1.0\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - nbins: 64\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - tree_type: binary\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - seed: 123\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - device: cpu\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m, …).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 95.23809523809523 %\n",
      "Ensembles (Soft Voting - Weighted): 95.23809523809523 %\n",
      "EvoTrees: 85.71428571428571 %\n",
      "Ensemble (Hard Voting): 95.23809523809523 %\n",
      "AdaBoost: 80.95238095238095 %\n",
      "Ensemble (Soft Voting - Equal): 95.23809523809523 %\n",
      "Ensemble (Stacking): 95.23809523809523 %\n",
      "Bagging (SVC): 71.42857142857143 %\n"
     ]
    }
   ],
   "source": [
    "using MLJ\n",
    "using MLJBase: accuracy\n",
    "\n",
    "# Load model (pure Julia implementations)\n",
    "AdaBoostStumpClassifier = @load AdaBoostStumpClassifier pkg=DecisionTree\n",
    "EvoTreeClassifier = @load EvoTreeClassifier pkg=EvoTrees\n",
    "\n",
    "# AdaBoost (similar to sklearn AdaBoostClassifier with stumps)\n",
    "models[\"AdaBoost\"] = AdaBoostStumpClassifier(n_iter = 30)\n",
    "machines[\"AdaBoost\"] = machine(models[\"AdaBoost\"], train_input, train_output) |> fit!\n",
    "\n",
    "\n",
    "# Gradient Boosting (similar to sklearn GradientBoostingClassifier)\n",
    "models[\"EvoTrees\"] = EvoTreeClassifier(\n",
    "    nrounds=30,\n",
    "    eta=1.0,\n",
    "    max_depth=2,\n",
    "    loss=:logistic\n",
    ")\n",
    "\n",
    "machines[\"EvoTrees\"] = machine(models[\"EvoTrees\"], train_input, train_output) |> fit!\n",
    "\n",
    "\n",
    "for (name, machine) in machines\n",
    "    acc = MLJ.accuracy(predict_mode(machine, test_input), test_output)\n",
    "    println(\"$name: $(acc*100) %\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43fc04b",
   "metadata": {},
   "source": [
    "### Question 7.3\n",
    "\n",
    "Develop a function to **train and evaluate a homogeneous ensemble** using **`EnsembleModel`** from **MLJ**, where all base estimators are of the **same type** (e.g., SVC, DecisionTree, etc.).  \n",
    "The function, named `trainClassEnsemble`, should follow a **stratified cross-validation** scheme and return at least the test metric(s) value.\n",
    "\n",
    "**Requirements and Steps (summary):**\n",
    "\n",
    "1. **Results vector:** Create a vector of length *k* to store, for each fold, the test metric(s) values.  \n",
    "2. **k-fold loop:** For each iteration, and using the provided stratified indices, build the four data partitions:  \n",
    "   - `X_train`, `y_train`, `X_test`, `y_test`.  \n",
    "3. **Base model generation:** Instantiate the **base model** (from Unit 6) with its **hyperparameters**.  \n",
    "4. **Build the homogeneous ensemble:** Wrap the base model inside an `EnsembleModel` (MLJ), configuring the number of models (`n`) and, if applicable, bagging parameters (`bagging_fraction`, `sampling_fraction`, `n_subfeatures`, etc.).  \n",
    "5. **Training:** Fit the ensemble on the training set (`machine`, `fit!`).  \n",
    "6. **(Optional) Internal validation:** If a validation set is required (e.g., for early stopping), apply a **hold-out** split on `X_train`, `y_train`.  \n",
    "7. **Evaluation:** Compute the chosen metric(s) on the test set for the current fold and store the results.  \n",
    "8. **Aggregation:** Finally, report the **average** and **standard deviation** of each metric across all folds.\n",
    "\n",
    "> **Important:** You must implement this as a **homogeneous ensemble** (all base learners of the same type) using **`EnsembleModel` from MLJ** (bagging approach). *Stacking* or *boosting* is not required here.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a55290ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainClassEnsemble (generic function with 2 methods)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function trainClassEnsemble(estimator::Symbol, \n",
    "        modelsHyperParameters:: Dict,\n",
    "        ensembleHyperParameters:: Dict,     \n",
    "        trainingDataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,2}},    \n",
    "        kFoldIndices::     Array{Int64,1})\n",
    "\n",
    "    # 1. Unpack dataset and convert One-Hot targets (Y) to 1D vector (y_labels)\n",
    "    # The signature (Bool,2) implies Y is a one-hot matrix (N x C).\n",
    "    # MLJ models require a 1D categorical vector.\n",
    "    X, Y = trainingDataset\n",
    "    numClasses = size(Y, 2)\n",
    "    \n",
    "    @assert numClasses > 1\n",
    "\n",
    "    # Use 1-based indices (1, 2, ..., C) as the class labels\n",
    "    classes = 1:numClasses\n",
    "    \n",
    "    # Convert NxC one-hot matrix Y to an N-element vector of integer labels\n",
    "    # findfirst(identity, row) finds the index of the 'true' value in each row\n",
    "    y_labels = [findfirst(identity, row) for row in eachrow(Y)]\n",
    "\n",
    "    # 2. Get k (number of folds)\n",
    "    numFolds = maximum(kFoldIndices)\n",
    "\n",
    "    # 3. Initialize metric accumulators (one entry per fold)\n",
    "    acc_f  = zeros(Float64, numFolds)\n",
    "    err_f  = zeros(Float64, numFolds)\n",
    "    sens_f = zeros(Float64, numFolds)\n",
    "    spec_f = zeros(Float64, numFolds)\n",
    "    ppv_f  = zeros(Float64, numFolds)\n",
    "    npv_f  = zeros(Float64, numFolds)\n",
    "    f1_f   = zeros(Float64, numFolds)\n",
    "    \n",
    "    # 4. Initialize global confusion matrix accumulator (as Floats for averaging)\n",
    "    globalCM = zeros(Float64, numClasses, numClasses)\n",
    "\n",
    "    # 5. k-fold cross-validation loop\n",
    "    for fold in 1:numFolds\n",
    "        # 5a. Split data for this fold\n",
    "        test_mask  = (kFoldIndices .== fold)\n",
    "        train_mask = .!test_mask\n",
    "\n",
    "        Xtr, Xte = X[train_mask, :], X[test_mask, :]\n",
    "        # Use the 1D integer labels for y\n",
    "        ytr_labels, yte_labels = y_labels[train_mask], y_labels[test_mask]\n",
    "\n",
    "        # 5b. Instantiate the base model\n",
    "        local base_model # Ensure model is scoped to the loop\n",
    "        \n",
    "        if estimator == :SVC\n",
    "            Cval   = Float64(get(modelsHyperParameters, :C, 1.0))\n",
    "            gamma  = Float64(get(modelsHyperParameters, :gamma, 0.0)) # 0.0 -> 1/n_features\n",
    "            degree = Int32(get(modelsHyperParameters, :degree, 3))\n",
    "            coef0  = Float64(get(modelsHyperParameters, :coef0, 0.0))\n",
    "            kern_s = String(get(modelsHyperParameters, :kernel, \"rbf\"))\n",
    "            kern   = _get_kernel(kern_s)\n",
    "\n",
    "            base_model = SVCModel(kernel = kern, cost = Cval, gamma = gamma, degree = degree, coef0 = coef0)\n",
    "\n",
    "        elseif estimator == :DecisionTreeClassifier\n",
    "            max_depth = get(modelsHyperParameters, :max_depth, nothing) # `nothing` means no limit\n",
    "            rng_dt    = get(modelsHyperParameters, :rng, Random.MersenneTwister(1))\n",
    "            base_model = DecisionTreeClassifier(max_depth = max_depth, rng = rng_dt)\n",
    "\n",
    "        elseif estimator == :KNeighborsClassifier\n",
    "            k_neighbors = Int(get(modelsHyperParameters, :n_neighbors, 5))\n",
    "            base_model = KNNModel(K = k_neighbors)\n",
    "        \n",
    "        else\n",
    "            error(\"Unknown estimator symbol: $estimator. Expected :SVC, :DecisionTreeClassifier, or :KNeighborsClassifier\")\n",
    "        end\n",
    "\n",
    "        # 5c. Build the homogeneous ensemble (EnsembleModel)\n",
    "        n_models = Int(get(ensembleHyperParameters, :n, 10))\n",
    "        bagging_frac = Float64(get(ensembleHyperParameters, :bagging_fraction, 0.8))\n",
    "        rng_seed = Int(get(ensembleHyperParameters, :rng, 123))\n",
    "        \n",
    "        ensemble_model = EnsembleModel(\n",
    "            model = base_model,\n",
    "            n = n_models,\n",
    "            bagging_fraction = bagging_frac,\n",
    "            rng = rng_seed,\n",
    "            acceleration = MLJ.CPUThreads()\n",
    "        )\n",
    "\n",
    "        # 5d. Training\n",
    "        Xtr_table = MLJ.table(Xtr)\n",
    "        # Convert 1D integer labels (1,2,3) to MLJ Categorical type\n",
    "        ytr_cat = categorical(ytr_labels, levels=classes) \n",
    "\n",
    "        mach = machine(ensemble_model, Xtr_table, ytr_cat)\n",
    "        fit!(mach, verbosity=0)\n",
    "\n",
    "        # 5e. Evaluation\n",
    "        Xte_table = MLJ.table(Xte)\n",
    "        # EnsembleModel (bagging) is probabilistic\n",
    "        yhat_prob = predict(mach, Xte_table) \n",
    "        # Get the class with the highest probability\n",
    "        yhat = mode.(yhat_prob) \n",
    "        \n",
    "        # Convert CategoricalValue predictions back to integer labels (1, 2, 3...)\n",
    "        yhat_labels = Int.(levelcode.(yhat))\n",
    "        yte_cat = categorical(yte_labels, levels=classes)\n",
    "\n",
    "        # 5f. Compute metrics using the (Any,1), (Any,1), (Any,1) version from Utils\n",
    "        (acc, err, sens, spec, ppv, npv, f1, CM) = Utils.confusionMatrix(\n",
    "            yhat, \n",
    "            yte_cat, \n",
    "            classes; \n",
    "            weighted=true # Or get(ensembleHyperParameters, :weighted_metrics, true)\n",
    "        )\n",
    "\n",
    "        # 5g. Store metrics for this fold\n",
    "        acc_f[fold]  = acc\n",
    "        err_f[fold]  = err\n",
    "        sens_f[fold] = sens\n",
    "        spec_f[fold] = spec\n",
    "        ppv_f[fold]  = ppv\n",
    "        npv_f[fold]  = npv\n",
    "        f1_f[fold]   = f1\n",
    "        \n",
    "        # 5h. Accumulate Confusion Matrix (deterministic, so just sum)\n",
    "        globalCM .+= Float64.(CM)\n",
    "    end # end k-fold loop\n",
    "\n",
    "    # 6. Aggregate metrics across all folds\n",
    "    return (\n",
    "        (mean(acc_f),  std(acc_f)),\n",
    "        (mean(err_f),  std(err_f)),\n",
    "        (mean(sens_f), std(sens_f)),\n",
    "        (mean(spec_f), std(spec_f)),\n",
    "        (mean(ppv_f),  std(ppv_f)),\n",
    "        (mean(npv_f),  std(npv_f)),\n",
    "        (mean(f1_f),   std(f1_f)),\n",
    "        globalCM # Return the summed confusion matrix\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a265be0c",
   "metadata": {},
   "source": [
    "### Question 7.4\n",
    "> ❓ Repeated the previous function, but this time for a heterogeneous Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "128d7aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainClassEnsemble (generic function with 2 methods)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function trainClassEnsemble(estimators::AbstractArray{Symbol, 1}, \n",
    "        modelsHyperParameters:: AbstractArray{Dict, 1},\n",
    "        ensembleHyperParameters:: Dict,     \n",
    "        trainingDataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,2}},    \n",
    "        kFoldIndices::     Array{Int64,1})\n",
    "# 1. Unpack dataset and check assertions\n",
    "    X, Y_onehot = trainingDataset\n",
    "    numClasses = size(Y_onehot, 2)\n",
    "    @assert length(estimators) == length(modelsHyperParameters) \"Must be one hyperparameter Dict per estimator\"\n",
    "    \n",
    "    if numClasses <= 1\n",
    "        error(\"trainingDataset targets matrix must have at least 2 columns (one-hot encoded). Found $(numClasses)\")\n",
    "    end\n",
    "\n",
    "    # Convert NxC one-hot matrix Y to an N-element vector of integer labels (1..C)\n",
    "    classes = 1:numClasses\n",
    "    y_labels = [findfirst(identity, row) for row in eachrow(Y_onehot)]\n",
    "\n",
    "    # 2. Get k (number of folds)\n",
    "    numFolds = maximum(kFoldIndices)\n",
    "\n",
    "    # 3. Initialize metric accumulators\n",
    "    acc_f  = zeros(Float64, numFolds)\n",
    "    err_f  = zeros(Float64, numFolds)\n",
    "    sens_f = zeros(Float64, numFolds)\n",
    "    spec_f = zeros(Float64, numFolds)\n",
    "    ppv_f  = zeros(Float64, numFolds)\n",
    "    npv_f  = zeros(Float64, numFolds)\n",
    "    f1_f   = zeros(Float64, numFolds)\n",
    "    \n",
    "    # 4. Initialize global confusion matrix accumulator\n",
    "    globalCM = zeros(Float64, numClasses, numClasses)\n",
    "\n",
    "    # 5. k-fold cross-validation loop\n",
    "    for fold in 1:numFolds\n",
    "        # 5a. Split data for this fold\n",
    "        test_mask  = (kFoldIndices .== fold)\n",
    "        train_mask = .!test_mask\n",
    "\n",
    "        Xtr, Xte = X[train_mask, :], X[test_mask, :]\n",
    "        ytr_labels, yte_labels = y_labels[train_mask], y_labels[test_mask]\n",
    "\n",
    "        # 5b. Instantiate the *list* of base models\n",
    "        base_models_list = Probabilistic[] # VotingClassifier requires Probabilistic models\n",
    "        \n",
    "        for (i, est_symbol) in enumerate(estimators)\n",
    "            params = modelsHyperParameters[i]\n",
    "            local base_model # Needs to be local to the outer loop\n",
    "            \n",
    "            if est_symbol == :SVC\n",
    "                Cval   = Float64(get(params, :C, 1.0))\n",
    "                gamma  = Float64(get(params, :gamma, 0.0))\n",
    "                kern_s = String(get(params, :kernel, \"rbf\"))\n",
    "                kern   = _get_kernel(kern_s)\n",
    "                # Use ProbabilisticSVC as required by the VotingClassifier definition\n",
    "                base_model = ProbabilisticSVCModel(kernel = kern, cost = Cval, gamma = gamma)\n",
    "            \n",
    "            elseif est_symbol == :DecisionTreeClassifier\n",
    "                max_depth = get(params, :max_depth, nothing)\n",
    "                rng_dt    = get(params, :rng, Random.MersenneTwister(1))\n",
    "                base_model = DecisionTreeClassifier(max_depth = max_depth, rng = rng_dt)\n",
    "            \n",
    "            elseif est_symbol == :KNeighborsClassifier\n",
    "                k_neighbors = Int(get(params, :n_neighbors, 5))\n",
    "                base_model = KNNClassifier(K = k_neighbors)\n",
    "            \n",
    "            else\n",
    "                error(\"Unsupported estimator: $est_symbol. Expected :SVC, :DecisionTreeClassifier, or :KNeighborsClassifier\")\n",
    "            end\n",
    "            push!(base_models_list, base_model)\n",
    "        end\n",
    "\n",
    "        # 5c. Build the heterogeneous ensemble (VotingClassifier)\n",
    "        voting_strategy = Symbol(get(ensembleHyperParameters, :voting, :hard))\n",
    "        weights = get(ensembleHyperParameters, :weights, nothing)\n",
    "        \n",
    "        ensemble_model = VotingClassifier(\n",
    "            models = base_models_list,\n",
    "            voting = voting_strategy,\n",
    "            weights = weights\n",
    "        )\n",
    "\n",
    "        # 5d. Training\n",
    "        Xtr_table = MLJ.table(Xtr)\n",
    "        ytr_cat = categorical(ytr_labels, levels=classes) \n",
    "\n",
    "        mach = machine(ensemble_model, Xtr_table, ytr_cat)\n",
    "        fit!(mach, verbosity=0)\n",
    "\n",
    "        # 5e. Evaluation\n",
    "        Xte_table = MLJ.table(Xte)\n",
    "        yhat_prob = predict(mach, Xte_table) # VotingClassifier predict returns probs\n",
    "        yhat = mode.(yhat_prob) # Get the most likely class\n",
    "        \n",
    "        # Convert CategoricalValue predictions back to integer labels\n",
    "        yhat_labels = Int.(levelcode.(yhat))\n",
    "        yte_cat = categorical(yte_labels, levels=classes)\n",
    "\n",
    "        # 5f. Compute metrics using the (Any,1), (Any,1), (Any,1) version from Utils\n",
    "        (acc, err, sens, spec, ppv, npv, f1, CM) = Utils.confusionMatrix(\n",
    "            yhat, \n",
    "            yte_cat, \n",
    "            classes; \n",
    "            weighted=true # Defaulting to weighted\n",
    "        )\n",
    "\n",
    "        # 5g. Store metrics for this fold\n",
    "        acc_f[fold]  = acc\n",
    "        err_f[fold]  = err\n",
    "        sens_f[fold] = sens\n",
    "        spec_f[fold] = spec\n",
    "        ppv_f[fold]  = ppv\n",
    "        npv_f[fold]  = npv\n",
    "        f1_f[fold]   = f1\n",
    "        \n",
    "        # 5h. Accumulate Confusion Matrix\n",
    "        globalCM .+= Float64.(CM)\n",
    "    end # end k-fold loop\n",
    "\n",
    "    # 6. Aggregate metrics across all folds\n",
    "    return (\n",
    "        (mean(acc_f),  std(acc_f)),\n",
    "        (mean(err_f),  std(err_f)),\n",
    "        (mean(sens_f), std(sens_f)),\n",
    "        (mean(spec_f), std(spec_f)),\n",
    "        (mean(ppv_f),  std(ppv_f)),\n",
    "        (mean(npv_f),  std(npv_f)),\n",
    "        (mean(f1_f),   std(f1_f)),\n",
    "        globalCM # Return the summed confusion matrix\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1154068c",
   "metadata": {},
   "source": [
    "## Techniques integrating the Ensemble approach\n",
    "\n",
    "Some of the best-known and currently used algorithms are based on this type of approach. Among these approaches, perhaps the most famous and widely used are those based on the generation of simple Decision Tress (DT). The reason for the use of the trees is their easy interpretation, as well as the speed of calculation and training. In the following we will see the two approaches known today in this sense, ***Random Forest*** and ***XGBoost***.\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "The **Random Forest** algorithm, proposed by Breiman and Cutler in 2006 (building upon an earlier idea by Ho in 1995, known as *Random Subspaces*), is one of the most representative examples of ensemble learning. It combines multiple simple classifiers — in this case, **Decision Trees (DTs)** — into a single, more robust model. Each tree in the forest is trained on a **bootstrap sample** (a random subset with replacement) of the original data, following a *bagging* approach. Because each tree is trained independently, the process can be fully **parallelized**. For classification problems, the final prediction is obtained by **majority vote** among all trees; for regression, by averaging their outputs.\n",
    "\n",
    "Random Forests are known for performing remarkably well with **minimal hyperparameter tuning**. Typically, the most important parameter is the number of trees (`n_trees` in MLJ or `n_estimators` in scikit-learn), which controls the size of the ensemble. A common heuristic suggests using:\n",
    "\n",
    "- *$\\sqrt{\\textrm{\\#feature}}$* for classification tasks  \n",
    "- *$\\frac{\\textrm{\\#feature}}{3}$* for regression tasks  \n",
    "\n",
    "Although increasing the number of trees usually improves performance, it tends to **saturate** beyond 500–1000 trees in most practical cases.\n",
    "\n",
    "In addition to the bootstrapping process, Random Forests introduce a **second level of randomness**: at each node split, only a random subset of features is considered as candidates for partitioning. This enhances the **diversity of the trees** and helps reduce the model’s variance while maintaining strong predictive power. An important byproduct of this mechanism is the ability to **quantify feature importance**. By analysing how much each variable contributes to the reduction of node impurity across all trees, Random Forests can estimate the relative importance of each feature.  \n",
    "This impurity-based importance is often used for **feature selection**. The most common impurity metric is the **Gini index**, defined as:\n",
    "\n",
    "$$G = \\sum_{i=1}^C p(i) * (1 - p(i))$$\n",
    "\n",
    "where $ C $ is the number of classes and $ p(i) $ is the probability of randomly selecting an instance of class $ i $.  Intuitively, it measures the probability of incorrectly classifying a randomly chosen instance if labels were assigned according to the class distribution. For an excellent visual explanation, see [this reference](https://victorzhou.com/blog/gini-impurity/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "668be4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import MLJDecisionTreeInterface ✔\n",
      "SVM: 85.71 %\n",
      "LR: 95.24 %\n",
      "Ensembles (Soft Voting - Weighted): 95.24 %\n",
      "EvoTrees: 85.71 %\n",
      "Ensemble (Hard Voting): 95.24 %\n",
      "AdaBoost: 80.95 %\n",
      "Ensemble (Soft Voting - Equal): 95.24 %\n",
      "Ensemble (Stacking): 95.24 %\n",
      "Bagging (SVC): 71.43 %\n",
      "RF: 85.71 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: could not import MLJBase.accuracy into Main\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFor silent loading, specify `verbosity=0`. \n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(RandomForestClassifier(max_depth = -1, …), …).\n"
     ]
    }
   ],
   "source": [
    "using MLJ\n",
    "using MLJBase: accuracy\n",
    "using Plots\n",
    "\n",
    "# Load the native Julia Random Forest model\n",
    "RandomForestClassifier = @load RandomForestClassifier pkg=DecisionTree\n",
    "\n",
    "# Define the model\n",
    "models[\"RF\"] = RandomForestClassifier(\n",
    "    n_trees=8,              \n",
    "    max_depth=-1,           \n",
    "    min_samples_split=2,\n",
    "    n_subfeatures=-1,       \n",
    "    sampling_fraction=1.0   \n",
    ")\n",
    "\n",
    "# Train The modeel\n",
    "machines[\"RF\"] = machine(models[\"RF\"], train_input, train_output) |> fit!\n",
    "\n",
    "    \n",
    "# Evaluate accuracy\n",
    "for (name, mach) in machines\n",
    "    acc = accuracy(predict_mode(mach, test_input), test_output)\n",
    "    println(\"$name: $(round(acc*100, digits=2)) %\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dbe372",
   "metadata": {},
   "source": [
    "### Key Hyperparameters\n",
    "\n",
    "| **Parameter**        | **Description** |\n",
    "|-----------------------|-----------------|\n",
    "| `n_trees`             | Number of trees in the forest (equivalent to `n_estimators` in scikit-learn). |\n",
    "| `max_depth`           | Maximum depth of each tree. Use `-1` for no limit. |\n",
    "| `min_samples_split`   | Minimum number of samples required to split a node. |\n",
    "| `n_subfeatures`       | Number of random features considered at each split (`-1` uses √(#features)). |\n",
    "| `sampling_fraction`   | Fraction of training samples used to build each tree (bootstrapping). |\n",
    "| `rng`                 | Random number generator for reproducibility. |\n",
    "\n",
    "In this example, the number of trees (`n_trees`) is defined following the heuristic of $\\sqrt{\\textrm{\\#features}}$. Because the dataset used in this example is relatively small, results may vary slightly between runs depending on the random partitions used for training.\n",
    "\n",
    "#### Feature Importance\n",
    "Once the model has been trained, we can compute the feature importance based on the average Gini impurity reduction across all trees in the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ea85f5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip880\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip880)\" d=\"M0 1600 L2400 1600 L2400 0 L0 0  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip881\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip880)\" d=\"M205.121 1423.18 L2352.76 1423.18 L2352.76 123.472 L205.121 123.472  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip882\">\n",
       "    <rect x=\"205\" y=\"123\" width=\"2149\" height=\"1301\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"265.903,1423.18 265.903,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"603.582,1423.18 603.582,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"941.26,1423.18 941.26,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1278.94,1423.18 1278.94,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1616.62,1423.18 1616.62,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1954.3,1423.18 1954.3,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2291.97,1423.18 2291.97,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,1398.7 2352.76,1398.7 \"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,1193.66 2352.76,1193.66 \"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,988.618 2352.76,988.618 \"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,783.578 2352.76,783.578 \"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,578.538 2352.76,578.538 \"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,373.498 2352.76,373.498 \"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,168.458 2352.76,168.458 \"/>\n",
       "<polyline clip-path=\"url(#clip880)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1423.18 2352.76,1423.18 \"/>\n",
       "<polyline clip-path=\"url(#clip880)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"265.903,1423.18 265.903,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip880)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"603.582,1423.18 603.582,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip880)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"941.26,1423.18 941.26,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip880)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1278.94,1423.18 1278.94,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip880)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1616.62,1423.18 1616.62,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip880)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1954.3,1423.18 1954.3,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip880)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2291.97,1423.18 2291.97,1404.28 \"/>\n",
       "<path clip-path=\"url(#clip880)\" d=\"M265.903 1454.1 Q262.292 1454.1 260.463 1457.66 Q258.658 1461.2 258.658 1468.33 Q258.658 1475.44 260.463 1479.01 Q262.292 1482.55 265.903 1482.55 Q269.537 1482.55 271.343 1479.01 Q273.172 1475.44 273.172 1468.33 Q273.172 1461.2 271.343 1457.66 Q269.537 1454.1 265.903 1454.1 M265.903 1450.39 Q271.713 1450.39 274.769 1455 Q277.847 1459.58 277.847 1468.33 Q277.847 1477.06 274.769 1481.67 Q271.713 1486.25 265.903 1486.25 Q260.093 1486.25 257.014 1481.67 Q253.959 1477.06 253.959 1468.33 Q253.959 1459.58 257.014 1455 Q260.093 1450.39 265.903 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M578.269 1481.64 L585.908 1481.64 L585.908 1455.28 L577.598 1456.95 L577.598 1452.69 L585.862 1451.02 L590.538 1451.02 L590.538 1481.64 L598.176 1481.64 L598.176 1485.58 L578.269 1485.58 L578.269 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M617.621 1454.1 Q614.01 1454.1 612.181 1457.66 Q610.375 1461.2 610.375 1468.33 Q610.375 1475.44 612.181 1479.01 Q614.01 1482.55 617.621 1482.55 Q621.255 1482.55 623.061 1479.01 Q624.889 1475.44 624.889 1468.33 Q624.889 1461.2 623.061 1457.66 Q621.255 1454.1 617.621 1454.1 M617.621 1450.39 Q623.431 1450.39 626.487 1455 Q629.565 1459.58 629.565 1468.33 Q629.565 1477.06 626.487 1481.67 Q623.431 1486.25 617.621 1486.25 Q611.811 1486.25 608.732 1481.67 Q605.676 1477.06 605.676 1468.33 Q605.676 1459.58 608.732 1455 Q611.811 1450.39 617.621 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M920.033 1481.64 L936.353 1481.64 L936.353 1485.58 L914.408 1485.58 L914.408 1481.64 Q917.07 1478.89 921.654 1474.26 Q926.26 1469.61 927.441 1468.27 Q929.686 1465.74 930.566 1464.01 Q931.468 1462.25 931.468 1460.56 Q931.468 1457.8 929.524 1456.07 Q927.603 1454.33 924.501 1454.33 Q922.302 1454.33 919.848 1455.09 Q917.418 1455.86 914.64 1457.41 L914.64 1452.69 Q917.464 1451.55 919.918 1450.97 Q922.371 1450.39 924.408 1450.39 Q929.779 1450.39 932.973 1453.08 Q936.167 1455.77 936.167 1460.26 Q936.167 1462.39 935.357 1464.31 Q934.57 1466.2 932.464 1468.8 Q931.885 1469.47 928.783 1472.69 Q925.681 1475.88 920.033 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M956.167 1454.1 Q952.556 1454.1 950.728 1457.66 Q948.922 1461.2 948.922 1468.33 Q948.922 1475.44 950.728 1479.01 Q952.556 1482.55 956.167 1482.55 Q959.802 1482.55 961.607 1479.01 Q963.436 1475.44 963.436 1468.33 Q963.436 1461.2 961.607 1457.66 Q959.802 1454.1 956.167 1454.1 M956.167 1450.39 Q961.977 1450.39 965.033 1455 Q968.112 1459.58 968.112 1468.33 Q968.112 1477.06 965.033 1481.67 Q961.977 1486.25 956.167 1486.25 Q950.357 1486.25 947.278 1481.67 Q944.223 1477.06 944.223 1468.33 Q944.223 1459.58 947.278 1455 Q950.357 1450.39 956.167 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M1267.78 1466.95 Q1271.14 1467.66 1273.01 1469.93 Q1274.91 1472.2 1274.91 1475.53 Q1274.91 1480.65 1271.39 1483.45 Q1267.87 1486.25 1261.39 1486.25 Q1259.22 1486.25 1256.9 1485.81 Q1254.61 1485.39 1252.16 1484.54 L1252.16 1480.02 Q1254.1 1481.16 1256.42 1481.74 Q1258.73 1482.32 1261.25 1482.32 Q1265.65 1482.32 1267.94 1480.58 Q1270.26 1478.84 1270.26 1475.53 Q1270.26 1472.48 1268.11 1470.77 Q1265.98 1469.03 1262.16 1469.03 L1258.13 1469.03 L1258.13 1465.19 L1262.34 1465.19 Q1265.79 1465.19 1267.62 1463.82 Q1269.45 1462.43 1269.45 1459.84 Q1269.45 1457.18 1267.55 1455.77 Q1265.67 1454.33 1262.16 1454.33 Q1260.23 1454.33 1258.04 1454.75 Q1255.84 1455.16 1253.2 1456.04 L1253.2 1451.88 Q1255.86 1451.14 1258.17 1450.77 Q1260.51 1450.39 1262.57 1450.39 Q1267.9 1450.39 1271 1452.83 Q1274.1 1455.23 1274.1 1459.35 Q1274.1 1462.22 1272.46 1464.21 Q1270.81 1466.18 1267.78 1466.95 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M1293.78 1454.1 Q1290.17 1454.1 1288.34 1457.66 Q1286.53 1461.2 1286.53 1468.33 Q1286.53 1475.44 1288.34 1479.01 Q1290.17 1482.55 1293.78 1482.55 Q1297.41 1482.55 1299.22 1479.01 Q1301.04 1475.44 1301.04 1468.33 Q1301.04 1461.2 1299.22 1457.66 Q1297.41 1454.1 1293.78 1454.1 M1293.78 1450.39 Q1299.59 1450.39 1302.64 1455 Q1305.72 1459.58 1305.72 1468.33 Q1305.72 1477.06 1302.64 1481.67 Q1299.59 1486.25 1293.78 1486.25 Q1287.97 1486.25 1284.89 1481.67 Q1281.83 1477.06 1281.83 1468.33 Q1281.83 1459.58 1284.89 1455 Q1287.97 1450.39 1293.78 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M1604.79 1455.09 L1592.98 1473.54 L1604.79 1473.54 L1604.79 1455.09 M1603.56 1451.02 L1609.44 1451.02 L1609.44 1473.54 L1614.37 1473.54 L1614.37 1477.43 L1609.44 1477.43 L1609.44 1485.58 L1604.79 1485.58 L1604.79 1477.43 L1589.19 1477.43 L1589.19 1472.92 L1603.56 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M1632.1 1454.1 Q1628.49 1454.1 1626.66 1457.66 Q1624.86 1461.2 1624.86 1468.33 Q1624.86 1475.44 1626.66 1479.01 Q1628.49 1482.55 1632.1 1482.55 Q1635.74 1482.55 1637.54 1479.01 Q1639.37 1475.44 1639.37 1468.33 Q1639.37 1461.2 1637.54 1457.66 Q1635.74 1454.1 1632.1 1454.1 M1632.1 1450.39 Q1637.91 1450.39 1640.97 1455 Q1644.05 1459.58 1644.05 1468.33 Q1644.05 1477.06 1640.97 1481.67 Q1637.91 1486.25 1632.1 1486.25 Q1626.29 1486.25 1623.21 1481.67 Q1620.16 1477.06 1620.16 1468.33 Q1620.16 1459.58 1623.21 1455 Q1626.29 1450.39 1632.1 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M1928.99 1451.02 L1947.35 1451.02 L1947.35 1454.96 L1933.28 1454.96 L1933.28 1463.43 Q1934.3 1463.08 1935.31 1462.92 Q1936.33 1462.73 1937.35 1462.73 Q1943.14 1462.73 1946.52 1465.9 Q1949.9 1469.08 1949.9 1474.49 Q1949.9 1480.07 1946.43 1483.17 Q1942.95 1486.25 1936.63 1486.25 Q1934.46 1486.25 1932.19 1485.88 Q1929.94 1485.51 1927.54 1484.77 L1927.54 1480.07 Q1929.62 1481.2 1931.84 1481.76 Q1934.06 1482.32 1936.54 1482.32 Q1940.55 1482.32 1942.88 1480.21 Q1945.22 1478.1 1945.22 1474.49 Q1945.22 1470.88 1942.88 1468.77 Q1940.55 1466.67 1936.54 1466.67 Q1934.67 1466.67 1932.79 1467.08 Q1930.94 1467.5 1928.99 1468.38 L1928.99 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M1969.11 1454.1 Q1965.5 1454.1 1963.67 1457.66 Q1961.86 1461.2 1961.86 1468.33 Q1961.86 1475.44 1963.67 1479.01 Q1965.5 1482.55 1969.11 1482.55 Q1972.74 1482.55 1974.55 1479.01 Q1976.38 1475.44 1976.38 1468.33 Q1976.38 1461.2 1974.55 1457.66 Q1972.74 1454.1 1969.11 1454.1 M1969.11 1450.39 Q1974.92 1450.39 1977.98 1455 Q1981.05 1459.58 1981.05 1468.33 Q1981.05 1477.06 1977.98 1481.67 Q1974.92 1486.25 1969.11 1486.25 Q1963.3 1486.25 1960.22 1481.67 Q1957.17 1477.06 1957.17 1468.33 Q1957.17 1459.58 1960.22 1455 Q1963.3 1450.39 1969.11 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M2277.38 1466.44 Q2274.23 1466.44 2272.38 1468.59 Q2270.55 1470.74 2270.55 1474.49 Q2270.55 1478.22 2272.38 1480.39 Q2274.23 1482.55 2277.38 1482.55 Q2280.53 1482.55 2282.36 1480.39 Q2284.21 1478.22 2284.21 1474.49 Q2284.21 1470.74 2282.36 1468.59 Q2280.53 1466.44 2277.38 1466.44 M2286.66 1451.78 L2286.66 1456.04 Q2284.9 1455.21 2283.1 1454.77 Q2281.31 1454.33 2279.55 1454.33 Q2274.93 1454.33 2272.47 1457.45 Q2270.04 1460.58 2269.69 1466.9 Q2271.06 1464.89 2273.12 1463.82 Q2275.18 1462.73 2277.66 1462.73 Q2282.87 1462.73 2285.87 1465.9 Q2288.91 1469.05 2288.91 1474.49 Q2288.91 1479.82 2285.76 1483.03 Q2282.61 1486.25 2277.38 1486.25 Q2271.38 1486.25 2268.21 1481.67 Q2265.04 1477.06 2265.04 1468.33 Q2265.04 1460.14 2268.93 1455.28 Q2272.82 1450.39 2279.37 1450.39 Q2281.13 1450.39 2282.91 1450.74 Q2284.72 1451.09 2286.66 1451.78 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M2306.96 1454.1 Q2303.35 1454.1 2301.52 1457.66 Q2299.72 1461.2 2299.72 1468.33 Q2299.72 1475.44 2301.52 1479.01 Q2303.35 1482.55 2306.96 1482.55 Q2310.6 1482.55 2312.4 1479.01 Q2314.23 1475.44 2314.23 1468.33 Q2314.23 1461.2 2312.4 1457.66 Q2310.6 1454.1 2306.96 1454.1 M2306.96 1450.39 Q2312.77 1450.39 2315.83 1455 Q2318.91 1459.58 2318.91 1468.33 Q2318.91 1477.06 2315.83 1481.67 Q2312.77 1486.25 2306.96 1486.25 Q2301.15 1486.25 2298.07 1481.67 Q2295.02 1477.06 2295.02 1468.33 Q2295.02 1459.58 2298.07 1455 Q2301.15 1450.39 2306.96 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M1169.35 1561.26 L1169.35 1548.5 L1158.85 1548.5 L1158.85 1543.22 L1175.72 1543.22 L1175.72 1563.62 Q1171.99 1566.26 1167.51 1567.63 Q1163.02 1568.97 1157.93 1568.97 Q1146.79 1568.97 1140.48 1562.47 Q1134.21 1555.95 1134.21 1544.33 Q1134.21 1532.68 1140.48 1526.19 Q1146.79 1519.66 1157.93 1519.66 Q1162.57 1519.66 1166.74 1520.81 Q1170.94 1521.96 1174.48 1524.18 L1174.48 1531.03 Q1170.91 1528 1166.9 1526.48 Q1162.89 1524.95 1158.47 1524.95 Q1149.75 1524.95 1145.35 1529.82 Q1140.99 1534.69 1140.99 1544.33 Q1140.99 1553.94 1145.35 1558.81 Q1149.75 1563.68 1158.47 1563.68 Q1161.87 1563.68 1164.55 1563.11 Q1167.22 1562.51 1169.35 1561.26 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M1187.21 1532.4 L1193.07 1532.4 L1193.07 1568.04 L1187.21 1568.04 L1187.21 1532.4 M1187.21 1518.52 L1193.07 1518.52 L1193.07 1525.93 L1187.21 1525.93 L1187.21 1518.52 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M1234.95 1546.53 L1234.95 1568.04 L1229.09 1568.04 L1229.09 1546.72 Q1229.09 1541.66 1227.12 1539.14 Q1225.15 1536.63 1221.2 1536.63 Q1216.46 1536.63 1213.72 1539.65 Q1210.98 1542.68 1210.98 1547.9 L1210.98 1568.04 L1205.1 1568.04 L1205.1 1532.4 L1210.98 1532.4 L1210.98 1537.93 Q1213.09 1534.72 1215.92 1533.13 Q1218.78 1531.54 1222.51 1531.54 Q1228.65 1531.54 1231.8 1535.36 Q1234.95 1539.14 1234.95 1546.53 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M1246.63 1532.4 L1252.49 1532.4 L1252.49 1568.04 L1246.63 1568.04 L1246.63 1532.4 M1246.63 1518.52 L1252.49 1518.52 L1252.49 1525.93 L1246.63 1525.93 L1246.63 1518.52 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M1318.12 1561.26 L1318.12 1548.5 L1307.62 1548.5 L1307.62 1543.22 L1324.49 1543.22 L1324.49 1563.62 Q1320.76 1566.26 1316.27 1567.63 Q1311.79 1568.97 1306.69 1568.97 Q1295.55 1568.97 1289.25 1562.47 Q1282.98 1555.95 1282.98 1544.33 Q1282.98 1532.68 1289.25 1526.19 Q1295.55 1519.66 1306.69 1519.66 Q1311.34 1519.66 1315.51 1520.81 Q1319.71 1521.96 1323.24 1524.18 L1323.24 1531.03 Q1319.68 1528 1315.67 1526.48 Q1311.66 1524.95 1307.23 1524.95 Q1298.51 1524.95 1294.12 1529.82 Q1289.76 1534.69 1289.76 1544.33 Q1289.76 1553.94 1294.12 1558.81 Q1298.51 1563.68 1307.23 1563.68 Q1310.64 1563.68 1313.31 1563.11 Q1315.99 1562.51 1318.12 1561.26 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M1352.18 1550.12 Q1345.08 1550.12 1342.34 1551.75 Q1339.6 1553.37 1339.6 1557.29 Q1339.6 1560.4 1341.64 1562.25 Q1343.71 1564.07 1347.24 1564.07 Q1352.11 1564.07 1355.04 1560.63 Q1358 1557.16 1358 1551.43 L1358 1550.12 L1352.18 1550.12 M1363.86 1547.71 L1363.86 1568.04 L1358 1568.04 L1358 1562.63 Q1356 1565.88 1353 1567.44 Q1350.01 1568.97 1345.68 1568.97 Q1340.21 1568.97 1336.96 1565.91 Q1333.75 1562.82 1333.75 1557.67 Q1333.75 1551.65 1337.76 1548.6 Q1341.8 1545.54 1349.79 1545.54 L1358 1545.54 L1358 1544.97 Q1358 1540.93 1355.33 1538.73 Q1352.69 1536.5 1347.88 1536.5 Q1344.82 1536.5 1341.93 1537.23 Q1339.03 1537.97 1336.36 1539.43 L1336.36 1534.02 Q1339.57 1532.78 1342.6 1532.17 Q1345.62 1531.54 1348.48 1531.54 Q1356.22 1531.54 1360.04 1535.55 Q1363.86 1539.56 1363.86 1547.71 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M1375.92 1532.4 L1381.78 1532.4 L1381.78 1568.04 L1375.92 1568.04 L1375.92 1532.4 M1375.92 1518.52 L1381.78 1518.52 L1381.78 1525.93 L1375.92 1525.93 L1375.92 1518.52 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M1423.66 1546.53 L1423.66 1568.04 L1417.81 1568.04 L1417.81 1546.72 Q1417.81 1541.66 1415.83 1539.14 Q1413.86 1536.63 1409.91 1536.63 Q1405.17 1536.63 1402.43 1539.65 Q1399.7 1542.68 1399.7 1547.9 L1399.7 1568.04 L1393.81 1568.04 L1393.81 1532.4 L1399.7 1532.4 L1399.7 1537.93 Q1401.8 1534.72 1404.63 1533.13 Q1407.49 1531.54 1411.22 1531.54 Q1417.36 1531.54 1420.51 1535.36 Q1423.66 1539.14 1423.66 1546.53 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip880)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1423.18 205.121,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip880)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1398.7 224.019,1398.7 \"/>\n",
       "<polyline clip-path=\"url(#clip880)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1193.66 224.019,1193.66 \"/>\n",
       "<polyline clip-path=\"url(#clip880)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,988.618 224.019,988.618 \"/>\n",
       "<polyline clip-path=\"url(#clip880)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,783.578 224.019,783.578 \"/>\n",
       "<polyline clip-path=\"url(#clip880)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,578.538 224.019,578.538 \"/>\n",
       "<polyline clip-path=\"url(#clip880)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,373.498 224.019,373.498 \"/>\n",
       "<polyline clip-path=\"url(#clip880)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,168.458 224.019,168.458 \"/>\n",
       "<path clip-path=\"url(#clip880)\" d=\"M157.177 1384.5 Q153.566 1384.5 151.737 1388.06 Q149.931 1391.6 149.931 1398.73 Q149.931 1405.84 151.737 1409.4 Q153.566 1412.95 157.177 1412.95 Q160.811 1412.95 162.616 1409.4 Q164.445 1405.84 164.445 1398.73 Q164.445 1391.6 162.616 1388.06 Q160.811 1384.5 157.177 1384.5 M157.177 1380.79 Q162.987 1380.79 166.042 1385.4 Q169.121 1389.98 169.121 1398.73 Q169.121 1407.46 166.042 1412.07 Q162.987 1416.65 157.177 1416.65 Q151.366 1416.65 148.288 1412.07 Q145.232 1407.46 145.232 1398.73 Q145.232 1389.98 148.288 1385.4 Q151.366 1380.79 157.177 1380.79 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M117.825 1207 L125.464 1207 L125.464 1180.64 L117.154 1182.3 L117.154 1178.04 L125.418 1176.38 L130.093 1176.38 L130.093 1207 L137.732 1207 L137.732 1210.94 L117.825 1210.94 L117.825 1207 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M157.177 1179.46 Q153.566 1179.46 151.737 1183.02 Q149.931 1186.56 149.931 1193.69 Q149.931 1200.8 151.737 1204.36 Q153.566 1207.91 157.177 1207.91 Q160.811 1207.91 162.616 1204.36 Q164.445 1200.8 164.445 1193.69 Q164.445 1186.56 162.616 1183.02 Q160.811 1179.46 157.177 1179.46 M157.177 1175.75 Q162.987 1175.75 166.042 1180.36 Q169.121 1184.94 169.121 1193.69 Q169.121 1202.42 166.042 1207.03 Q162.987 1211.61 157.177 1211.61 Q151.366 1211.61 148.288 1207.03 Q145.232 1202.42 145.232 1193.69 Q145.232 1184.94 148.288 1180.36 Q151.366 1175.75 157.177 1175.75 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M121.043 1001.96 L137.362 1001.96 L137.362 1005.9 L115.418 1005.9 L115.418 1001.96 Q118.08 999.208 122.663 994.579 Q127.269 989.926 128.45 988.583 Q130.695 986.06 131.575 984.324 Q132.478 982.565 132.478 980.875 Q132.478 978.12 130.533 976.384 Q128.612 974.648 125.51 974.648 Q123.311 974.648 120.857 975.412 Q118.427 976.176 115.649 977.727 L115.649 973.005 Q118.473 971.87 120.927 971.292 Q123.38 970.713 125.418 970.713 Q130.788 970.713 133.982 973.398 Q137.177 976.083 137.177 980.574 Q137.177 982.704 136.367 984.625 Q135.579 986.523 133.473 989.116 Q132.894 989.787 129.792 993.005 Q126.691 996.199 121.043 1001.96 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M157.177 974.417 Q153.566 974.417 151.737 977.981 Q149.931 981.523 149.931 988.653 Q149.931 995.759 151.737 999.324 Q153.566 1002.87 157.177 1002.87 Q160.811 1002.87 162.616 999.324 Q164.445 995.759 164.445 988.653 Q164.445 981.523 162.616 977.981 Q160.811 974.417 157.177 974.417 M157.177 970.713 Q162.987 970.713 166.042 975.319 Q169.121 979.903 169.121 988.653 Q169.121 997.38 166.042 1001.99 Q162.987 1006.57 157.177 1006.57 Q151.366 1006.57 148.288 1001.99 Q145.232 997.38 145.232 988.653 Q145.232 979.903 148.288 975.319 Q151.366 970.713 157.177 970.713 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M131.181 782.224 Q134.538 782.941 136.413 785.21 Q138.311 787.478 138.311 790.812 Q138.311 795.927 134.792 798.728 Q131.274 801.529 124.793 801.529 Q122.617 801.529 120.302 801.089 Q118.01 800.673 115.556 799.816 L115.556 795.302 Q117.501 796.437 119.816 797.015 Q122.13 797.594 124.654 797.594 Q129.052 797.594 131.343 795.858 Q133.658 794.122 133.658 790.812 Q133.658 787.756 131.505 786.043 Q129.376 784.307 125.556 784.307 L121.529 784.307 L121.529 780.465 L125.742 780.465 Q129.191 780.465 131.019 779.099 Q132.848 777.71 132.848 775.117 Q132.848 772.455 130.95 771.043 Q129.075 769.608 125.556 769.608 Q123.635 769.608 121.436 770.025 Q119.237 770.441 116.598 771.321 L116.598 767.154 Q119.26 766.414 121.575 766.043 Q123.913 765.673 125.973 765.673 Q131.297 765.673 134.399 768.104 Q137.501 770.511 137.501 774.631 Q137.501 777.502 135.857 779.492 Q134.214 781.46 131.181 782.224 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M157.177 769.377 Q153.566 769.377 151.737 772.941 Q149.931 776.483 149.931 783.613 Q149.931 790.719 151.737 794.284 Q153.566 797.826 157.177 797.826 Q160.811 797.826 162.616 794.284 Q164.445 790.719 164.445 783.613 Q164.445 776.483 162.616 772.941 Q160.811 769.377 157.177 769.377 M157.177 765.673 Q162.987 765.673 166.042 770.279 Q169.121 774.863 169.121 783.613 Q169.121 792.34 166.042 796.946 Q162.987 801.529 157.177 801.529 Q151.366 801.529 148.288 796.946 Q145.232 792.34 145.232 783.613 Q145.232 774.863 148.288 770.279 Q151.366 765.673 157.177 765.673 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M129.862 565.332 L118.056 583.781 L129.862 583.781 L129.862 565.332 M128.635 561.258 L134.515 561.258 L134.515 583.781 L139.445 583.781 L139.445 587.67 L134.515 587.67 L134.515 595.818 L129.862 595.818 L129.862 587.67 L114.26 587.67 L114.26 583.156 L128.635 561.258 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M157.177 564.337 Q153.566 564.337 151.737 567.901 Q149.931 571.443 149.931 578.573 Q149.931 585.679 151.737 589.244 Q153.566 592.786 157.177 592.786 Q160.811 592.786 162.616 589.244 Q164.445 585.679 164.445 578.573 Q164.445 571.443 162.616 567.901 Q160.811 564.337 157.177 564.337 M157.177 560.633 Q162.987 560.633 166.042 565.239 Q169.121 569.823 169.121 578.573 Q169.121 587.299 166.042 591.906 Q162.987 596.489 157.177 596.489 Q151.366 596.489 148.288 591.906 Q145.232 587.299 145.232 578.573 Q145.232 569.823 148.288 565.239 Q151.366 560.633 157.177 560.633 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M117.061 356.218 L135.417 356.218 L135.417 360.153 L121.343 360.153 L121.343 368.625 Q122.362 368.278 123.38 368.116 Q124.399 367.931 125.418 367.931 Q131.205 367.931 134.584 371.102 Q137.964 374.273 137.964 379.69 Q137.964 385.269 134.492 388.371 Q131.019 391.449 124.7 391.449 Q122.524 391.449 120.255 391.079 Q118.01 390.708 115.603 389.968 L115.603 385.269 Q117.686 386.403 119.908 386.959 Q122.13 387.514 124.607 387.514 Q128.612 387.514 130.95 385.408 Q133.288 383.301 133.288 379.69 Q133.288 376.079 130.95 373.972 Q128.612 371.866 124.607 371.866 Q122.732 371.866 120.857 372.283 Q119.006 372.699 117.061 373.579 L117.061 356.218 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M157.177 359.297 Q153.566 359.297 151.737 362.861 Q149.931 366.403 149.931 373.533 Q149.931 380.639 151.737 384.204 Q153.566 387.746 157.177 387.746 Q160.811 387.746 162.616 384.204 Q164.445 380.639 164.445 373.533 Q164.445 366.403 162.616 362.861 Q160.811 359.297 157.177 359.297 M157.177 355.593 Q162.987 355.593 166.042 360.199 Q169.121 364.783 169.121 373.533 Q169.121 382.259 166.042 386.866 Q162.987 391.449 157.177 391.449 Q151.366 391.449 148.288 386.866 Q145.232 382.259 145.232 373.533 Q145.232 364.783 148.288 360.199 Q151.366 355.593 157.177 355.593 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M127.593 166.594 Q124.445 166.594 122.593 168.747 Q120.765 170.9 120.765 174.65 Q120.765 178.377 122.593 180.553 Q124.445 182.706 127.593 182.706 Q130.742 182.706 132.57 180.553 Q134.422 178.377 134.422 174.65 Q134.422 170.9 132.57 168.747 Q130.742 166.594 127.593 166.594 M136.876 151.942 L136.876 156.201 Q135.117 155.368 133.311 154.928 Q131.529 154.488 129.769 154.488 Q125.14 154.488 122.686 157.613 Q120.255 160.738 119.908 167.057 Q121.274 165.044 123.334 163.979 Q125.394 162.891 127.871 162.891 Q133.08 162.891 136.089 166.062 Q139.121 169.21 139.121 174.65 Q139.121 179.974 135.973 183.192 Q132.825 186.409 127.593 186.409 Q121.598 186.409 118.427 181.826 Q115.256 177.219 115.256 168.493 Q115.256 160.298 119.144 155.437 Q123.033 150.553 129.584 150.553 Q131.343 150.553 133.126 150.9 Q134.931 151.247 136.876 151.942 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M157.177 154.257 Q153.566 154.257 151.737 157.821 Q149.931 161.363 149.931 168.493 Q149.931 175.599 151.737 179.164 Q153.566 182.706 157.177 182.706 Q160.811 182.706 162.616 179.164 Q164.445 175.599 164.445 168.493 Q164.445 161.363 162.616 157.821 Q160.811 154.257 157.177 154.257 M157.177 150.553 Q162.987 150.553 166.042 155.159 Q169.121 159.743 169.121 168.493 Q169.121 177.219 166.042 181.826 Q162.987 186.409 157.177 186.409 Q151.366 186.409 148.288 181.826 Q145.232 177.219 145.232 168.493 Q145.232 159.743 148.288 155.159 Q151.366 150.553 157.177 150.553 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M16.4842 891.553 L16.4842 864.244 L21.895 864.244 L21.895 885.124 L35.8996 885.124 L35.8996 866.281 L41.3104 866.281 L41.3104 885.124 L64.0042 885.124 L64.0042 891.553 L16.4842 891.553 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M44.7161 827.387 L47.5806 827.387 L47.5806 854.314 Q53.6281 853.932 56.8109 850.685 Q59.9619 847.407 59.9619 841.582 Q59.9619 838.208 59.1344 835.057 Q58.3069 831.875 56.6518 828.755 L62.1899 828.755 Q63.5267 831.906 64.227 835.217 Q64.9272 838.527 64.9272 841.932 Q64.9272 850.462 59.9619 855.46 Q54.9967 860.425 46.5303 860.425 Q37.7774 860.425 32.6531 855.714 Q27.4968 850.972 27.4968 842.951 Q27.4968 835.758 32.1438 831.588 Q36.7589 827.387 44.7161 827.387 M42.9973 833.243 Q38.1912 833.307 35.3266 835.949 Q32.4621 838.559 32.4621 842.887 Q32.4621 847.789 35.2312 850.749 Q38.0002 853.677 43.0292 854.123 L42.9973 833.243 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M46.0847 801.574 Q46.0847 808.672 47.7079 811.409 Q49.3312 814.146 53.2461 814.146 Q56.3653 814.146 58.2114 812.109 Q60.0256 810.04 60.0256 806.507 Q60.0256 801.637 56.5881 798.709 Q53.1188 795.749 47.3897 795.749 L46.0847 795.749 L46.0847 801.574 M43.6657 789.893 L64.0042 789.893 L64.0042 795.749 L58.5933 795.749 Q61.8398 797.754 63.3994 800.746 Q64.9272 803.738 64.9272 808.067 Q64.9272 813.541 61.8716 816.788 Q58.7843 820.003 53.6281 820.003 Q47.6125 820.003 44.5569 815.992 Q41.5014 811.95 41.5014 803.961 L41.5014 795.749 L40.9285 795.749 Q36.8862 795.749 34.6901 798.423 Q32.4621 801.065 32.4621 805.871 Q32.4621 808.926 33.1941 811.823 Q33.9262 814.719 35.3903 817.393 L29.9795 817.393 Q28.7381 814.178 28.1334 811.154 Q27.4968 808.13 27.4968 805.266 Q27.4968 797.532 31.5072 793.712 Q35.5176 789.893 43.6657 789.893 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M18.2347 772.037 L28.3562 772.037 L28.3562 759.974 L32.9077 759.974 L32.9077 772.037 L52.2594 772.037 Q56.6199 772.037 57.8613 770.859 Q59.1026 769.65 59.1026 765.99 L59.1026 759.974 L64.0042 759.974 L64.0042 765.99 Q64.0042 772.769 61.4897 775.347 Q58.9434 777.925 52.2594 777.925 L32.9077 777.925 L32.9077 782.222 L28.3562 782.222 L28.3562 777.925 L18.2347 777.925 L18.2347 772.037 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M49.9359 752.876 L28.3562 752.876 L28.3562 747.02 L49.7131 747.02 Q54.7739 747.02 57.3202 745.046 Q59.8346 743.073 59.8346 739.126 Q59.8346 734.384 56.8109 731.647 Q53.7872 728.877 48.5673 728.877 L28.3562 728.877 L28.3562 723.021 L64.0042 723.021 L64.0042 728.877 L58.5296 728.877 Q61.7762 731.01 63.3676 733.843 Q64.9272 736.644 64.9272 740.368 Q64.9272 746.51 61.1078 749.693 Q57.2883 752.876 49.9359 752.876 M27.4968 738.14 L27.4968 738.14 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M33.8307 690.301 Q33.2578 691.288 33.0032 692.466 Q32.7167 693.611 32.7167 695.012 Q32.7167 699.977 35.9632 702.651 Q39.1779 705.292 45.2253 705.292 L64.0042 705.292 L64.0042 711.181 L28.3562 711.181 L28.3562 705.292 L33.8944 705.292 Q30.6479 703.446 29.0883 700.486 Q27.4968 697.526 27.4968 693.293 Q27.4968 692.688 27.5923 691.956 Q27.656 691.224 27.8151 690.333 L33.8307 690.301 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M44.7161 655.099 L47.5806 655.099 L47.5806 682.026 Q53.6281 681.644 56.8109 678.397 Q59.9619 675.119 59.9619 669.294 Q59.9619 665.921 59.1344 662.77 Q58.3069 659.587 56.6518 656.468 L62.1899 656.468 Q63.5267 659.619 64.227 662.929 Q64.9272 666.239 64.9272 669.645 Q64.9272 678.175 59.9619 683.172 Q54.9967 688.137 46.5303 688.137 Q37.7774 688.137 32.6531 683.426 Q27.4968 678.684 27.4968 670.663 Q27.4968 663.47 32.1438 659.3 Q36.7589 655.099 44.7161 655.099 M42.9973 660.955 Q38.1912 661.019 35.3266 663.661 Q32.4621 666.271 32.4621 670.599 Q32.4621 675.501 35.2312 678.461 Q38.0002 681.389 43.0292 681.835 L42.9973 660.955 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M877.575 12.096 L912.332 12.096 L912.332 18.9825 L885.758 18.9825 L885.758 36.8065 L909.739 36.8065 L909.739 43.6931 L885.758 43.6931 L885.758 72.576 L877.575 72.576 L877.575 12.096 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M959.241 48.0275 L959.241 51.6733 L924.97 51.6733 Q925.457 59.3701 929.588 63.421 Q933.761 67.4314 941.174 67.4314 Q945.468 67.4314 949.478 66.3781 Q953.529 65.3249 957.499 63.2184 L957.499 70.267 Q953.489 71.9684 949.276 72.8596 Q945.063 73.7508 940.728 73.7508 Q929.872 73.7508 923.512 67.4314 Q917.193 61.1119 917.193 50.3365 Q917.193 39.1965 923.188 32.6746 Q929.224 26.1121 939.432 26.1121 Q948.587 26.1121 953.894 32.0264 Q959.241 37.9003 959.241 48.0275 M951.787 45.84 Q951.706 39.7232 948.344 36.0774 Q945.022 32.4315 939.513 32.4315 Q933.275 32.4315 929.507 35.9558 Q925.781 39.4801 925.213 45.8805 L951.787 45.84 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M992.094 49.7694 Q983.06 49.7694 979.577 51.8354 Q976.093 53.9013 976.093 58.8839 Q976.093 62.8538 978.685 65.2034 Q981.319 67.5124 985.815 67.5124 Q992.013 67.5124 995.74 63.1374 Q999.507 58.7219 999.507 51.4303 L999.507 49.7694 L992.094 49.7694 M1006.96 46.6907 L1006.96 72.576 L999.507 72.576 L999.507 65.6895 Q996.955 69.8214 993.147 71.8063 Q989.339 73.7508 983.83 73.7508 Q976.863 73.7508 972.731 69.8619 Q968.639 65.9325 968.639 59.3701 Q968.639 51.7138 973.743 47.825 Q978.888 43.9361 989.056 43.9361 L999.507 43.9361 L999.507 43.2069 Q999.507 38.0623 996.104 35.2672 Q992.742 32.4315 986.625 32.4315 Q982.736 32.4315 979.05 33.3632 Q975.364 34.295 971.961 36.1584 L971.961 29.2718 Q976.052 27.692 979.901 26.9223 Q983.749 26.1121 987.395 26.1121 Q997.239 26.1121 1002.1 31.2163 Q1006.96 36.3204 1006.96 46.6907 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M1029.69 14.324 L1029.69 27.2059 L1045.04 27.2059 L1045.04 32.9987 L1029.69 32.9987 L1029.69 57.6282 Q1029.69 63.1779 1031.19 64.7578 Q1032.72 66.3376 1037.38 66.3376 L1045.04 66.3376 L1045.04 72.576 L1037.38 72.576 Q1028.75 72.576 1025.47 69.3758 Q1022.19 66.1351 1022.19 57.6282 L1022.19 32.9987 L1016.72 32.9987 L1016.72 27.2059 L1022.19 27.2059 L1022.19 14.324 L1029.69 14.324 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M1054.07 54.671 L1054.07 27.2059 L1061.53 27.2059 L1061.53 54.3874 Q1061.53 60.8284 1064.04 64.0691 Q1066.55 67.2693 1071.57 67.2693 Q1077.61 67.2693 1081.09 63.421 Q1084.62 59.5726 1084.62 52.9291 L1084.62 27.2059 L1092.07 27.2059 L1092.07 72.576 L1084.62 72.576 L1084.62 65.6084 Q1081.9 69.7404 1078.3 71.7658 Q1074.73 73.7508 1069.99 73.7508 Q1062.17 73.7508 1058.12 68.8897 Q1054.07 64.0286 1054.07 54.671 M1072.83 26.1121 L1072.83 26.1121 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M1133.71 34.1734 Q1132.46 33.4443 1130.96 33.1202 Q1129.5 32.7556 1127.72 32.7556 Q1121.4 32.7556 1118 36.8875 Q1114.63 40.9789 1114.63 48.6757 L1114.63 72.576 L1107.14 72.576 L1107.14 27.2059 L1114.63 27.2059 L1114.63 34.2544 Q1116.98 30.1225 1120.75 28.1376 Q1124.52 26.1121 1129.91 26.1121 Q1130.68 26.1121 1131.61 26.2337 Q1132.54 26.3147 1133.67 26.5172 L1133.71 34.1734 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M1178.52 48.0275 L1178.52 51.6733 L1144.25 51.6733 Q1144.73 59.3701 1148.86 63.421 Q1153.04 67.4314 1160.45 67.4314 Q1164.74 67.4314 1168.75 66.3781 Q1172.8 65.3249 1176.77 63.2184 L1176.77 70.267 Q1172.76 71.9684 1168.55 72.8596 Q1164.34 73.7508 1160 73.7508 Q1149.15 73.7508 1142.79 67.4314 Q1136.47 61.1119 1136.47 50.3365 Q1136.47 39.1965 1142.46 32.6746 Q1148.5 26.1121 1158.71 26.1121 Q1167.86 26.1121 1173.17 32.0264 Q1178.52 37.9003 1178.52 48.0275 M1171.06 45.84 Q1170.98 39.7232 1167.62 36.0774 Q1164.3 32.4315 1158.79 32.4315 Q1152.55 32.4315 1148.78 35.9558 Q1145.06 39.4801 1144.49 45.8805 L1171.06 45.84 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M1217.45 12.096 L1225.63 12.096 L1225.63 72.576 L1217.45 72.576 L1217.45 12.096 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M1276.91 35.9153 Q1279.71 30.8922 1283.6 28.5022 Q1287.49 26.1121 1292.75 26.1121 Q1299.84 26.1121 1303.69 31.0947 Q1307.54 36.0368 1307.54 45.1919 L1307.54 72.576 L1300.04 72.576 L1300.04 45.4349 Q1300.04 38.913 1297.73 35.7533 Q1295.43 32.5936 1290.69 32.5936 Q1284.89 32.5936 1281.53 36.4419 Q1278.17 40.2903 1278.17 46.9338 L1278.17 72.576 L1270.67 72.576 L1270.67 45.4349 Q1270.67 38.8725 1268.37 35.7533 Q1266.06 32.5936 1261.24 32.5936 Q1255.52 32.5936 1252.16 36.4824 Q1248.8 40.3308 1248.8 46.9338 L1248.8 72.576 L1241.31 72.576 L1241.31 27.2059 L1248.8 27.2059 L1248.8 34.2544 Q1251.35 30.082 1254.92 28.0971 Q1258.48 26.1121 1263.38 26.1121 Q1268.33 26.1121 1271.77 28.6237 Q1275.25 31.1352 1276.91 35.9153 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M1329.62 65.7705 L1329.62 89.8329 L1322.12 89.8329 L1322.12 27.2059 L1329.62 27.2059 L1329.62 34.0924 Q1331.96 30.0415 1335.53 28.0971 Q1339.13 26.1121 1344.12 26.1121 Q1352.38 26.1121 1357.53 32.6746 Q1362.71 39.2371 1362.71 49.9314 Q1362.71 60.6258 1357.53 67.1883 Q1352.38 73.7508 1344.12 73.7508 Q1339.13 73.7508 1335.53 71.8063 Q1331.96 69.8214 1329.62 65.7705 M1354.97 49.9314 Q1354.97 41.7081 1351.57 37.0496 Q1348.21 32.3505 1342.29 32.3505 Q1336.38 32.3505 1332.98 37.0496 Q1329.62 41.7081 1329.62 49.9314 Q1329.62 58.1548 1332.98 62.8538 Q1336.38 67.5124 1342.29 67.5124 Q1348.21 67.5124 1351.57 62.8538 Q1354.97 58.1548 1354.97 49.9314 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M1392.65 32.4315 Q1386.65 32.4315 1383.17 37.1306 Q1379.68 41.7891 1379.68 49.9314 Q1379.68 58.0738 1383.13 62.7728 Q1386.61 67.4314 1392.65 67.4314 Q1398.6 67.4314 1402.09 62.7323 Q1405.57 58.0333 1405.57 49.9314 Q1405.57 41.8701 1402.09 37.1711 Q1398.6 32.4315 1392.65 32.4315 M1392.65 26.1121 Q1402.37 26.1121 1407.92 32.4315 Q1413.47 38.7509 1413.47 49.9314 Q1413.47 61.0714 1407.92 67.4314 Q1402.37 73.7508 1392.65 73.7508 Q1382.88 73.7508 1377.33 67.4314 Q1371.83 61.0714 1371.83 49.9314 Q1371.83 38.7509 1377.33 32.4315 Q1382.88 26.1121 1392.65 26.1121 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M1452.11 34.1734 Q1450.86 33.4443 1449.36 33.1202 Q1447.9 32.7556 1446.12 32.7556 Q1439.8 32.7556 1436.4 36.8875 Q1433.03 40.9789 1433.03 48.6757 L1433.03 72.576 L1425.54 72.576 L1425.54 27.2059 L1433.03 27.2059 L1433.03 34.2544 Q1435.38 30.1225 1439.15 28.1376 Q1442.92 26.1121 1448.31 26.1121 Q1449.08 26.1121 1450.01 26.2337 Q1450.94 26.3147 1452.07 26.5172 L1452.11 34.1734 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M1467.31 14.324 L1467.31 27.2059 L1482.66 27.2059 L1482.66 32.9987 L1467.31 32.9987 L1467.31 57.6282 Q1467.31 63.1779 1468.8 64.7578 Q1470.34 66.3376 1475 66.3376 L1482.66 66.3376 L1482.66 72.576 L1475 72.576 Q1466.37 72.576 1463.09 69.3758 Q1459.81 66.1351 1459.81 57.6282 L1459.81 32.9987 L1454.34 32.9987 L1454.34 27.2059 L1459.81 27.2059 L1459.81 14.324 L1467.31 14.324 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M1513.08 49.7694 Q1504.05 49.7694 1500.56 51.8354 Q1497.08 53.9013 1497.08 58.8839 Q1497.08 62.8538 1499.67 65.2034 Q1502.31 67.5124 1506.8 67.5124 Q1513 67.5124 1516.73 63.1374 Q1520.49 58.7219 1520.49 51.4303 L1520.49 49.7694 L1513.08 49.7694 M1527.95 46.6907 L1527.95 72.576 L1520.49 72.576 L1520.49 65.6895 Q1517.94 69.8214 1514.13 71.8063 Q1510.33 73.7508 1504.82 73.7508 Q1497.85 73.7508 1493.72 69.8619 Q1489.63 65.9325 1489.63 59.3701 Q1489.63 51.7138 1494.73 47.825 Q1499.87 43.9361 1510.04 43.9361 L1520.49 43.9361 L1520.49 43.2069 Q1520.49 38.0623 1517.09 35.2672 Q1513.73 32.4315 1507.61 32.4315 Q1503.72 32.4315 1500.04 33.3632 Q1496.35 34.295 1492.95 36.1584 L1492.95 29.2718 Q1497.04 27.692 1500.89 26.9223 Q1504.74 26.1121 1508.38 26.1121 Q1518.23 26.1121 1523.09 31.2163 Q1527.95 36.3204 1527.95 46.6907 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M1581.01 45.1919 L1581.01 72.576 L1573.56 72.576 L1573.56 45.4349 Q1573.56 38.994 1571.05 35.7938 Q1568.54 32.5936 1563.51 32.5936 Q1557.48 32.5936 1553.99 36.4419 Q1550.51 40.2903 1550.51 46.9338 L1550.51 72.576 L1543.02 72.576 L1543.02 27.2059 L1550.51 27.2059 L1550.51 34.2544 Q1553.18 30.163 1556.79 28.1376 Q1560.44 26.1121 1565.18 26.1121 Q1572.99 26.1121 1577 30.9732 Q1581.01 35.7938 1581.01 45.1919 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M1628.53 28.9478 L1628.53 35.9153 Q1625.37 34.1734 1622.17 33.3227 Q1619.01 32.4315 1615.77 32.4315 Q1608.52 32.4315 1604.51 37.0496 Q1600.5 41.6271 1600.5 49.9314 Q1600.5 58.2358 1604.51 62.8538 Q1608.52 67.4314 1615.77 67.4314 Q1619.01 67.4314 1622.17 66.5807 Q1625.37 65.6895 1628.53 63.9476 L1628.53 70.8341 Q1625.41 72.2924 1622.05 73.0216 Q1618.73 73.7508 1614.96 73.7508 Q1604.71 73.7508 1598.68 67.3098 Q1592.64 60.8689 1592.64 49.9314 Q1592.64 38.832 1598.72 32.472 Q1604.83 26.1121 1615.45 26.1121 Q1618.89 26.1121 1622.17 26.8413 Q1625.45 27.5299 1628.53 28.9478 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip880)\" d=\"M1680.3 48.0275 L1680.3 51.6733 L1646.03 51.6733 Q1646.52 59.3701 1650.65 63.421 Q1654.82 67.4314 1662.23 67.4314 Q1666.53 67.4314 1670.54 66.3781 Q1674.59 65.3249 1678.56 63.2184 L1678.56 70.267 Q1674.55 71.9684 1670.34 72.8596 Q1666.12 73.7508 1661.79 73.7508 Q1650.93 73.7508 1644.57 67.4314 Q1638.25 61.1119 1638.25 50.3365 Q1638.25 39.1965 1644.25 32.6746 Q1650.28 26.1121 1660.49 26.1121 Q1669.65 26.1121 1674.95 32.0264 Q1680.3 37.9003 1680.3 48.0275 M1672.85 45.84 Q1672.77 39.7232 1669.41 36.0774 Q1666.08 32.4315 1660.57 32.4315 Q1654.34 32.4315 1650.57 35.9558 Q1646.84 39.4801 1646.27 45.8805 L1672.85 45.84 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip882)\" d=\"M266.107 1386.4 L265.903 1386.4 L265.903 1369.99 L266.107 1369.99 L266.107 1386.4 L266.107 1386.4  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.107,1386.4 265.903,1386.4 265.903,1369.99 266.107,1369.99 266.107,1386.4 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.203 1365.89 L265.903 1365.89 L265.903 1349.49 L266.203 1349.49 L266.203 1365.89 L266.203 1365.89  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.203,1365.89 265.903,1365.89 265.903,1349.49 266.203,1349.49 266.203,1365.89 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.06 1345.39 L265.903 1345.39 L265.903 1328.98 L266.06 1328.98 L266.06 1345.39 L266.06 1345.39  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.06,1345.39 265.903,1345.39 265.903,1328.98 266.06,1328.98 266.06,1345.39 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M267.077 1324.88 L265.903 1324.88 L265.903 1308.48 L267.077 1308.48 L267.077 1324.88 L267.077 1324.88  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"267.077,1324.88 265.903,1324.88 265.903,1308.48 267.077,1308.48 267.077,1324.88 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.252 1304.38 L265.903 1304.38 L265.903 1287.98 L266.252 1287.98 L266.252 1304.38 L266.252 1304.38  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.252,1304.38 265.903,1304.38 265.903,1287.98 266.252,1287.98 266.252,1304.38 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.375 1283.88 L265.903 1283.88 L265.903 1267.47 L266.375 1267.47 L266.375 1283.88 L266.375 1283.88  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.375,1283.88 265.903,1283.88 265.903,1267.47 266.375,1267.47 266.375,1283.88 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.288 1263.37 L265.903 1263.37 L265.903 1246.97 L266.288 1246.97 L266.288 1263.37 L266.288 1263.37  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.288,1263.37 265.903,1263.37 265.903,1246.97 266.288,1246.97 266.288,1263.37 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M267.214 1242.87 L265.903 1242.87 L265.903 1226.46 L267.214 1226.46 L267.214 1242.87 L267.214 1242.87  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"267.214,1242.87 265.903,1242.87 265.903,1226.46 267.214,1226.46 267.214,1242.87 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M267.013 1222.36 L265.903 1222.36 L265.903 1205.96 L267.013 1205.96 L267.013 1222.36 L267.013 1222.36  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"267.013,1222.36 265.903,1222.36 265.903,1205.96 267.013,1205.96 267.013,1222.36 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M267.106 1201.86 L265.903 1201.86 L265.903 1185.46 L267.106 1185.46 L267.106 1201.86 L267.106 1201.86  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"267.106,1201.86 265.903,1201.86 265.903,1185.46 267.106,1185.46 267.106,1201.86 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.882 1181.36 L265.903 1181.36 L265.903 1164.95 L266.882 1164.95 L266.882 1181.36 L266.882 1181.36  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.882,1181.36 265.903,1181.36 265.903,1164.95 266.882,1164.95 266.882,1181.36 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M267.934 1160.85 L265.903 1160.85 L265.903 1144.45 L267.934 1144.45 L267.934 1160.85 L267.934 1160.85  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"267.934,1160.85 265.903,1160.85 265.903,1144.45 267.934,1144.45 267.934,1160.85 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.195 1140.35 L265.903 1140.35 L265.903 1123.94 L266.195 1123.94 L266.195 1140.35 L266.195 1140.35  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.195,1140.35 265.903,1140.35 265.903,1123.94 266.195,1123.94 266.195,1140.35 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.403 1119.84 L265.903 1119.84 L265.903 1103.44 L266.403 1103.44 L266.403 1119.84 L266.403 1119.84  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.403,1119.84 265.903,1119.84 265.903,1103.44 266.403,1103.44 266.403,1119.84 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.567 1099.34 L265.903 1099.34 L265.903 1082.94 L266.567 1082.94 L266.567 1099.34 L266.567 1099.34  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.567,1099.34 265.903,1099.34 265.903,1082.94 266.567,1082.94 266.567,1099.34 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.678 1078.84 L265.903 1078.84 L265.903 1062.43 L266.678 1062.43 L266.678 1078.84 L266.678 1078.84  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.678,1078.84 265.903,1078.84 265.903,1062.43 266.678,1062.43 266.678,1078.84 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.247 1058.33 L265.903 1058.33 L265.903 1041.93 L266.247 1041.93 L266.247 1058.33 L266.247 1058.33  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.247,1058.33 265.903,1058.33 265.903,1041.93 266.247,1041.93 266.247,1058.33 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.351 1037.83 L265.903 1037.83 L265.903 1021.42 L266.351 1021.42 L266.351 1037.83 L266.351 1037.83  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.351,1037.83 265.903,1037.83 265.903,1021.42 266.351,1021.42 266.351,1037.83 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.047 1017.32 L265.903 1017.32 L265.903 1000.92 L266.047 1000.92 L266.047 1017.32 L266.047 1017.32  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.047,1017.32 265.903,1017.32 265.903,1000.92 266.047,1000.92 266.047,1017.32 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.636 996.82 L265.903 996.82 L265.903 980.416 L266.636 980.416 L266.636 996.82 L266.636 996.82  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.636,996.82 265.903,996.82 265.903,980.416 266.636,980.416 266.636,996.82 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.97 976.316 L265.903 976.316 L265.903 959.912 L266.97 959.912 L266.97 976.316 L266.97 976.316  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.97,976.316 265.903,976.316 265.903,959.912 266.97,959.912 266.97,976.316 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M265.903 955.812 L265.903 955.812 L265.903 939.408 L265.903 939.408 L265.903 955.812 L265.903 955.812  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"265.903,955.812 265.903,955.812 265.903,939.408 265.903,955.812 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.414 935.308 L265.903 935.308 L265.903 918.904 L266.414 918.904 L266.414 935.308 L266.414 935.308  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.414,935.308 265.903,935.308 265.903,918.904 266.414,918.904 266.414,935.308 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.244 914.804 L265.903 914.804 L265.903 898.4 L266.244 898.4 L266.244 914.804 L266.244 914.804  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.244,914.804 265.903,914.804 265.903,898.4 266.244,898.4 266.244,914.804 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.099 894.3 L265.903 894.3 L265.903 877.896 L266.099 877.896 L266.099 894.3 L266.099 894.3  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.099,894.3 265.903,894.3 265.903,877.896 266.099,877.896 266.099,894.3 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.21 873.796 L265.903 873.796 L265.903 857.392 L266.21 857.392 L266.21 873.796 L266.21 873.796  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.21,873.796 265.903,873.796 265.903,857.392 266.21,857.392 266.21,873.796 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M267.265 853.292 L265.903 853.292 L265.903 836.888 L267.265 836.888 L267.265 853.292 L267.265 853.292  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"267.265,853.292 265.903,853.292 265.903,836.888 267.265,836.888 267.265,853.292 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.451 832.788 L265.903 832.788 L265.903 816.384 L266.451 816.384 L266.451 832.788 L266.451 832.788  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.451,832.788 265.903,832.788 265.903,816.384 266.451,816.384 266.451,832.788 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.56 812.284 L265.903 812.284 L265.903 795.88 L266.56 795.88 L266.56 812.284 L266.56 812.284  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.56,812.284 265.903,812.284 265.903,795.88 266.56,795.88 266.56,812.284 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.11 791.78 L265.903 791.78 L265.903 775.376 L266.11 775.376 L266.11 791.78 L266.11 791.78  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.11,791.78 265.903,791.78 265.903,775.376 266.11,775.376 266.11,791.78 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.803 771.276 L265.903 771.276 L265.903 754.872 L266.803 754.872 L266.803 771.276 L266.803 771.276  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.803,771.276 265.903,771.276 265.903,754.872 266.803,754.872 266.803,771.276 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.013 750.772 L265.903 750.772 L265.903 734.368 L266.013 734.368 L266.013 750.772 L266.013 750.772  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.013,750.772 265.903,750.772 265.903,734.368 266.013,734.368 266.013,750.772 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.373 730.268 L265.903 730.268 L265.903 713.864 L266.373 713.864 L266.373 730.268 L266.373 730.268  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.373,730.268 265.903,730.268 265.903,713.864 266.373,713.864 266.373,730.268 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M265.976 709.764 L265.903 709.764 L265.903 693.36 L265.976 693.36 L265.976 709.764 L265.976 709.764  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"265.976,709.764 265.903,709.764 265.903,693.36 265.976,693.36 265.976,709.764 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M265.997 689.26 L265.903 689.26 L265.903 672.856 L265.997 672.856 L265.997 689.26 L265.997 689.26  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"265.997,689.26 265.903,689.26 265.903,672.856 265.997,672.856 265.997,689.26 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M267.028 668.756 L265.903 668.756 L265.903 652.352 L267.028 652.352 L267.028 668.756 L267.028 668.756  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"267.028,668.756 265.903,668.756 265.903,652.352 267.028,652.352 267.028,668.756 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.378 648.252 L265.903 648.252 L265.903 631.848 L266.378 631.848 L266.378 648.252 L266.378 648.252  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.378,648.252 265.903,648.252 265.903,631.848 266.378,631.848 266.378,648.252 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.111 627.748 L265.903 627.748 L265.903 611.344 L266.111 611.344 L266.111 627.748 L266.111 627.748  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.111,627.748 265.903,627.748 265.903,611.344 266.111,611.344 266.111,627.748 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.285 607.244 L265.903 607.244 L265.903 590.84 L266.285 590.84 L266.285 607.244 L266.285 607.244  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.285,607.244 265.903,607.244 265.903,590.84 266.285,590.84 266.285,607.244 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.149 586.74 L265.903 586.74 L265.903 570.336 L266.149 570.336 L266.149 586.74 L266.149 586.74  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.149,586.74 265.903,586.74 265.903,570.336 266.149,570.336 266.149,586.74 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.044 566.236 L265.903 566.236 L265.903 549.832 L266.044 549.832 L266.044 566.236 L266.044 566.236  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.044,566.236 265.903,566.236 265.903,549.832 266.044,549.832 266.044,566.236 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.63 545.732 L265.903 545.732 L265.903 529.328 L266.63 529.328 L266.63 545.732 L266.63 545.732  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.63,545.732 265.903,545.732 265.903,529.328 266.63,529.328 266.63,545.732 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.596 525.228 L265.903 525.228 L265.903 508.824 L266.596 508.824 L266.596 525.228 L266.596 525.228  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.596,525.228 265.903,525.228 265.903,508.824 266.596,508.824 266.596,525.228 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.411 504.724 L265.903 504.724 L265.903 488.32 L266.411 488.32 L266.411 504.724 L266.411 504.724  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.411,504.724 265.903,504.724 265.903,488.32 266.411,488.32 266.411,504.724 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M265.903 484.22 L265.903 484.22 L265.903 467.816 L265.903 467.816 L265.903 484.22 L265.903 484.22  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"265.903,484.22 265.903,484.22 265.903,467.816 265.903,484.22 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.24 463.716 L265.903 463.716 L265.903 447.312 L266.24 447.312 L266.24 463.716 L266.24 463.716  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.24,463.716 265.903,463.716 265.903,447.312 266.24,447.312 266.24,463.716 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.235 443.212 L265.903 443.212 L265.903 426.808 L266.235 426.808 L266.235 443.212 L266.235 443.212  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.235,443.212 265.903,443.212 265.903,426.808 266.235,426.808 266.235,443.212 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.569 422.708 L265.903 422.708 L265.903 406.304 L266.569 406.304 L266.569 422.708 L266.569 422.708  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.569,422.708 265.903,422.708 265.903,406.304 266.569,406.304 266.569,422.708 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M268.499 402.204 L265.903 402.204 L265.903 385.8 L268.499 385.8 L268.499 402.204 L268.499 402.204  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"268.499,402.204 265.903,402.204 265.903,385.8 268.499,385.8 268.499,402.204 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M265.981 381.7 L265.903 381.7 L265.903 365.296 L265.981 365.296 L265.981 381.7 L265.981 381.7  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"265.981,381.7 265.903,381.7 265.903,365.296 265.981,365.296 265.981,381.7 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.805 361.196 L265.903 361.196 L265.903 344.792 L266.805 344.792 L266.805 361.196 L266.805 361.196  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.805,361.196 265.903,361.196 265.903,344.792 266.805,344.792 266.805,361.196 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.325 340.692 L265.903 340.692 L265.903 324.288 L266.325 324.288 L266.325 340.692 L266.325 340.692  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.325,340.692 265.903,340.692 265.903,324.288 266.325,324.288 266.325,340.692 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.117 320.188 L265.903 320.188 L265.903 303.784 L266.117 303.784 L266.117 320.188 L266.117 320.188  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.117,320.188 265.903,320.188 265.903,303.784 266.117,303.784 266.117,320.188 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M265.986 299.684 L265.903 299.684 L265.903 283.28 L265.986 283.28 L265.986 299.684 L265.986 299.684  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"265.986,299.684 265.903,299.684 265.903,283.28 265.986,283.28 265.986,299.684 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.456 279.18 L265.903 279.18 L265.903 262.776 L266.456 262.776 L266.456 279.18 L266.456 279.18  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.456,279.18 265.903,279.18 265.903,262.776 266.456,262.776 266.456,279.18 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M265.985 258.676 L265.903 258.676 L265.903 242.272 L265.985 242.272 L265.985 258.676 L265.985 258.676  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"265.985,258.676 265.903,258.676 265.903,242.272 265.985,242.272 265.985,258.676 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M267.116 238.172 L265.903 238.172 L265.903 221.768 L267.116 221.768 L267.116 238.172 L267.116 238.172  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"267.116,238.172 265.903,238.172 265.903,221.768 267.116,221.768 267.116,238.172 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.306 217.668 L265.903 217.668 L265.903 201.264 L266.306 201.264 L266.306 217.668 L266.306 217.668  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.306,217.668 265.903,217.668 265.903,201.264 266.306,201.264 266.306,217.668 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.451 197.164 L265.903 197.164 L265.903 180.76 L266.451 180.76 L266.451 197.164 L266.451 197.164  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.451,197.164 265.903,197.164 265.903,180.76 266.451,180.76 266.451,197.164 \"/>\n",
       "<path clip-path=\"url(#clip882)\" d=\"M266.325 176.66 L265.903 176.66 L265.903 160.256 L266.325 160.256 L266.325 176.66 L266.325 176.66  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip882)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.325,176.66 265.903,176.66 265.903,160.256 266.325,160.256 266.325,176.66 \"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"299.671\" cy=\"1398.57\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"333.439\" cy=\"1398.52\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"367.207\" cy=\"1398.6\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"400.974\" cy=\"1397.99\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"434.742\" cy=\"1398.49\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"468.51\" cy=\"1398.41\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"502.278\" cy=\"1398.46\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"536.046\" cy=\"1397.9\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"569.814\" cy=\"1398.02\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"603.582\" cy=\"1397.97\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"637.349\" cy=\"1398.1\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"671.117\" cy=\"1397.46\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"704.885\" cy=\"1398.52\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"738.653\" cy=\"1398.39\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"772.421\" cy=\"1398.29\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"806.189\" cy=\"1398.23\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"839.956\" cy=\"1398.49\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"873.724\" cy=\"1398.43\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"907.492\" cy=\"1398.61\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"941.26\" cy=\"1398.25\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"975.028\" cy=\"1398.05\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1008.8\" cy=\"1398.7\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1042.56\" cy=\"1398.39\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1076.33\" cy=\"1398.49\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1110.1\" cy=\"1398.58\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1143.87\" cy=\"1398.51\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1177.63\" cy=\"1397.87\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1211.4\" cy=\"1398.37\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1245.17\" cy=\"1398.3\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1278.94\" cy=\"1398.57\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1312.71\" cy=\"1398.15\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1346.47\" cy=\"1398.63\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1380.24\" cy=\"1398.41\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1414.01\" cy=\"1398.65\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1447.78\" cy=\"1398.64\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1481.55\" cy=\"1398.01\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1515.31\" cy=\"1398.41\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1549.08\" cy=\"1398.57\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1582.85\" cy=\"1398.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1616.62\" cy=\"1398.55\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1650.38\" cy=\"1398.61\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1684.15\" cy=\"1398.26\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1717.92\" cy=\"1398.28\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1751.69\" cy=\"1398.39\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1785.46\" cy=\"1398.7\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1819.22\" cy=\"1398.49\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1852.99\" cy=\"1398.5\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1886.76\" cy=\"1398.29\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1920.53\" cy=\"1397.12\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1954.3\" cy=\"1398.65\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1988.06\" cy=\"1398.15\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"2021.83\" cy=\"1398.44\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"2055.6\" cy=\"1398.57\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"2089.37\" cy=\"1398.65\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"2123.13\" cy=\"1398.36\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"2156.9\" cy=\"1398.65\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"2190.67\" cy=\"1397.96\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"2224.44\" cy=\"1398.45\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"2258.21\" cy=\"1398.37\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip882)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"2291.97\" cy=\"1398.44\" r=\"2\"/>\n",
       "</svg>\n"
      ],
      "text/html": [
       "<img src=\"data:image/png;base64,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\" />"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract fitted model parameters\n",
    "fitted_model = fitted_params(machines[\"RF\"])\n",
    "\n",
    "# Obtain feature importance values\n",
    "# feature_importances = fitted_model.feature_importance\n",
    "\n",
    "feature_importances = fitted_model.forest.featim\n",
    "\n",
    "# Plot feature importance\n",
    "p = bar(\n",
    "    1:length(feature_importances),\n",
    "    feature_importances,\n",
    "    orientation = :horizontal,\n",
    "    legend = false\n",
    ")\n",
    "xlabel!(p, \"Gini Gain\")\n",
    "ylabel!(p, \"Feature\")\n",
    "title!(p, \"Feature Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4feb1ec",
   "metadata": {},
   "source": [
    "As shown in the plot, most of the predictive information may be concentrated in a small number of features.\n",
    "Therefore, this metric can also be used for feature filtering or selection, which will be discussed in subsequent sections.\n",
    "\n",
    "Random Forests represent one of the most robust and widely used ensemble methods.\n",
    "They leverage bagging and feature randomness to build diverse trees, reducing variance and improving generalisation.\n",
    "Moreover, they naturally provide interpretable measures such as feature importance, making them not only powerful predictors but also useful tools for exploratory data analysis.\n",
    "\n",
    "### XGBoost (eXtreme Gradient Boosting)\n",
    "\n",
    "Finally, in this last section, Gradient Boosting should be mentioned again, specifically an implementation that in recent years has become very famous for its versatility and speed. This implementation is known as ***XGBoost (eXtreme Gradient Boosting)*** , which has stood out especially in competitions such as the Kaggle platform for its speed in obtaining results and robustness. \n",
    "\n",
    "The ***XGBoost*** will be a similar ensemble to Random Forest but uses a different base classifier known as CART (classification and regression trees) instead of *Decision Trees*. This change comes from the need for the algorithm to obtain the probability of the decisions, as was the case with *Gradient Tree Boosting*. The other fundamental change in this algotimo, since it is based on *Gradient Tree Boosting*, is the change from *bagging* to *boosting* strategy for the creation of the classifier training sets.\n",
    "\n",
    "Subsequently, this technique performs an additive training approach whose weights are adjusted based on a **Declining Gradient** on a *loss* function to be defined. By adding the *loss* function with the regularisation term, the second derivative of the functions can be calculated in order to update the classification weights of the different trees. The calculation of this gradient thus allows the adjustment of the values of the classifiers that are generated following a given one in order to allow the weights to focus attention on the patterns that are incorrectly classified. The mathematical details of the implementation can be found in this [link](https://xgboost.readthedocs.io/en/stable/tutorials/model.html).\n",
    "\n",
    "Unlike the other approaches we have seen, the `xgboost` is not currently implemented in `scikit learn` but it is integrated in MLJ. However, the reference version must be installed if it is not already present on the machine which is the one used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a03972c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg;\n",
    "Pkg.add(\"XGBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499e0c0f",
   "metadata": {},
   "source": [
    "After that installation, the library could be used as shown in the following example. Unlike other implementations, the Julia implementation supports Julia Array, SparseMatrixCSC, libSVM format text and XGBoost binary file as input.  Althouugh the vastly options given by Julia libraría in deep to change internaly to the format [LIBSVM](https://xgboost.readthedocs.io/en/stable/tutorials/input_format.html) as any other library. This library has not all the posibilities and, more especificl, the BitVector is not supported nowadays in their function `DMatrix`. So, an small change in the format is required in order to use the library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f7fd5e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "using XGBoost;\n",
    "\n",
    "train_input = input_data\n",
    "train_output = output_data\n",
    "\n",
    "test_input = input_data\n",
    "test_output = output_data\n",
    "\n",
    "train_output_asNumber= Vector{Number}(train_output);\n",
    "\n",
    "@assert train_output_asNumber isa Vector{Number}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad57fb4",
   "metadata": {},
   "source": [
    "Once this data adaptation is done, you can proceed with the training of a model from the `xgboost` library. To do so, it is only necessary to call the function train with the corresponding parameters. Among these parameters, the most important are:\n",
    "\n",
    "- **eta**, term that will determine the compression of the weights after each new stage of *boosting*. It takes values between 0 and 1.\n",
    "- **max_depth**, maximum depth of the trees has by default a value of 6, increasing it will allow more complex models.\n",
    "- **gamma**, parameter that controls the minimum loss reduction necessary to perform a new partition on a leaf node of the tree. The higher it is, the more conservative it will be\n",
    "- **alpha** and **lambda**, are the parameters that control the L1 and L2 regulation respectively.\n",
    "- **objective**, sets the loss function to be used which can be one of the predefined ones, which can be consulted in this [link](https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-tree-booster)\n",
    "\n",
    "Further it is only necessary to set the maximum number of iterations of the boosting process as shown in the following example with 20 rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "eec905d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mXGBoost: starting training.\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39m[19:22:41] WARNING: /workspace/srcdir/xgboost/src/learner.cc:740: \n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mParameters: { \"rounds\" } are not used.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ XGBoost ~/.julia/packages/XGBoost/5SES5/src/XGBoost.jl:34\u001b[39m\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[1]\ttrain-rmse:0.06685667952816027\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[2]\ttrain-rmse:0.03784067051373981\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[3]\ttrain-rmse:0.02057971291063566\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[4]\ttrain-rmse:0.01269548720252575\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[5]\ttrain-rmse:0.00709807877639992\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[6]\ttrain-rmse:0.00428344147805759\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[7]\ttrain-rmse:0.00305694459598080\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[8]\ttrain-rmse:0.00142192344718740\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[9]\ttrain-rmse:0.00098684141249462\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[10]\ttrain-rmse:0.00068744187963404\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining rounds complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Booster()"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_data = DMatrix(train_input, label=train_output_asNumber)\n",
    "\n",
    "model = xgboost(svm_data, rounds=20, eta = 1, max_depth = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c275ed94",
   "metadata": {},
   "source": [
    "In the following piece of code, several parameters as passed as a dictionary and two different metrics are calculated. First, error refers to the incorrectly classified ones over the total amount, and the second one is the Area Under the Curve ROC (AUC).\n",
    "\n",
    "### Question 7.5\n",
    "> ❓ Which is the canonical name of the first measure that is been monitored?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04f5411",
   "metadata": {},
   "source": [
    "Error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "40ab769b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mXGBoost: starting training.\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39m[19:22:41] WARNING: /workspace/srcdir/xgboost/src/learner.cc:740: \n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mParameters: { \"metrics\", \"param\", \"rounds\" } are not used.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ XGBoost ~/.julia/packages/XGBoost/5SES5/src/XGBoost.jl:34\u001b[39m\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[1]\ttrain-rmse:0.36104272805218679\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[2]\ttrain-rmse:0.26349314167054072\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[3]\ttrain-rmse:0.19403141904608778\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[4]\ttrain-rmse:0.14254849879038894\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[5]\ttrain-rmse:0.10912439741860382\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[6]\ttrain-rmse:0.08405248067916692\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[7]\ttrain-rmse:0.06422803732657284\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[8]\ttrain-rmse:0.05041922616002128\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[9]\ttrain-rmse:0.04010784619540812\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[10]\ttrain-rmse:0.03205229098624832\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining rounds complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "208-element Vector{Float32}:\n",
       " 0.01804065\n",
       " 0.031605005\n",
       " 0.028208893\n",
       " 0.01256614\n",
       " 0.08909455\n",
       " 0.016385488\n",
       " 0.048917405\n",
       " 0.073377535\n",
       " 0.03411845\n",
       " 0.014371509\n",
       " 0.010147011\n",
       " 0.02828527\n",
       " 0.013967805\n",
       " ⋮\n",
       " 0.9912259\n",
       " 0.9912259\n",
       " 0.9912259\n",
       " 0.9773253\n",
       " 0.9912259\n",
       " 0.9912259\n",
       " 1.0010829\n",
       " 0.9912259\n",
       " 0.9906951\n",
       " 0.9912259\n",
       " 0.9912259\n",
       " 0.98940575"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param = [\"max_depth\" => 2,\n",
    "         \"eta\" => 1,\n",
    "         \"objective\" => \"binary:logistic\"]\n",
    "metrics = metrics = [\"error\", \"auc\"]\n",
    "model = xgboost(DMatrix(train_input, label=train_output_asNumber), rounds=20, param=param, metrics=metrics)\n",
    "\n",
    "pred = XGBoost.predict(model, train_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6120deba",
   "metadata": {},
   "source": [
    "***Important***.\n",
    "\n",
    "In case a validation set is used, this must be passed in the *evals* parameter of the training function. In addition, and only when the mentioned *evals* parameter is defined, you can set the rounds for the pre-stop with the *early_stopping_rounds* parameter of the training function. The code would be similar to:\n",
    "``` julia\n",
    "    evals = DMatrix(val_input, label=val_output)\n",
    "    xgb_model = xgb.train(param, train_input, num_round,label = train_output_asNumber, evals=evals,\n",
    "                    early_stopping_rounds=10)\n",
    "```\n",
    "\n",
    "The value provided in the output corresponds to the sum of the outputs of the trees, being between 0 and 1 for membership of a given class. Since this is a binary class, simply set a limit of 0.5 to the output to determine what the answer is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a468354c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error of XGboost= 0.0\n"
     ]
    }
   ],
   "source": [
    "using XGBoost: predict as predict_xgb\n",
    "\n",
    "pred = predict_xgb(model, test_input)\n",
    "print(\"Error of XGboost= \", sum((pred .> 0.5) .!= test_output) / float(size(pred)[1]), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569d26f2",
   "metadata": {},
   "source": [
    "Finally, as in the case of the Random Forest it is possible to identify the importance and paint it for each of the variables in the ranking. With the following code it is possible to see such a marker ordered in a ascendent way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "74e86f1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip940\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip940)\" d=\"M0 1600 L2400 1600 L2400 0 L0 0  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip941\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip940)\" d=\"M205.121 1423.18 L2352.76 1423.18 L2352.76 123.472 L205.121 123.472  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip942\">\n",
       "    <rect x=\"205\" y=\"123\" width=\"2149\" height=\"1301\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"265.903,1423.18 265.903,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"603.582,1423.18 603.582,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"941.26,1423.18 941.26,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1278.94,1423.18 1278.94,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1616.62,1423.18 1616.62,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1954.3,1423.18 1954.3,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2291.97,1423.18 2291.97,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,1403.47 2352.76,1403.47 \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,1190.14 2352.76,1190.14 \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,976.803 2352.76,976.803 \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,763.47 2352.76,763.47 \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,550.137 2352.76,550.137 \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,336.805 2352.76,336.805 \"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,123.472 2352.76,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1423.18 2352.76,1423.18 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"265.903,1423.18 265.903,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"603.582,1423.18 603.582,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"941.26,1423.18 941.26,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1278.94,1423.18 1278.94,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1616.62,1423.18 1616.62,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1954.3,1423.18 1954.3,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2291.97,1423.18 2291.97,1404.28 \"/>\n",
       "<path clip-path=\"url(#clip940)\" d=\"M265.903 1454.1 Q262.292 1454.1 260.463 1457.66 Q258.658 1461.2 258.658 1468.33 Q258.658 1475.44 260.463 1479.01 Q262.292 1482.55 265.903 1482.55 Q269.537 1482.55 271.343 1479.01 Q273.172 1475.44 273.172 1468.33 Q273.172 1461.2 271.343 1457.66 Q269.537 1454.1 265.903 1454.1 M265.903 1450.39 Q271.713 1450.39 274.769 1455 Q277.847 1459.58 277.847 1468.33 Q277.847 1477.06 274.769 1481.67 Q271.713 1486.25 265.903 1486.25 Q260.093 1486.25 257.014 1481.67 Q253.959 1477.06 253.959 1468.33 Q253.959 1459.58 257.014 1455 Q260.093 1450.39 265.903 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M578.269 1481.64 L585.908 1481.64 L585.908 1455.28 L577.598 1456.95 L577.598 1452.69 L585.862 1451.02 L590.538 1451.02 L590.538 1481.64 L598.176 1481.64 L598.176 1485.58 L578.269 1485.58 L578.269 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M617.621 1454.1 Q614.01 1454.1 612.181 1457.66 Q610.375 1461.2 610.375 1468.33 Q610.375 1475.44 612.181 1479.01 Q614.01 1482.55 617.621 1482.55 Q621.255 1482.55 623.061 1479.01 Q624.889 1475.44 624.889 1468.33 Q624.889 1461.2 623.061 1457.66 Q621.255 1454.1 617.621 1454.1 M617.621 1450.39 Q623.431 1450.39 626.487 1455 Q629.565 1459.58 629.565 1468.33 Q629.565 1477.06 626.487 1481.67 Q623.431 1486.25 617.621 1486.25 Q611.811 1486.25 608.732 1481.67 Q605.676 1477.06 605.676 1468.33 Q605.676 1459.58 608.732 1455 Q611.811 1450.39 617.621 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M920.033 1481.64 L936.353 1481.64 L936.353 1485.58 L914.408 1485.58 L914.408 1481.64 Q917.07 1478.89 921.654 1474.26 Q926.26 1469.61 927.441 1468.27 Q929.686 1465.74 930.566 1464.01 Q931.468 1462.25 931.468 1460.56 Q931.468 1457.8 929.524 1456.07 Q927.603 1454.33 924.501 1454.33 Q922.302 1454.33 919.848 1455.09 Q917.418 1455.86 914.64 1457.41 L914.64 1452.69 Q917.464 1451.55 919.918 1450.97 Q922.371 1450.39 924.408 1450.39 Q929.779 1450.39 932.973 1453.08 Q936.167 1455.77 936.167 1460.26 Q936.167 1462.39 935.357 1464.31 Q934.57 1466.2 932.464 1468.8 Q931.885 1469.47 928.783 1472.69 Q925.681 1475.88 920.033 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M956.167 1454.1 Q952.556 1454.1 950.728 1457.66 Q948.922 1461.2 948.922 1468.33 Q948.922 1475.44 950.728 1479.01 Q952.556 1482.55 956.167 1482.55 Q959.802 1482.55 961.607 1479.01 Q963.436 1475.44 963.436 1468.33 Q963.436 1461.2 961.607 1457.66 Q959.802 1454.1 956.167 1454.1 M956.167 1450.39 Q961.977 1450.39 965.033 1455 Q968.112 1459.58 968.112 1468.33 Q968.112 1477.06 965.033 1481.67 Q961.977 1486.25 956.167 1486.25 Q950.357 1486.25 947.278 1481.67 Q944.223 1477.06 944.223 1468.33 Q944.223 1459.58 947.278 1455 Q950.357 1450.39 956.167 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1267.78 1466.95 Q1271.14 1467.66 1273.01 1469.93 Q1274.91 1472.2 1274.91 1475.53 Q1274.91 1480.65 1271.39 1483.45 Q1267.87 1486.25 1261.39 1486.25 Q1259.22 1486.25 1256.9 1485.81 Q1254.61 1485.39 1252.16 1484.54 L1252.16 1480.02 Q1254.1 1481.16 1256.42 1481.74 Q1258.73 1482.32 1261.25 1482.32 Q1265.65 1482.32 1267.94 1480.58 Q1270.26 1478.84 1270.26 1475.53 Q1270.26 1472.48 1268.11 1470.77 Q1265.98 1469.03 1262.16 1469.03 L1258.13 1469.03 L1258.13 1465.19 L1262.34 1465.19 Q1265.79 1465.19 1267.62 1463.82 Q1269.45 1462.43 1269.45 1459.84 Q1269.45 1457.18 1267.55 1455.77 Q1265.67 1454.33 1262.16 1454.33 Q1260.23 1454.33 1258.04 1454.75 Q1255.84 1455.16 1253.2 1456.04 L1253.2 1451.88 Q1255.86 1451.14 1258.17 1450.77 Q1260.51 1450.39 1262.57 1450.39 Q1267.9 1450.39 1271 1452.83 Q1274.1 1455.23 1274.1 1459.35 Q1274.1 1462.22 1272.46 1464.21 Q1270.81 1466.18 1267.78 1466.95 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1293.78 1454.1 Q1290.17 1454.1 1288.34 1457.66 Q1286.53 1461.2 1286.53 1468.33 Q1286.53 1475.44 1288.34 1479.01 Q1290.17 1482.55 1293.78 1482.55 Q1297.41 1482.55 1299.22 1479.01 Q1301.04 1475.44 1301.04 1468.33 Q1301.04 1461.2 1299.22 1457.66 Q1297.41 1454.1 1293.78 1454.1 M1293.78 1450.39 Q1299.59 1450.39 1302.64 1455 Q1305.72 1459.58 1305.72 1468.33 Q1305.72 1477.06 1302.64 1481.67 Q1299.59 1486.25 1293.78 1486.25 Q1287.97 1486.25 1284.89 1481.67 Q1281.83 1477.06 1281.83 1468.33 Q1281.83 1459.58 1284.89 1455 Q1287.97 1450.39 1293.78 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1604.79 1455.09 L1592.98 1473.54 L1604.79 1473.54 L1604.79 1455.09 M1603.56 1451.02 L1609.44 1451.02 L1609.44 1473.54 L1614.37 1473.54 L1614.37 1477.43 L1609.44 1477.43 L1609.44 1485.58 L1604.79 1485.58 L1604.79 1477.43 L1589.19 1477.43 L1589.19 1472.92 L1603.56 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1632.1 1454.1 Q1628.49 1454.1 1626.66 1457.66 Q1624.86 1461.2 1624.86 1468.33 Q1624.86 1475.44 1626.66 1479.01 Q1628.49 1482.55 1632.1 1482.55 Q1635.74 1482.55 1637.54 1479.01 Q1639.37 1475.44 1639.37 1468.33 Q1639.37 1461.2 1637.54 1457.66 Q1635.74 1454.1 1632.1 1454.1 M1632.1 1450.39 Q1637.91 1450.39 1640.97 1455 Q1644.05 1459.58 1644.05 1468.33 Q1644.05 1477.06 1640.97 1481.67 Q1637.91 1486.25 1632.1 1486.25 Q1626.29 1486.25 1623.21 1481.67 Q1620.16 1477.06 1620.16 1468.33 Q1620.16 1459.58 1623.21 1455 Q1626.29 1450.39 1632.1 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1928.99 1451.02 L1947.35 1451.02 L1947.35 1454.96 L1933.28 1454.96 L1933.28 1463.43 Q1934.3 1463.08 1935.31 1462.92 Q1936.33 1462.73 1937.35 1462.73 Q1943.14 1462.73 1946.52 1465.9 Q1949.9 1469.08 1949.9 1474.49 Q1949.9 1480.07 1946.43 1483.17 Q1942.95 1486.25 1936.63 1486.25 Q1934.46 1486.25 1932.19 1485.88 Q1929.94 1485.51 1927.54 1484.77 L1927.54 1480.07 Q1929.62 1481.2 1931.84 1481.76 Q1934.06 1482.32 1936.54 1482.32 Q1940.55 1482.32 1942.88 1480.21 Q1945.22 1478.1 1945.22 1474.49 Q1945.22 1470.88 1942.88 1468.77 Q1940.55 1466.67 1936.54 1466.67 Q1934.67 1466.67 1932.79 1467.08 Q1930.94 1467.5 1928.99 1468.38 L1928.99 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1969.11 1454.1 Q1965.5 1454.1 1963.67 1457.66 Q1961.86 1461.2 1961.86 1468.33 Q1961.86 1475.44 1963.67 1479.01 Q1965.5 1482.55 1969.11 1482.55 Q1972.74 1482.55 1974.55 1479.01 Q1976.38 1475.44 1976.38 1468.33 Q1976.38 1461.2 1974.55 1457.66 Q1972.74 1454.1 1969.11 1454.1 M1969.11 1450.39 Q1974.92 1450.39 1977.98 1455 Q1981.05 1459.58 1981.05 1468.33 Q1981.05 1477.06 1977.98 1481.67 Q1974.92 1486.25 1969.11 1486.25 Q1963.3 1486.25 1960.22 1481.67 Q1957.17 1477.06 1957.17 1468.33 Q1957.17 1459.58 1960.22 1455 Q1963.3 1450.39 1969.11 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M2277.38 1466.44 Q2274.23 1466.44 2272.38 1468.59 Q2270.55 1470.74 2270.55 1474.49 Q2270.55 1478.22 2272.38 1480.39 Q2274.23 1482.55 2277.38 1482.55 Q2280.53 1482.55 2282.36 1480.39 Q2284.21 1478.22 2284.21 1474.49 Q2284.21 1470.74 2282.36 1468.59 Q2280.53 1466.44 2277.38 1466.44 M2286.66 1451.78 L2286.66 1456.04 Q2284.9 1455.21 2283.1 1454.77 Q2281.31 1454.33 2279.55 1454.33 Q2274.93 1454.33 2272.47 1457.45 Q2270.04 1460.58 2269.69 1466.9 Q2271.06 1464.89 2273.12 1463.82 Q2275.18 1462.73 2277.66 1462.73 Q2282.87 1462.73 2285.87 1465.9 Q2288.91 1469.05 2288.91 1474.49 Q2288.91 1479.82 2285.76 1483.03 Q2282.61 1486.25 2277.38 1486.25 Q2271.38 1486.25 2268.21 1481.67 Q2265.04 1477.06 2265.04 1468.33 Q2265.04 1460.14 2268.93 1455.28 Q2272.82 1450.39 2279.37 1450.39 Q2281.13 1450.39 2282.91 1450.74 Q2284.72 1451.09 2286.66 1451.78 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M2306.96 1454.1 Q2303.35 1454.1 2301.52 1457.66 Q2299.72 1461.2 2299.72 1468.33 Q2299.72 1475.44 2301.52 1479.01 Q2303.35 1482.55 2306.96 1482.55 Q2310.6 1482.55 2312.4 1479.01 Q2314.23 1475.44 2314.23 1468.33 Q2314.23 1461.2 2312.4 1457.66 Q2310.6 1454.1 2306.96 1454.1 M2306.96 1450.39 Q2312.77 1450.39 2315.83 1455 Q2318.91 1459.58 2318.91 1468.33 Q2318.91 1477.06 2315.83 1481.67 Q2312.77 1486.25 2306.96 1486.25 Q2301.15 1486.25 2298.07 1481.67 Q2295.02 1477.06 2295.02 1468.33 Q2295.02 1459.58 2298.07 1455 Q2301.15 1450.39 2306.96 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1243.74 1561.26 L1243.74 1548.5 L1233.23 1548.5 L1233.23 1543.22 L1250.1 1543.22 L1250.1 1563.62 Q1246.38 1566.26 1241.89 1567.63 Q1237.4 1568.97 1232.31 1568.97 Q1221.17 1568.97 1214.87 1562.47 Q1208.6 1555.95 1208.6 1544.33 Q1208.6 1532.68 1214.87 1526.19 Q1221.17 1519.66 1232.31 1519.66 Q1236.96 1519.66 1241.13 1520.81 Q1245.33 1521.96 1248.86 1524.18 L1248.86 1531.03 Q1245.3 1528 1241.29 1526.48 Q1237.27 1524.95 1232.85 1524.95 Q1224.13 1524.95 1219.74 1529.82 Q1215.38 1534.69 1215.38 1544.33 Q1215.38 1553.94 1219.74 1558.81 Q1224.13 1563.68 1232.85 1563.68 Q1236.26 1563.68 1238.93 1563.11 Q1241.6 1562.51 1243.74 1561.26 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1277.79 1550.12 Q1270.69 1550.12 1267.96 1551.75 Q1265.22 1553.37 1265.22 1557.29 Q1265.22 1560.4 1267.26 1562.25 Q1269.33 1564.07 1272.86 1564.07 Q1277.73 1564.07 1280.66 1560.63 Q1283.62 1557.16 1283.62 1551.43 L1283.62 1550.12 L1277.79 1550.12 M1289.47 1547.71 L1289.47 1568.04 L1283.62 1568.04 L1283.62 1562.63 Q1281.61 1565.88 1278.62 1567.44 Q1275.63 1568.97 1271.3 1568.97 Q1265.83 1568.97 1262.58 1565.91 Q1259.36 1562.82 1259.36 1557.67 Q1259.36 1551.65 1263.37 1548.6 Q1267.42 1545.54 1275.41 1545.54 L1283.62 1545.54 L1283.62 1544.97 Q1283.62 1540.93 1280.94 1538.73 Q1278.3 1536.5 1273.5 1536.5 Q1270.44 1536.5 1267.54 1537.23 Q1264.65 1537.97 1261.97 1539.43 L1261.97 1534.02 Q1265.19 1532.78 1268.21 1532.17 Q1271.24 1531.54 1274.1 1531.54 Q1281.83 1531.54 1285.65 1535.55 Q1289.47 1539.56 1289.47 1547.71 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1301.54 1532.4 L1307.39 1532.4 L1307.39 1568.04 L1301.54 1568.04 L1301.54 1532.4 M1301.54 1518.52 L1307.39 1518.52 L1307.39 1525.93 L1301.54 1525.93 L1301.54 1518.52 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1349.28 1546.53 L1349.28 1568.04 L1343.42 1568.04 L1343.42 1546.72 Q1343.42 1541.66 1341.45 1539.14 Q1339.48 1536.63 1335.53 1536.63 Q1330.79 1536.63 1328.05 1539.65 Q1325.31 1542.68 1325.31 1547.9 L1325.31 1568.04 L1319.42 1568.04 L1319.42 1532.4 L1325.31 1532.4 L1325.31 1537.93 Q1327.41 1534.72 1330.25 1533.13 Q1333.11 1531.54 1336.83 1531.54 Q1342.98 1531.54 1346.13 1535.36 Q1349.28 1539.14 1349.28 1546.53 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1423.18 205.121,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1403.47 224.019,1403.47 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1190.14 224.019,1190.14 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,976.803 224.019,976.803 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,763.47 224.019,763.47 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,550.137 224.019,550.137 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,336.805 224.019,336.805 \"/>\n",
       "<polyline clip-path=\"url(#clip940)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,123.472 224.019,123.472 \"/>\n",
       "<path clip-path=\"url(#clip940)\" d=\"M157.177 1389.27 Q153.566 1389.27 151.737 1392.83 Q149.931 1396.37 149.931 1403.5 Q149.931 1410.61 151.737 1414.17 Q153.566 1417.72 157.177 1417.72 Q160.811 1417.72 162.616 1414.17 Q164.445 1410.61 164.445 1403.5 Q164.445 1396.37 162.616 1392.83 Q160.811 1389.27 157.177 1389.27 M157.177 1385.56 Q162.987 1385.56 166.042 1390.17 Q169.121 1394.75 169.121 1403.5 Q169.121 1412.23 166.042 1416.84 Q162.987 1421.42 157.177 1421.42 Q151.366 1421.42 148.288 1416.84 Q145.232 1412.23 145.232 1403.5 Q145.232 1394.75 148.288 1390.17 Q151.366 1385.56 157.177 1385.56 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M117.825 1203.48 L125.464 1203.48 L125.464 1177.11 L117.154 1178.78 L117.154 1174.52 L125.418 1172.86 L130.093 1172.86 L130.093 1203.48 L137.732 1203.48 L137.732 1207.42 L117.825 1207.42 L117.825 1203.48 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M157.177 1175.93 Q153.566 1175.93 151.737 1179.5 Q149.931 1183.04 149.931 1190.17 Q149.931 1197.28 151.737 1200.84 Q153.566 1204.38 157.177 1204.38 Q160.811 1204.38 162.616 1200.84 Q164.445 1197.28 164.445 1190.17 Q164.445 1183.04 162.616 1179.5 Q160.811 1175.93 157.177 1175.93 M157.177 1172.23 Q162.987 1172.23 166.042 1176.84 Q169.121 1181.42 169.121 1190.17 Q169.121 1198.9 166.042 1203.5 Q162.987 1208.09 157.177 1208.09 Q151.366 1208.09 148.288 1203.5 Q145.232 1198.9 145.232 1190.17 Q145.232 1181.42 148.288 1176.84 Q151.366 1172.23 157.177 1172.23 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M121.043 990.147 L137.362 990.147 L137.362 994.083 L115.418 994.083 L115.418 990.147 Q118.08 987.393 122.663 982.763 Q127.269 978.111 128.45 976.768 Q130.695 974.245 131.575 972.509 Q132.478 970.749 132.478 969.06 Q132.478 966.305 130.533 964.569 Q128.612 962.833 125.51 962.833 Q123.311 962.833 120.857 963.597 Q118.427 964.361 115.649 965.912 L115.649 961.189 Q118.473 960.055 120.927 959.476 Q123.38 958.898 125.418 958.898 Q130.788 958.898 133.982 961.583 Q137.177 964.268 137.177 968.759 Q137.177 970.888 136.367 972.81 Q135.579 974.708 133.473 977.3 Q132.894 977.972 129.792 981.189 Q126.691 984.384 121.043 990.147 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M157.177 962.601 Q153.566 962.601 151.737 966.166 Q149.931 969.708 149.931 976.837 Q149.931 983.944 151.737 987.509 Q153.566 991.05 157.177 991.05 Q160.811 991.05 162.616 987.509 Q164.445 983.944 164.445 976.837 Q164.445 969.708 162.616 966.166 Q160.811 962.601 157.177 962.601 M157.177 958.898 Q162.987 958.898 166.042 963.504 Q169.121 968.087 169.121 976.837 Q169.121 985.564 166.042 990.171 Q162.987 994.754 157.177 994.754 Q151.366 994.754 148.288 990.171 Q145.232 985.564 145.232 976.837 Q145.232 968.087 148.288 963.504 Q151.366 958.898 157.177 958.898 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M131.181 762.116 Q134.538 762.833 136.413 765.102 Q138.311 767.37 138.311 770.704 Q138.311 775.819 134.792 778.62 Q131.274 781.421 124.793 781.421 Q122.617 781.421 120.302 780.981 Q118.01 780.565 115.556 779.708 L115.556 775.194 Q117.501 776.329 119.816 776.907 Q122.13 777.486 124.654 777.486 Q129.052 777.486 131.343 775.75 Q133.658 774.014 133.658 770.704 Q133.658 767.648 131.505 765.935 Q129.376 764.199 125.556 764.199 L121.529 764.199 L121.529 760.357 L125.742 760.357 Q129.191 760.357 131.019 758.991 Q132.848 757.602 132.848 755.009 Q132.848 752.347 130.95 750.935 Q129.075 749.5 125.556 749.5 Q123.635 749.5 121.436 749.917 Q119.237 750.334 116.598 751.213 L116.598 747.046 Q119.26 746.306 121.575 745.935 Q123.913 745.565 125.973 745.565 Q131.297 745.565 134.399 747.996 Q137.501 750.403 137.501 754.523 Q137.501 757.394 135.857 759.384 Q134.214 761.352 131.181 762.116 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M157.177 749.269 Q153.566 749.269 151.737 752.833 Q149.931 756.375 149.931 763.505 Q149.931 770.611 151.737 774.176 Q153.566 777.718 157.177 777.718 Q160.811 777.718 162.616 774.176 Q164.445 770.611 164.445 763.505 Q164.445 756.375 162.616 752.833 Q160.811 749.269 157.177 749.269 M157.177 745.565 Q162.987 745.565 166.042 750.171 Q169.121 754.755 169.121 763.505 Q169.121 772.232 166.042 776.838 Q162.987 781.421 157.177 781.421 Q151.366 781.421 148.288 776.838 Q145.232 772.232 145.232 763.505 Q145.232 754.755 148.288 750.171 Q151.366 745.565 157.177 745.565 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M129.862 536.931 L118.056 555.38 L129.862 555.38 L129.862 536.931 M128.635 532.857 L134.515 532.857 L134.515 555.38 L139.445 555.38 L139.445 559.269 L134.515 559.269 L134.515 567.417 L129.862 567.417 L129.862 559.269 L114.26 559.269 L114.26 554.755 L128.635 532.857 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M157.177 535.936 Q153.566 535.936 151.737 539.501 Q149.931 543.043 149.931 550.172 Q149.931 557.279 151.737 560.843 Q153.566 564.385 157.177 564.385 Q160.811 564.385 162.616 560.843 Q164.445 557.279 164.445 550.172 Q164.445 543.043 162.616 539.501 Q160.811 535.936 157.177 535.936 M157.177 532.232 Q162.987 532.232 166.042 536.839 Q169.121 541.422 169.121 550.172 Q169.121 558.899 166.042 563.505 Q162.987 568.089 157.177 568.089 Q151.366 568.089 148.288 563.505 Q145.232 558.899 145.232 550.172 Q145.232 541.422 148.288 536.839 Q151.366 532.232 157.177 532.232 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M117.061 319.525 L135.417 319.525 L135.417 323.46 L121.343 323.46 L121.343 331.932 Q122.362 331.585 123.38 331.423 Q124.399 331.238 125.418 331.238 Q131.205 331.238 134.584 334.409 Q137.964 337.58 137.964 342.997 Q137.964 348.576 134.492 351.677 Q131.019 354.756 124.7 354.756 Q122.524 354.756 120.255 354.386 Q118.01 354.015 115.603 353.275 L115.603 348.576 Q117.686 349.71 119.908 350.265 Q122.13 350.821 124.607 350.821 Q128.612 350.821 130.95 348.714 Q133.288 346.608 133.288 342.997 Q133.288 339.386 130.95 337.279 Q128.612 335.173 124.607 335.173 Q122.732 335.173 120.857 335.589 Q119.006 336.006 117.061 336.886 L117.061 319.525 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M157.177 322.603 Q153.566 322.603 151.737 326.168 Q149.931 329.71 149.931 336.839 Q149.931 343.946 151.737 347.511 Q153.566 351.052 157.177 351.052 Q160.811 351.052 162.616 347.511 Q164.445 343.946 164.445 336.839 Q164.445 329.71 162.616 326.168 Q160.811 322.603 157.177 322.603 M157.177 318.9 Q162.987 318.9 166.042 323.506 Q169.121 328.09 169.121 336.839 Q169.121 345.566 166.042 350.173 Q162.987 354.756 157.177 354.756 Q151.366 354.756 148.288 350.173 Q145.232 345.566 145.232 336.839 Q145.232 328.09 148.288 323.506 Q151.366 318.9 157.177 318.9 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M127.593 121.609 Q124.445 121.609 122.593 123.761 Q120.765 125.914 120.765 129.664 Q120.765 133.391 122.593 135.567 Q124.445 137.72 127.593 137.72 Q130.742 137.72 132.57 135.567 Q134.422 133.391 134.422 129.664 Q134.422 125.914 132.57 123.761 Q130.742 121.609 127.593 121.609 M136.876 106.956 L136.876 111.215 Q135.117 110.382 133.311 109.942 Q131.529 109.502 129.769 109.502 Q125.14 109.502 122.686 112.627 Q120.255 115.752 119.908 122.072 Q121.274 120.058 123.334 118.993 Q125.394 117.905 127.871 117.905 Q133.08 117.905 136.089 121.076 Q139.121 124.224 139.121 129.664 Q139.121 134.988 135.973 138.206 Q132.825 141.423 127.593 141.423 Q121.598 141.423 118.427 136.84 Q115.256 132.234 115.256 123.507 Q115.256 115.312 119.144 110.451 Q123.033 105.567 129.584 105.567 Q131.343 105.567 133.126 105.914 Q134.931 106.262 136.876 106.956 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M157.177 109.271 Q153.566 109.271 151.737 112.836 Q149.931 116.377 149.931 123.507 Q149.931 130.613 151.737 134.178 Q153.566 137.72 157.177 137.72 Q160.811 137.72 162.616 134.178 Q164.445 130.613 164.445 123.507 Q164.445 116.377 162.616 112.836 Q160.811 109.271 157.177 109.271 M157.177 105.567 Q162.987 105.567 166.042 110.174 Q169.121 114.757 169.121 123.507 Q169.121 132.234 166.042 136.84 Q162.987 141.423 157.177 141.423 Q151.366 141.423 148.288 136.84 Q145.232 132.234 145.232 123.507 Q145.232 114.757 148.288 110.174 Q151.366 105.567 157.177 105.567 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M16.4842 891.553 L16.4842 864.244 L21.895 864.244 L21.895 885.124 L35.8996 885.124 L35.8996 866.281 L41.3104 866.281 L41.3104 885.124 L64.0042 885.124 L64.0042 891.553 L16.4842 891.553 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M44.7161 827.387 L47.5806 827.387 L47.5806 854.314 Q53.6281 853.932 56.8109 850.685 Q59.9619 847.407 59.9619 841.582 Q59.9619 838.208 59.1344 835.057 Q58.3069 831.875 56.6518 828.755 L62.1899 828.755 Q63.5267 831.906 64.227 835.217 Q64.9272 838.527 64.9272 841.932 Q64.9272 850.462 59.9619 855.46 Q54.9967 860.425 46.5303 860.425 Q37.7774 860.425 32.6531 855.714 Q27.4968 850.972 27.4968 842.951 Q27.4968 835.758 32.1438 831.588 Q36.7589 827.387 44.7161 827.387 M42.9973 833.243 Q38.1912 833.307 35.3266 835.949 Q32.4621 838.559 32.4621 842.887 Q32.4621 847.789 35.2312 850.749 Q38.0002 853.677 43.0292 854.123 L42.9973 833.243 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M46.0847 801.574 Q46.0847 808.672 47.7079 811.409 Q49.3312 814.146 53.2461 814.146 Q56.3653 814.146 58.2114 812.109 Q60.0256 810.04 60.0256 806.507 Q60.0256 801.637 56.5881 798.709 Q53.1188 795.749 47.3897 795.749 L46.0847 795.749 L46.0847 801.574 M43.6657 789.893 L64.0042 789.893 L64.0042 795.749 L58.5933 795.749 Q61.8398 797.754 63.3994 800.746 Q64.9272 803.738 64.9272 808.067 Q64.9272 813.541 61.8716 816.788 Q58.7843 820.003 53.6281 820.003 Q47.6125 820.003 44.5569 815.992 Q41.5014 811.95 41.5014 803.961 L41.5014 795.749 L40.9285 795.749 Q36.8862 795.749 34.6901 798.423 Q32.4621 801.065 32.4621 805.871 Q32.4621 808.926 33.1941 811.823 Q33.9262 814.719 35.3903 817.393 L29.9795 817.393 Q28.7381 814.178 28.1334 811.154 Q27.4968 808.13 27.4968 805.266 Q27.4968 797.532 31.5072 793.712 Q35.5176 789.893 43.6657 789.893 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M18.2347 772.037 L28.3562 772.037 L28.3562 759.974 L32.9077 759.974 L32.9077 772.037 L52.2594 772.037 Q56.6199 772.037 57.8613 770.859 Q59.1026 769.65 59.1026 765.99 L59.1026 759.974 L64.0042 759.974 L64.0042 765.99 Q64.0042 772.769 61.4897 775.347 Q58.9434 777.925 52.2594 777.925 L32.9077 777.925 L32.9077 782.222 L28.3562 782.222 L28.3562 777.925 L18.2347 777.925 L18.2347 772.037 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M49.9359 752.876 L28.3562 752.876 L28.3562 747.02 L49.7131 747.02 Q54.7739 747.02 57.3202 745.046 Q59.8346 743.073 59.8346 739.126 Q59.8346 734.384 56.8109 731.647 Q53.7872 728.877 48.5673 728.877 L28.3562 728.877 L28.3562 723.021 L64.0042 723.021 L64.0042 728.877 L58.5296 728.877 Q61.7762 731.01 63.3676 733.843 Q64.9272 736.644 64.9272 740.368 Q64.9272 746.51 61.1078 749.693 Q57.2883 752.876 49.9359 752.876 M27.4968 738.14 L27.4968 738.14 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M33.8307 690.301 Q33.2578 691.288 33.0032 692.466 Q32.7167 693.611 32.7167 695.012 Q32.7167 699.977 35.9632 702.651 Q39.1779 705.292 45.2253 705.292 L64.0042 705.292 L64.0042 711.181 L28.3562 711.181 L28.3562 705.292 L33.8944 705.292 Q30.6479 703.446 29.0883 700.486 Q27.4968 697.526 27.4968 693.293 Q27.4968 692.688 27.5923 691.956 Q27.656 691.224 27.8151 690.333 L33.8307 690.301 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M44.7161 655.099 L47.5806 655.099 L47.5806 682.026 Q53.6281 681.644 56.8109 678.397 Q59.9619 675.119 59.9619 669.294 Q59.9619 665.921 59.1344 662.77 Q58.3069 659.587 56.6518 656.468 L62.1899 656.468 Q63.5267 659.619 64.227 662.929 Q64.9272 666.239 64.9272 669.645 Q64.9272 678.175 59.9619 683.172 Q54.9967 688.137 46.5303 688.137 Q37.7774 688.137 32.6531 683.426 Q27.4968 678.684 27.4968 670.663 Q27.4968 663.47 32.1438 659.3 Q36.7589 655.099 44.7161 655.099 M42.9973 660.955 Q38.1912 661.019 35.3266 663.661 Q32.4621 666.271 32.4621 670.599 Q32.4621 675.501 35.2312 678.461 Q38.0002 681.389 43.0292 681.835 L42.9973 660.955 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M877.575 12.096 L912.332 12.096 L912.332 18.9825 L885.758 18.9825 L885.758 36.8065 L909.739 36.8065 L909.739 43.6931 L885.758 43.6931 L885.758 72.576 L877.575 72.576 L877.575 12.096 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M959.241 48.0275 L959.241 51.6733 L924.97 51.6733 Q925.457 59.3701 929.588 63.421 Q933.761 67.4314 941.174 67.4314 Q945.468 67.4314 949.478 66.3781 Q953.529 65.3249 957.499 63.2184 L957.499 70.267 Q953.489 71.9684 949.276 72.8596 Q945.063 73.7508 940.728 73.7508 Q929.872 73.7508 923.512 67.4314 Q917.193 61.1119 917.193 50.3365 Q917.193 39.1965 923.188 32.6746 Q929.224 26.1121 939.432 26.1121 Q948.587 26.1121 953.894 32.0264 Q959.241 37.9003 959.241 48.0275 M951.787 45.84 Q951.706 39.7232 948.344 36.0774 Q945.022 32.4315 939.513 32.4315 Q933.275 32.4315 929.507 35.9558 Q925.781 39.4801 925.213 45.8805 L951.787 45.84 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M992.094 49.7694 Q983.06 49.7694 979.577 51.8354 Q976.093 53.9013 976.093 58.8839 Q976.093 62.8538 978.685 65.2034 Q981.319 67.5124 985.815 67.5124 Q992.013 67.5124 995.74 63.1374 Q999.507 58.7219 999.507 51.4303 L999.507 49.7694 L992.094 49.7694 M1006.96 46.6907 L1006.96 72.576 L999.507 72.576 L999.507 65.6895 Q996.955 69.8214 993.147 71.8063 Q989.339 73.7508 983.83 73.7508 Q976.863 73.7508 972.731 69.8619 Q968.639 65.9325 968.639 59.3701 Q968.639 51.7138 973.743 47.825 Q978.888 43.9361 989.056 43.9361 L999.507 43.9361 L999.507 43.2069 Q999.507 38.0623 996.104 35.2672 Q992.742 32.4315 986.625 32.4315 Q982.736 32.4315 979.05 33.3632 Q975.364 34.295 971.961 36.1584 L971.961 29.2718 Q976.052 27.692 979.901 26.9223 Q983.749 26.1121 987.395 26.1121 Q997.239 26.1121 1002.1 31.2163 Q1006.96 36.3204 1006.96 46.6907 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1029.69 14.324 L1029.69 27.2059 L1045.04 27.2059 L1045.04 32.9987 L1029.69 32.9987 L1029.69 57.6282 Q1029.69 63.1779 1031.19 64.7578 Q1032.72 66.3376 1037.38 66.3376 L1045.04 66.3376 L1045.04 72.576 L1037.38 72.576 Q1028.75 72.576 1025.47 69.3758 Q1022.19 66.1351 1022.19 57.6282 L1022.19 32.9987 L1016.72 32.9987 L1016.72 27.2059 L1022.19 27.2059 L1022.19 14.324 L1029.69 14.324 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1054.07 54.671 L1054.07 27.2059 L1061.53 27.2059 L1061.53 54.3874 Q1061.53 60.8284 1064.04 64.0691 Q1066.55 67.2693 1071.57 67.2693 Q1077.61 67.2693 1081.09 63.421 Q1084.62 59.5726 1084.62 52.9291 L1084.62 27.2059 L1092.07 27.2059 L1092.07 72.576 L1084.62 72.576 L1084.62 65.6084 Q1081.9 69.7404 1078.3 71.7658 Q1074.73 73.7508 1069.99 73.7508 Q1062.17 73.7508 1058.12 68.8897 Q1054.07 64.0286 1054.07 54.671 M1072.83 26.1121 L1072.83 26.1121 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1133.71 34.1734 Q1132.46 33.4443 1130.96 33.1202 Q1129.5 32.7556 1127.72 32.7556 Q1121.4 32.7556 1118 36.8875 Q1114.63 40.9789 1114.63 48.6757 L1114.63 72.576 L1107.14 72.576 L1107.14 27.2059 L1114.63 27.2059 L1114.63 34.2544 Q1116.98 30.1225 1120.75 28.1376 Q1124.52 26.1121 1129.91 26.1121 Q1130.68 26.1121 1131.61 26.2337 Q1132.54 26.3147 1133.67 26.5172 L1133.71 34.1734 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1178.52 48.0275 L1178.52 51.6733 L1144.25 51.6733 Q1144.73 59.3701 1148.86 63.421 Q1153.04 67.4314 1160.45 67.4314 Q1164.74 67.4314 1168.75 66.3781 Q1172.8 65.3249 1176.77 63.2184 L1176.77 70.267 Q1172.76 71.9684 1168.55 72.8596 Q1164.34 73.7508 1160 73.7508 Q1149.15 73.7508 1142.79 67.4314 Q1136.47 61.1119 1136.47 50.3365 Q1136.47 39.1965 1142.46 32.6746 Q1148.5 26.1121 1158.71 26.1121 Q1167.86 26.1121 1173.17 32.0264 Q1178.52 37.9003 1178.52 48.0275 M1171.06 45.84 Q1170.98 39.7232 1167.62 36.0774 Q1164.3 32.4315 1158.79 32.4315 Q1152.55 32.4315 1148.78 35.9558 Q1145.06 39.4801 1144.49 45.8805 L1171.06 45.84 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1217.45 12.096 L1225.63 12.096 L1225.63 72.576 L1217.45 72.576 L1217.45 12.096 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1276.91 35.9153 Q1279.71 30.8922 1283.6 28.5022 Q1287.49 26.1121 1292.75 26.1121 Q1299.84 26.1121 1303.69 31.0947 Q1307.54 36.0368 1307.54 45.1919 L1307.54 72.576 L1300.04 72.576 L1300.04 45.4349 Q1300.04 38.913 1297.73 35.7533 Q1295.43 32.5936 1290.69 32.5936 Q1284.89 32.5936 1281.53 36.4419 Q1278.17 40.2903 1278.17 46.9338 L1278.17 72.576 L1270.67 72.576 L1270.67 45.4349 Q1270.67 38.8725 1268.37 35.7533 Q1266.06 32.5936 1261.24 32.5936 Q1255.52 32.5936 1252.16 36.4824 Q1248.8 40.3308 1248.8 46.9338 L1248.8 72.576 L1241.31 72.576 L1241.31 27.2059 L1248.8 27.2059 L1248.8 34.2544 Q1251.35 30.082 1254.92 28.0971 Q1258.48 26.1121 1263.38 26.1121 Q1268.33 26.1121 1271.77 28.6237 Q1275.25 31.1352 1276.91 35.9153 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1329.62 65.7705 L1329.62 89.8329 L1322.12 89.8329 L1322.12 27.2059 L1329.62 27.2059 L1329.62 34.0924 Q1331.96 30.0415 1335.53 28.0971 Q1339.13 26.1121 1344.12 26.1121 Q1352.38 26.1121 1357.53 32.6746 Q1362.71 39.2371 1362.71 49.9314 Q1362.71 60.6258 1357.53 67.1883 Q1352.38 73.7508 1344.12 73.7508 Q1339.13 73.7508 1335.53 71.8063 Q1331.96 69.8214 1329.62 65.7705 M1354.97 49.9314 Q1354.97 41.7081 1351.57 37.0496 Q1348.21 32.3505 1342.29 32.3505 Q1336.38 32.3505 1332.98 37.0496 Q1329.62 41.7081 1329.62 49.9314 Q1329.62 58.1548 1332.98 62.8538 Q1336.38 67.5124 1342.29 67.5124 Q1348.21 67.5124 1351.57 62.8538 Q1354.97 58.1548 1354.97 49.9314 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1392.65 32.4315 Q1386.65 32.4315 1383.17 37.1306 Q1379.68 41.7891 1379.68 49.9314 Q1379.68 58.0738 1383.13 62.7728 Q1386.61 67.4314 1392.65 67.4314 Q1398.6 67.4314 1402.09 62.7323 Q1405.57 58.0333 1405.57 49.9314 Q1405.57 41.8701 1402.09 37.1711 Q1398.6 32.4315 1392.65 32.4315 M1392.65 26.1121 Q1402.37 26.1121 1407.92 32.4315 Q1413.47 38.7509 1413.47 49.9314 Q1413.47 61.0714 1407.92 67.4314 Q1402.37 73.7508 1392.65 73.7508 Q1382.88 73.7508 1377.33 67.4314 Q1371.83 61.0714 1371.83 49.9314 Q1371.83 38.7509 1377.33 32.4315 Q1382.88 26.1121 1392.65 26.1121 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1452.11 34.1734 Q1450.86 33.4443 1449.36 33.1202 Q1447.9 32.7556 1446.12 32.7556 Q1439.8 32.7556 1436.4 36.8875 Q1433.03 40.9789 1433.03 48.6757 L1433.03 72.576 L1425.54 72.576 L1425.54 27.2059 L1433.03 27.2059 L1433.03 34.2544 Q1435.38 30.1225 1439.15 28.1376 Q1442.92 26.1121 1448.31 26.1121 Q1449.08 26.1121 1450.01 26.2337 Q1450.94 26.3147 1452.07 26.5172 L1452.11 34.1734 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1467.31 14.324 L1467.31 27.2059 L1482.66 27.2059 L1482.66 32.9987 L1467.31 32.9987 L1467.31 57.6282 Q1467.31 63.1779 1468.8 64.7578 Q1470.34 66.3376 1475 66.3376 L1482.66 66.3376 L1482.66 72.576 L1475 72.576 Q1466.37 72.576 1463.09 69.3758 Q1459.81 66.1351 1459.81 57.6282 L1459.81 32.9987 L1454.34 32.9987 L1454.34 27.2059 L1459.81 27.2059 L1459.81 14.324 L1467.31 14.324 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1513.08 49.7694 Q1504.05 49.7694 1500.56 51.8354 Q1497.08 53.9013 1497.08 58.8839 Q1497.08 62.8538 1499.67 65.2034 Q1502.31 67.5124 1506.8 67.5124 Q1513 67.5124 1516.73 63.1374 Q1520.49 58.7219 1520.49 51.4303 L1520.49 49.7694 L1513.08 49.7694 M1527.95 46.6907 L1527.95 72.576 L1520.49 72.576 L1520.49 65.6895 Q1517.94 69.8214 1514.13 71.8063 Q1510.33 73.7508 1504.82 73.7508 Q1497.85 73.7508 1493.72 69.8619 Q1489.63 65.9325 1489.63 59.3701 Q1489.63 51.7138 1494.73 47.825 Q1499.87 43.9361 1510.04 43.9361 L1520.49 43.9361 L1520.49 43.2069 Q1520.49 38.0623 1517.09 35.2672 Q1513.73 32.4315 1507.61 32.4315 Q1503.72 32.4315 1500.04 33.3632 Q1496.35 34.295 1492.95 36.1584 L1492.95 29.2718 Q1497.04 27.692 1500.89 26.9223 Q1504.74 26.1121 1508.38 26.1121 Q1518.23 26.1121 1523.09 31.2163 Q1527.95 36.3204 1527.95 46.6907 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1581.01 45.1919 L1581.01 72.576 L1573.56 72.576 L1573.56 45.4349 Q1573.56 38.994 1571.05 35.7938 Q1568.54 32.5936 1563.51 32.5936 Q1557.48 32.5936 1553.99 36.4419 Q1550.51 40.2903 1550.51 46.9338 L1550.51 72.576 L1543.02 72.576 L1543.02 27.2059 L1550.51 27.2059 L1550.51 34.2544 Q1553.18 30.163 1556.79 28.1376 Q1560.44 26.1121 1565.18 26.1121 Q1572.99 26.1121 1577 30.9732 Q1581.01 35.7938 1581.01 45.1919 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1628.53 28.9478 L1628.53 35.9153 Q1625.37 34.1734 1622.17 33.3227 Q1619.01 32.4315 1615.77 32.4315 Q1608.52 32.4315 1604.51 37.0496 Q1600.5 41.6271 1600.5 49.9314 Q1600.5 58.2358 1604.51 62.8538 Q1608.52 67.4314 1615.77 67.4314 Q1619.01 67.4314 1622.17 66.5807 Q1625.37 65.6895 1628.53 63.9476 L1628.53 70.8341 Q1625.41 72.2924 1622.05 73.0216 Q1618.73 73.7508 1614.96 73.7508 Q1604.71 73.7508 1598.68 67.3098 Q1592.64 60.8689 1592.64 49.9314 Q1592.64 38.832 1598.72 32.472 Q1604.83 26.1121 1615.45 26.1121 Q1618.89 26.1121 1622.17 26.8413 Q1625.45 27.5299 1628.53 28.9478 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip940)\" d=\"M1680.3 48.0275 L1680.3 51.6733 L1646.03 51.6733 Q1646.52 59.3701 1650.65 63.421 Q1654.82 67.4314 1662.23 67.4314 Q1666.53 67.4314 1670.54 66.3781 Q1674.59 65.3249 1678.56 63.2184 L1678.56 70.267 Q1674.55 71.9684 1670.34 72.8596 Q1666.12 73.7508 1661.79 73.7508 Q1650.93 73.7508 1644.57 67.4314 Q1638.25 61.1119 1638.25 50.3365 Q1638.25 39.1965 1644.25 32.6746 Q1650.28 26.1121 1660.49 26.1121 Q1669.65 26.1121 1674.95 32.0264 Q1680.3 37.9003 1680.3 48.0275 M1672.85 45.84 Q1672.77 39.7232 1669.41 36.0774 Q1666.08 32.4315 1660.57 32.4315 Q1654.34 32.4315 1650.57 35.9558 Q1646.84 39.4801 1646.27 45.8805 L1672.85 45.84 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip942)\" d=\"M637.349 1390.67 L265.903 1390.67 L265.903 1373.6 L637.349 1373.6 L637.349 1390.67 L637.349 1390.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"637.349,1390.67 265.903,1390.67 265.903,1373.6 637.349,1373.6 637.349,1390.67 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M806.189 1369.33 L265.903 1369.33 L265.903 1352.27 L806.189 1352.27 L806.189 1369.33 L806.189 1369.33  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"806.189,1369.33 265.903,1369.33 265.903,1352.27 806.189,1352.27 806.189,1369.33 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M1414.01 1348 L265.903 1348 L265.903 1330.93 L1414.01 1330.93 L1414.01 1348 L1414.01 1348  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1414.01,1348 265.903,1348 265.903,1330.93 1414.01,1330.93 1414.01,1348 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M1312.71 1326.67 L265.903 1326.67 L265.903 1309.6 L1312.71 1309.6 L1312.71 1326.67 L1312.71 1326.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1312.71,1326.67 265.903,1326.67 265.903,1309.6 1312.71,1309.6 1312.71,1326.67 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M2021.83 1305.33 L265.903 1305.33 L265.903 1288.27 L2021.83 1288.27 L2021.83 1305.33 L2021.83 1305.33  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2021.83,1305.33 265.903,1305.33 265.903,1288.27 2021.83,1288.27 2021.83,1305.33 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M1481.55 1284 L265.903 1284 L265.903 1266.94 L1481.55 1266.94 L1481.55 1284 L1481.55 1284  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1481.55,1284 265.903,1284 265.903,1266.94 1481.55,1266.94 1481.55,1284 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M772.421 1262.67 L265.903 1262.67 L265.903 1245.6 L772.421 1245.6 L772.421 1262.67 L772.421 1262.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"772.421,1262.67 265.903,1262.67 265.903,1245.6 772.421,1245.6 772.421,1262.67 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M400.974 1241.34 L265.903 1241.34 L265.903 1224.27 L400.974 1224.27 L400.974 1241.34 L400.974 1241.34  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"400.974,1241.34 265.903,1241.34 265.903,1224.27 400.974,1224.27 400.974,1241.34 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M1785.46 1220 L265.903 1220 L265.903 1202.94 L1785.46 1202.94 L1785.46 1220 L1785.46 1220  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1785.46,1220 265.903,1220 265.903,1202.94 1785.46,1202.94 1785.46,1220 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M1177.63 1198.67 L265.903 1198.67 L265.903 1181.6 L1177.63 1181.6 L1177.63 1198.67 L1177.63 1198.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1177.63,1198.67 265.903,1198.67 265.903,1181.6 1177.63,1181.6 1177.63,1198.67 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M873.724 1177.34 L265.903 1177.34 L265.903 1160.27 L873.724 1160.27 L873.724 1177.34 L873.724 1177.34  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"873.724,1177.34 265.903,1177.34 265.903,1160.27 873.724,1160.27 873.724,1177.34 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M569.814 1156 L265.903 1156 L265.903 1138.94 L569.814 1138.94 L569.814 1156 L569.814 1156  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"569.814,1156 265.903,1156 265.903,1138.94 569.814,1138.94 569.814,1156 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M1042.56 1134.67 L265.903 1134.67 L265.903 1117.6 L1042.56 1117.6 L1042.56 1134.67 L1042.56 1134.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1042.56,1134.67 265.903,1134.67 265.903,1117.6 1042.56,1117.6 1042.56,1134.67 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M2123.13 1113.34 L265.903 1113.34 L265.903 1096.27 L2123.13 1096.27 L2123.13 1113.34 L2123.13 1113.34  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2123.13,1113.34 265.903,1113.34 265.903,1096.27 2123.13,1096.27 2123.13,1113.34 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M1211.4 1092 L265.903 1092 L265.903 1074.94 L1211.4 1074.94 L1211.4 1092 L1211.4 1092  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1211.4,1092 265.903,1092 265.903,1074.94 1211.4,1074.94 1211.4,1092 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M1616.62 1070.67 L265.903 1070.67 L265.903 1053.6 L1616.62 1053.6 L1616.62 1070.67 L1616.62 1070.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1616.62,1070.67 265.903,1070.67 265.903,1053.6 1616.62,1053.6 1616.62,1070.67 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M1717.92 1049.34 L265.903 1049.34 L265.903 1032.27 L1717.92 1032.27 L1717.92 1049.34 L1717.92 1049.34  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1717.92,1049.34 265.903,1049.34 265.903,1032.27 1717.92,1032.27 1717.92,1049.34 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M1886.76 1028 L265.903 1028 L265.903 1010.94 L1886.76 1010.94 L1886.76 1028 L1886.76 1028  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1886.76,1028 265.903,1028 265.903,1010.94 1886.76,1010.94 1886.76,1028 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M1852.99 1006.67 L265.903 1006.67 L265.903 989.603 L1852.99 989.603 L1852.99 1006.67 L1852.99 1006.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1852.99,1006.67 265.903,1006.67 265.903,989.603 1852.99,989.603 1852.99,1006.67 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M1751.69 985.336 L265.903 985.336 L265.903 968.269 L1751.69 968.269 L1751.69 985.336 L1751.69 985.336  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1751.69,985.336 265.903,985.336 265.903,968.269 1751.69,968.269 1751.69,985.336 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M1988.06 964.003 L265.903 964.003 L265.903 946.936 L1988.06 946.936 L1988.06 964.003 L1988.06 964.003  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1988.06,964.003 265.903,964.003 265.903,946.936 1988.06,946.936 1988.06,964.003 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M1515.31 942.669 L265.903 942.669 L265.903 925.603 L1515.31 925.603 L1515.31 942.669 L1515.31 942.669  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1515.31,942.669 265.903,942.669 265.903,925.603 1515.31,925.603 1515.31,942.669 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M299.671 921.336 L265.903 921.336 L265.903 904.27 L299.671 904.27 L299.671 921.336 L299.671 921.336  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"299.671,921.336 265.903,921.336 265.903,904.27 299.671,904.27 299.671,921.336 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M536.046 900.003 L265.903 900.003 L265.903 882.936 L536.046 882.936 L536.046 900.003 L536.046 900.003  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"536.046,900.003 265.903,900.003 265.903,882.936 536.046,882.936 536.046,900.003 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M367.207 878.67 L265.903 878.67 L265.903 861.603 L367.207 861.603 L367.207 878.67 L367.207 878.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"367.207,878.67 265.903,878.67 265.903,861.603 367.207,861.603 367.207,878.67 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M333.439 857.336 L265.903 857.336 L265.903 840.27 L333.439 840.27 L333.439 857.336 L333.439 857.336  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"333.439,857.336 265.903,857.336 265.903,840.27 333.439,840.27 333.439,857.336 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M975.028 836.003 L265.903 836.003 L265.903 818.937 L975.028 818.937 L975.028 836.003 L975.028 836.003  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"975.028,836.003 265.903,836.003 265.903,818.937 975.028,818.937 975.028,836.003 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M941.26 814.67 L265.903 814.67 L265.903 797.603 L941.26 797.603 L941.26 814.67 L941.26 814.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"941.26,814.67 265.903,814.67 265.903,797.603 941.26,797.603 941.26,814.67 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M1549.08 793.337 L265.903 793.337 L265.903 776.27 L1549.08 776.27 L1549.08 793.337 L1549.08 793.337  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1549.08,793.337 265.903,793.337 265.903,776.27 1549.08,776.27 1549.08,793.337 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M839.956 772.003 L265.903 772.003 L265.903 754.937 L839.956 754.937 L839.956 772.003 L839.956 772.003  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"839.956,772.003 265.903,772.003 265.903,754.937 839.956,754.937 839.956,772.003 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M434.742 750.67 L265.903 750.67 L265.903 733.603 L434.742 733.603 L434.742 750.67 L434.742 750.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"434.742,750.67 265.903,750.67 265.903,733.603 434.742,733.603 434.742,750.67 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M502.278 729.337 L265.903 729.337 L265.903 712.27 L502.278 712.27 L502.278 729.337 L502.278 729.337  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"502.278,729.337 265.903,729.337 265.903,712.27 502.278,712.27 502.278,729.337 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M468.51 708.004 L265.903 708.004 L265.903 690.937 L468.51 690.937 L468.51 708.004 L468.51 708.004  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"468.51,708.004 265.903,708.004 265.903,690.937 468.51,690.937 468.51,708.004 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M603.582 686.67 L265.903 686.67 L265.903 669.604 L603.582 669.604 L603.582 686.67 L603.582 686.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"603.582,686.67 265.903,686.67 265.903,669.604 603.582,669.604 603.582,686.67 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M1278.94 665.337 L265.903 665.337 L265.903 648.27 L1278.94 648.27 L1278.94 665.337 L1278.94 665.337  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1278.94,665.337 265.903,665.337 265.903,648.27 1278.94,648.27 1278.94,665.337 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M671.117 644.004 L265.903 644.004 L265.903 626.937 L671.117 626.937 L671.117 644.004 L671.117 644.004  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"671.117,644.004 265.903,644.004 265.903,626.937 671.117,626.937 671.117,644.004 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M1684.15 622.67 L265.903 622.67 L265.903 605.604 L1684.15 605.604 L1684.15 622.67 L1684.15 622.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1684.15,622.67 265.903,622.67 265.903,605.604 1684.15,605.604 1684.15,622.67 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M1143.87 601.337 L265.903 601.337 L265.903 584.271 L1143.87 584.271 L1143.87 601.337 L1143.87 601.337  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1143.87,601.337 265.903,601.337 265.903,584.271 1143.87,584.271 1143.87,601.337 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M1380.24 580.004 L265.903 580.004 L265.903 562.937 L1380.24 562.937 L1380.24 580.004 L1380.24 580.004  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1380.24,580.004 265.903,580.004 265.903,562.937 1380.24,562.937 1380.24,580.004 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M1819.22 558.671 L265.903 558.671 L265.903 541.604 L1819.22 541.604 L1819.22 558.671 L1819.22 558.671  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1819.22,558.671 265.903,558.671 265.903,541.604 1819.22,541.604 1819.22,558.671 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M1110.1 537.337 L265.903 537.337 L265.903 520.271 L1110.1 520.271 L1110.1 537.337 L1110.1 537.337  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1110.1,537.337 265.903,537.337 265.903,520.271 1110.1,520.271 1110.1,537.337 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M2089.37 516.004 L265.903 516.004 L265.903 498.938 L2089.37 498.938 L2089.37 516.004 L2089.37 516.004  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2089.37,516.004 265.903,516.004 265.903,498.938 2089.37,498.938 2089.37,516.004 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M1346.47 494.671 L265.903 494.671 L265.903 477.604 L1346.47 477.604 L1346.47 494.671 L1346.47 494.671  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1346.47,494.671 265.903,494.671 265.903,477.604 1346.47,477.604 1346.47,494.671 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M2190.67 473.338 L265.903 473.338 L265.903 456.271 L2190.67 456.271 L2190.67 473.338 L2190.67 473.338  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2190.67,473.338 265.903,473.338 265.903,456.271 2190.67,456.271 2190.67,473.338 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M1076.33 452.004 L265.903 452.004 L265.903 434.938 L1076.33 434.938 L1076.33 452.004 L1076.33 452.004  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1076.33,452.004 265.903,452.004 265.903,434.938 1076.33,434.938 1076.33,452.004 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M907.492 430.671 L265.903 430.671 L265.903 413.605 L907.492 413.605 L907.492 430.671 L907.492 430.671  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"907.492,430.671 265.903,430.671 265.903,413.605 907.492,413.605 907.492,430.671 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M1582.85 409.338 L265.903 409.338 L265.903 392.271 L1582.85 392.271 L1582.85 409.338 L1582.85 409.338  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1582.85,409.338 265.903,409.338 265.903,392.271 1582.85,392.271 1582.85,409.338 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M1008.8 388.005 L265.903 388.005 L265.903 370.938 L1008.8 370.938 L1008.8 388.005 L1008.8 388.005  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1008.8,388.005 265.903,388.005 265.903,370.938 1008.8,370.938 1008.8,388.005 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M2258.21 366.671 L265.903 366.671 L265.903 349.605 L2258.21 349.605 L2258.21 366.671 L2258.21 366.671  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2258.21,366.671 265.903,366.671 265.903,349.605 2258.21,349.605 2258.21,366.671 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M2156.9 345.338 L265.903 345.338 L265.903 328.271 L2156.9 328.271 L2156.9 345.338 L2156.9 345.338  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2156.9,345.338 265.903,345.338 265.903,328.271 2156.9,328.271 2156.9,345.338 \"/>\n",
       "<path clip-path=\"url(#clip942)\" d=\"M2291.97 324.005 L265.903 324.005 L265.903 306.938 L2291.97 306.938 L2291.97 324.005 L2291.97 324.005  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip942)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2291.97,324.005 265.903,324.005 265.903,306.938 2291.97,306.938 2291.97,324.005 \"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"299.671\" cy=\"1168.8\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"333.439\" cy=\"1062.14\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"367.207\" cy=\"678.137\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"400.974\" cy=\"742.137\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"434.742\" cy=\"294.138\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"468.51\" cy=\"635.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"502.278\" cy=\"1083.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"536.046\" cy=\"1318.13\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"569.814\" cy=\"443.471\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"603.582\" cy=\"827.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"637.349\" cy=\"1019.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"671.117\" cy=\"1211.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"704.885\" cy=\"912.803\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"738.653\" cy=\"230.138\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"772.421\" cy=\"806.137\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"806.189\" cy=\"550.137\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"839.956\" cy=\"486.138\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"873.724\" cy=\"379.471\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"907.492\" cy=\"400.805\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"941.26\" cy=\"464.804\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"975.028\" cy=\"315.471\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1008.8\" cy=\"614.137\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1042.56\" cy=\"1382.13\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1076.33\" cy=\"1232.8\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1110.1\" cy=\"1339.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1143.87\" cy=\"1360.8\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1177.63\" cy=\"955.469\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1211.4\" cy=\"976.803\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1245.17\" cy=\"592.804\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1278.94\" cy=\"1040.8\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1312.71\" cy=\"1296.8\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1346.47\" cy=\"1254.14\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1380.24\" cy=\"1275.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1414.01\" cy=\"1190.14\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1447.78\" cy=\"763.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1481.55\" cy=\"1147.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1515.31\" cy=\"507.471\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1549.08\" cy=\"848.803\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1582.85\" cy=\"699.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1616.62\" cy=\"422.138\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1650.38\" cy=\"870.136\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1684.15\" cy=\"251.472\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1717.92\" cy=\"720.803\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1751.69\" cy=\"187.472\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1785.46\" cy=\"891.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1819.22\" cy=\"998.136\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1852.99\" cy=\"571.471\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1886.76\" cy=\"934.136\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1920.53\" cy=\"144.805\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip942)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1954.3\" cy=\"208.805\" r=\"2\"/>\n",
       "</svg>\n"
      ],
      "text/html": [
       "<img src=\"data:image/png;base64,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\" />"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_gain =  [(first(x),last(x)) for x in importance(model)]\n",
    "feature, gain = first.(feature_gain), last.(feature_gain)\n",
    "\n",
    "using Plots;\n",
    "\n",
    "p = bar(feature, y=gain, orientation=\"h\", legend=false)\n",
    "xlabel!(p,\"Gain\")\n",
    "ylabel!(p,\"Feature\")\n",
    "title!(\"Feature Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d533db36",
   "metadata": {},
   "source": [
    "As you can see, not all features has the same importance. It should be notice that the Feature axis identifies the position in the Vector  feature which is ordered by the gain value by default.\n",
    "\n",
    "### Integration with MLJ\n",
    "\n",
    "As mentioned previously, **XGBoost** can be seamlessly integrated into the **MLJ framework**, allowing it to be used consistently with other models, pipelines, and tuning workflows.  \n",
    "This integration is provided through the [`MLJXGBoostInterface.jl`](https://github.com/JuliaAI/MLJXGBoostInterface.jl) package, which acts as a bridge between MLJ and the native XGBoost implementation.\n",
    "\n",
    "To load the model, simply use the `@load` macro from MLJ:\n",
    "\n",
    "```julia\n",
    "# Load data and ensure correct scientific types\n",
    "using DataFrames, MLJ\n",
    "\n",
    "# Load XGBoost classifier\n",
    "XGBoostClassifier = @load XGBoostClassifier pkg=XGBoost\n",
    "```\n",
    "Once loaded, the classifier can be configured, trained, and evaluated like any other MLJ model.\n",
    "\n",
    "```julia\n",
    "# Define model and parameters\n",
    "xgb_model = XGBoostClassifier(\n",
    "    num_round = 100,\n",
    "    eta = 0.1, \n",
    "    max_depth = 6,\n",
    "    objective = \"multi:softprob\"       # suitable for multi-class problems\n",
    ")\n",
    "\n",
    "# Bind model and data\n",
    "mach = machine(xgb_model, X, y)\n",
    "\n",
    "# Train the model\n",
    "fit!(mach, verbosity = 1)\n",
    "\n",
    "# Evaluate using cross-validation\n",
    "cv_result = evaluate!(\n",
    "    mach,\n",
    "    resampling = CV(nfolds = 5, shuffle = true),\n",
    "    measure = [accuracy, cross_entropy],\n",
    "    verbosity = 0\n",
    ")\n",
    "\n",
    "cv_result.measurement\n",
    "\n",
    "# ==============================\n",
    "# Example of Tuning of the model\n",
    "# ==============================\n",
    "\n",
    "# Define a parameter range to explore\n",
    "r_eta = range(xgb_model, :eta, lower=0.01, upper=0.3)\n",
    "r_depth = range(xgb_model, :max_depth, lower=3, upper=10)\n",
    "\n",
    "# Define tuning strategy\n",
    "tuned_xgb = TunedModel(\n",
    "    model = xgb_model,\n",
    "    resampling = CV(nfolds=5, shuffle=true),\n",
    "    range = [r_eta, r_depth],\n",
    "    measure = accuracy,\n",
    "    tuning = Grid(resolution=5),\n",
    "    operation = predict_mode\n",
    ")\n",
    "\n",
    "# Train tuned model\n",
    "mach_tuned = machine(tuned_xgb, X, y)\n",
    "fit!(mach_tuned)\n",
    "\n",
    "```\n",
    "\n",
    "## Julia Notes\n",
    "\n",
    "### Understanding `coerce` in MLJ\n",
    "\n",
    "When working with datasets in MLJ, it’s important to understand that MLJ distinguishes between **machine types** (e.g., `Int64`, `Float64`, `String`) and **scientific types** (or `scitypes`), which describe how data should be *interpreted* for modeling.\n",
    "\n",
    "### Why `coerce` is Needed\n",
    "\n",
    "MLJ models don’t rely on the raw Julia types — instead, they expect variables to have *scientific meanings*:\n",
    "- A numeric column can represent a **continuous** feature.\n",
    "- A string column can represent a **categorical** feature.\n",
    "- A boolean or integer can be **ordered** or **unordered**, depending on context.\n",
    "\n",
    "Since Julia doesn’t know this automatically, we use the `coerce()` function to explicitly tell MLJ how to interpret each column.  \n",
    "This ensures compatibility between your data and the MLJ model you plan to use.\n",
    "\n",
    "---\n",
    "\n",
    "### Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9470a0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌──────────┬────────────┬─────────┐\n",
       "│\u001b[22m names    \u001b[0m│\u001b[22m scitypes   \u001b[0m│\u001b[22m types   \u001b[0m│\n",
       "├──────────┼────────────┼─────────┤\n",
       "│ Column1  │ Continuous │ Float64 │\n",
       "│ Column2  │ Continuous │ Float64 │\n",
       "│ Column3  │ Continuous │ Float64 │\n",
       "│ Column4  │ Continuous │ Float64 │\n",
       "│ Column5  │ Continuous │ Float64 │\n",
       "│ Column6  │ Continuous │ Float64 │\n",
       "│ Column7  │ Continuous │ Float64 │\n",
       "│ Column8  │ Continuous │ Float64 │\n",
       "│ Column9  │ Continuous │ Float64 │\n",
       "│ Column10 │ Continuous │ Float64 │\n",
       "│ Column11 │ Continuous │ Float64 │\n",
       "│ Column12 │ Continuous │ Float64 │\n",
       "│ Column13 │ Continuous │ Float64 │\n",
       "│ Column14 │ Continuous │ Float64 │\n",
       "│ Column15 │ Continuous │ Float64 │\n",
       "│ Column16 │ Continuous │ Float64 │\n",
       "│ Column17 │ Continuous │ Float64 │\n",
       "│ Column18 │ Continuous │ Float64 │\n",
       "│ Column19 │ Continuous │ Float64 │\n",
       "│ Column20 │ Continuous │ Float64 │\n",
       "│ Column21 │ Continuous │ Float64 │\n",
       "│ Column22 │ Continuous │ Float64 │\n",
       "│    ⋮     │     ⋮      │    ⋮    │\n",
       "└──────────┴────────────┴─────────┘\n",
       "\u001b[36m                    38 rows omitted\u001b[0m\n"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MLJ, DataFrames, CSV\n",
    "\n",
    "# Load the dataset\n",
    "data = CSV.read(\"sonar.all_data\", DataFrame, header=false)\n",
    "\n",
    "# Inspect current column types\n",
    "schema(data)\n",
    "\n",
    "# Suppose the last column is the target ('Rock' or 'Mine')\n",
    "y, X = unpack(data, ==(:Column61), rng=123)\n",
    "\n",
    "# Convert the target variable to categorical\n",
    "y = coerce(y, Multiclass)\n",
    "\n",
    "# Convert all feature columns to continuous\n",
    "X = coerce(X, autotype(X, rules = (:discrete_to_continuous,)))\n",
    "\n",
    "# Verify the new scientific types\n",
    "schema(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e39a8f6",
   "metadata": {},
   "source": [
    "\n",
    "### Common examples\n",
    "\n",
    "| Situation                            | What to Do              | Example                              |\n",
    "| ------------------------------------ | ----------------------- | ------------------------------------ |\n",
    "| Categorical labels stored as strings | Convert to `Multiclass` | `y = coerce(y, Multiclass)`          |\n",
    "| Numerical features                   | Convert to `Continuous` | `X = coerce(X, :var1 => Continuous)` |\n",
    "| Automatic inference                  | Use `autotype()`        | `autotype(X)`                        |\n",
    "| Check current scientific types       | Use `schema()`          | `schema(X)`                          |\n",
    "\n",
    "### What Happens If You Skip coerce\n",
    "\n",
    "If you skip the coercion step:\n",
    "\n",
    "* MLJ might misinterpret your target as continuous, blocking classification models.\n",
    "* Some algorithms (e.g., DecisionTree, NaiveBayes) may fail or produce wrong predictions.\n",
    "* The machine() function might refuse to bind the model and data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b376d4",
   "metadata": {},
   "source": [
    "## Voting Classifier\n",
    "\n",
    "Following this lines there are several examples of how to use the new Voting Classifier that we have implemented.\n",
    "\n",
    "In previous examples, we created the ensemble as follows:\n",
    "\n",
    "```julia\n",
    "voting_hard = VotingClassifier(models=base_models_list, voting=:hard)\n",
    "voting_soft = VotingClassifier(models=base_models_list, voting=:soft)\n",
    "\n",
    "mach_hard = machine(voting_hard, train_input, train_output) |> fit!\n",
    "mach_soft = machine(voting_soft, train_input, train_output) |> fit!\n",
    "```\n",
    "\n",
    "* `voting=:hard` → uses majority voting (based on class labels).\n",
    "\n",
    "* `voting=:soft` → uses average probability voting (requires models that output probabilities).\n",
    "\n",
    "The following examples show how to modify and integrate the VotingClassifier dynamically within your workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f45bc191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current voting type: hard\n",
      "Voting type changed to: soft\n"
     ]
    }
   ],
   "source": [
    "base_models_list = base_models\n",
    "\n",
    "# Example 1: Changing the Voting Type Dynamically\n",
    "ensemble = VotingClassifier(models=base_models_list, voting=:hard)\n",
    "println(\"\\nCurrent voting type: $(ensemble.voting)\")\n",
    "\n",
    "ensemble.voting = :soft\n",
    "println(\"Voting type changed to: $(ensemble.voting)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "39a28c86",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: `@pipeline` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.\nin expression starting at In[146]:5",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: `@pipeline` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.\nin expression starting at In[146]:5",
      ""
     ]
    }
   ],
   "source": [
    "using MLJBase\n",
    "using MLJ # Added for the @pipeline macro\n",
    "\n",
    "# Example 2: Using the VotingClassifier in a Pipeline\n",
    "pipe_hard = @pipeline(\n",
    "    Standardizer(),\n",
    "    VotingClassifier(models=base_models_list, voting=:hard)\n",
    ")\n",
    "\n",
    "pipe_soft = @pipeline(\n",
    "    Standardizer(),\n",
    "    VotingClassifier(models=base_models_list, voting=:soft)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade52dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Voting CV accuracy: 75.94 %\n",
      "Soft Voting CV accuracy: 75.94 %\n"
     ]
    }
   ],
   "source": [
    "using MLJ\n",
    "\n",
    "train_input_mlj = MLJ.table(train_input_standardized)\n",
    "train_output_mlj = categorical(output_data[train_idx])\n",
    "\n",
    "voting_hard = models[\"Ensemble (Hard Voting)\"]\n",
    "voting_soft = models[\"Ensemble (Soft Voting - Equal)\"]\n",
    "\n",
    "# Example 3: Cross-Validation Comparing Both Voting Strategies\n",
    "# Use the restored MLJ data\n",
    "cv_hard = evaluate!(\n",
    "    machine(voting_hard, train_input_mlj, train_output_mlj),\n",
    "    resampling=CV(nfolds=5),\n",
    "    measure=MLJ.accuracy,\n",
    "    check_measure=false\n",
    ")\n",
    "println(\"Hard Voting CV accuracy: $(round(mean(cv_hard.measurement)*100, digits=2)) %\")\n",
    "\n",
    "cv_soft = evaluate!(\n",
    "    machine(voting_soft, train_input_mlj, train_output_mlj),\n",
    "    resampling=CV(nfolds=5),\n",
    "    measure=MLJ.accuracy,\n",
    "    check_measure=false\n",
    ")\n",
    "println(\"Soft Voting CV accuracy: $(round(mean(cv_soft.measurement)*100, digits=2)) %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.7",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
